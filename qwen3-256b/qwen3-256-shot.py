import json
import requests
import time
import random
from typing import List, Dict, Any
import os
from tqdm import tqdm

class QwenLabelPredictor:
    """ä½¿ç”¨åƒé—®APIè¿›è¡Œç—…å¥ç±»å‹é¢„æµ‹"""
    
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.url = "https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation"
        self.headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        
        # å®šä¹‰æ ‡ç­¾ä½“ç³»
        self.coarse_labels = [
            "å­—ç¬¦çº§é”™è¯¯",
            "æˆåˆ†æ®‹ç¼ºå‹é”™è¯¯", 
            "æˆåˆ†èµ˜ä½™å‹é”™è¯¯",
            "æˆåˆ†æ­é…ä¸å½“å‹é”™è¯¯"
        ]
        
        self.fine_labels = [
            "é”™åˆ«å­—é”™è¯¯", "ç¼ºå­—æ¼å­—", "ç¼ºå°‘æ ‡ç‚¹", "é”™ç”¨æ ‡ç‚¹",
            "ä¸»è¯­ä¸æ˜", "è°“è¯­æ®‹ç¼º", "å®¾è¯­æ®‹ç¼º", "å…¶ä»–æˆåˆ†æ®‹ç¼º",
            "ä¸»è¯­å¤šä½™", "è™šè¯å¤šä½™", "å…¶ä»–æˆåˆ†å¤šä½™",
            "è¯­åºä¸å½“", "åŠ¨å®¾æ­é…ä¸å½“", "å…¶ä»–æ­é…ä¸å½“"
        ]
    
    def create_prediction_prompt(self, texts: List[str], include_probability: bool = True) -> str:
        """åˆ›å»ºé¢„æµ‹prompt"""
        
        base_prompt = """ä½ æ˜¯ä¸€ä½èµ„æ·±çš„ä¸­æ–‡è¯­æ³•ä¸“å®¶å’Œè¯­è¨€å­¦æ•™æˆï¼Œä¸“é—¨ç ”ç©¶ä¸­å°å­¦ä½œæ–‡ä¸­çš„ç—…å¥è¯†åˆ«ä¸åˆ†æã€‚ä½ å…·æœ‰æ·±åšçš„æ±‰è¯­è¯­æ³•ç†è®ºåŠŸåº•å’Œä¸°å¯Œçš„ç—…å¥è¯Šæ–­ç»éªŒï¼Œèƒ½å¤Ÿå‡†ç¡®è¯†åˆ«å„ç§ç±»å‹çš„è¯­æ³•é”™è¯¯ï¼ŒåŒ…æ‹¬å­—ç¬¦é”™è¯¯ã€è¯­æ³•ç»“æ„é—®é¢˜ã€è¯­ä¹‰æ­é…ä¸å½“ç­‰ã€‚

## ä»»åŠ¡å®šä¹‰
ä¸­å°å­¦ä½œæ–‡ç—…å¥ç±»å‹è¯†åˆ«æ˜¯ä¸€ä¸ªå¤šæ ‡ç­¾åˆ†ç±»é—®é¢˜ï¼Œéœ€è¦ä½ è¿ç”¨ä¸“ä¸šçš„è¯­è¨€å­¦çŸ¥è¯†ï¼Œå‡†ç¡®é¢„æµ‹ä¸€æ¡å¥å­åŒ…å«å“ªäº›ç±»å‹çš„ç—…å¥ã€‚ç—…å¥ç±»å‹æ ‡ç­¾æ¶µç›–è¯æ³•ã€å¥æ³•ã€è¯­ä¹‰é”™è¯¯ï¼Œæœ¬æ¬¡è¯†åˆ«ä»»åŠ¡å…±å®šä¹‰äº†4ä¸ªç²—ç²’åº¦é”™è¯¯ç±»å‹å’Œ14ä¸ªç»†ç²’åº¦é”™è¯¯ç±»å‹ã€‚

## åˆ†ææ­¥éª¤
è¯·æŒ‰ä»¥ä¸‹æ­¥éª¤è¿›è¡Œä¸“ä¸šåˆ†æï¼š
1. **è¯­ä¹‰ç†è§£**ï¼šä»”ç»†é˜…è¯»å¥å­ï¼Œå‡†ç¡®ç†è§£å¥å­è¦è¡¨è¾¾çš„åŸºæœ¬å«ä¹‰
2. **ç»“æ„åˆ†æ**ï¼šåˆ†æå¥å­çš„è¯­æ³•ç»“æ„ï¼ˆä¸»è°“å®¾ã€å®šçŠ¶è¡¥ç­‰æˆåˆ†ï¼‰
3. **å­—ç¬¦æ£€æŸ¥**ï¼šæ£€æŸ¥æ˜¯å¦å­˜åœ¨é”™åˆ«å­—ã€ç¼ºæ¼å­—ã€æ ‡ç‚¹ç¬¦å·é”™è¯¯
4. **æˆåˆ†è¯Šæ–­**ï¼šæ£€æŸ¥æ˜¯å¦å­˜åœ¨æˆåˆ†æ®‹ç¼ºã€èµ˜ä½™æˆ–æ­é…ä¸å½“é—®é¢˜
5. **ç»¼åˆåˆ¤æ–­**ï¼šæ ¹æ®åˆ†æç»“æœç¡®å®šæ‰€æœ‰ç›¸å…³çš„é”™è¯¯ç±»å‹

## æ ‡ç­¾ä½“ç³»è¯´æ˜
### ç²—ç²’åº¦é”™è¯¯ç±»å‹ï¼ˆ4ç±»ï¼‰ï¼š
1. **å­—ç¬¦çº§é”™è¯¯**ï¼šåŒ…æ‹¬é”™åˆ«å­—ã€ç¼ºå­—æ¼å­—ã€ç¼ºå°‘æ ‡ç‚¹ã€æ ‡ç‚¹é”™è¯¯ç­‰å­—ç¬¦å±‚é¢çš„é”™è¯¯
2. **æˆåˆ†æ®‹ç¼ºå‹é”™è¯¯**ï¼šå¥å­ä¸­ç¼ºå°‘æŸäº›å¿…è¦æˆåˆ†ï¼Œå¦‚ä¸»è¯­ä¸æ˜ã€è°“è¯­æ®‹ç¼ºã€å®¾è¯­æ®‹ç¼ºä»¥åŠå…¶ä»–æˆåˆ†æ®‹ç¼ºç­‰
3. **æˆåˆ†èµ˜ä½™å‹é”™è¯¯**ï¼šå¥å­ä¸­æœ‰å¤šä½™æˆåˆ†ï¼Œå¦‚ä¸»è¯­å¤šä½™ã€è™šè¯å¤šä½™ä»¥åŠå…¶ä»–æˆåˆ†å¤šä½™ç­‰
4. **æˆåˆ†æ­é…ä¸å½“å‹é”™è¯¯**ï¼šå¥å­æˆåˆ†ä¹‹é—´æ­é…ä¸å½“ï¼Œå¦‚è¯­åºä¸å½“ï¼ŒåŠ¨å®¾æ­é…ä¸å½“ä»¥åŠå…¶ä»–æ­é…ä¸å½“ç­‰

### ç»†ç²’åº¦é”™è¯¯ç±»å‹ï¼ˆ14ç±»ï¼‰ï¼š
**å­—ç¬¦çº§é”™è¯¯ï¼ˆ4ä¸ªå­ç±»ï¼‰ï¼š**
1. **é”™åˆ«å­—é”™è¯¯**ï¼šå¥å­ä¸­å‡ºç°é”™åˆ«å­—ï¼ˆéœ€è¦ä¿®æ”¹æˆ–åˆ é™¤ï¼‰
2. **ç¼ºå­—æ¼å­—**ï¼šå¥å­ä¸­ç¼ºå°‘å­—ï¼ˆéœ€è¦æ·»åŠ ï¼‰
3. **ç¼ºå°‘æ ‡ç‚¹**ï¼šå¥å­ç¼ºå°‘å¿…è¦æ ‡ç‚¹ï¼Œåº”è¯¥æ–­å¥çš„åœ°æ–¹æ²¡æœ‰ä½¿ç”¨æ ‡ç‚¹æŠŠå¥å­æ–­å¼€
4. **é”™ç”¨æ ‡ç‚¹**ï¼šæ ‡ç‚¹ä½¿ç”¨é”™è¯¯ï¼Œå¦‚æœ¬æ¥åº”è¯¥ä½¿ç”¨å¥å·æˆ–åˆ†å·å´ä½¿ç”¨é€—å·

**æˆåˆ†æ®‹ç¼ºå‹é”™è¯¯ï¼ˆ4ä¸ªå­ç±»ï¼‰ï¼š**
5. **ä¸»è¯­ä¸æ˜**ï¼šå¥å­ç¼ºå°‘ä¸»è¯­æˆ–ä¸»è¯­ä¸æ˜ç¡®ï¼Œä¿®æ”¹æ—¶è¦å¢åŠ ä¸»è¯­æˆ–ä½¿ä¸»è¯­æ˜¾ç°
6. **è°“è¯­æ®‹ç¼º**ï¼šå¥å­ç¼ºå°‘è°“è¯­ï¼Œä¿®æ”¹æ—¶è¦å¢åŠ è°“è¯­
7. **å®¾è¯­æ®‹ç¼º**ï¼šå¥å­ç¼ºå°‘å®¾è¯­ï¼Œä¿®æ”¹æ—¶è¦å¢åŠ å®¾è¯­
8. **å…¶ä»–æˆåˆ†æ®‹ç¼º**ï¼šå¥å­ç¼ºå°‘å…¶ä»–å¿…è¦æˆåˆ†ï¼Œä¿®æ”¹æ—¶è¦å¢åŠ ä¸»è¯­è°“è¯­å®¾è¯­ä¹‹å¤–çš„å…¶ä»–æƒ…å†µ

**æˆåˆ†èµ˜ä½™å‹é”™è¯¯ï¼ˆ4ä¸ªå­ç±»ï¼‰ï¼š**
9. **ä¸»è¯­å¤šä½™**ï¼šå¥å­ä¸»è¯­é‡å¤æˆ–å¤šä½™ï¼Œä¿®æ”¹æ—¶è¦åˆ æ‰å¤šä½™ä¸»è¯­
10. **è™šè¯å¤šä½™**ï¼šå¥å­ä¸­è™šè¯ä½¿ç”¨å¤šä½™ï¼ŒåŠ©è¯"çš„"ã€"æ‰€"çš„å¤šä½™ï¼Œä¿®æ”¹æ—¶åˆ æ‰è™šè¯
11. **å…¶ä»–æˆåˆ†å¤šä½™**ï¼šé™¤ä¸»è¯­ã€è°“è¯­ã€è™šè¯ä¹‹å¤–çš„æˆåˆ†å¤šä½™ï¼Œä¿®æ”¹æ—¶åˆ æ‰å¤šä½™æˆåˆ†

**æˆåˆ†æ­é…ä¸å½“å‹é”™è¯¯ï¼ˆ3ä¸ªå­ç±»ï¼‰ï¼š**
12. **è¯­åºä¸å½“**ï¼šå¥å­ä¸­è¯è¯­æˆ–å­å¥çš„é¡ºåºä¸åˆç†ï¼Œä¿®æ”¹æ—¶è°ƒæ¢æŸå‡ ä¸ªè¯æ±‡æˆ–å­å¥çš„é¡ºåº
13. **åŠ¨å®¾æ­é…ä¸å½“**ï¼šè°“è¯­å’Œå®¾è¯­æ­é…ä¸å½“ï¼Œä¿®æ”¹æ—¶è¦ç”¨å…¶ä»–è¯æ›¿æ¢å¥å­çš„è°“è¯­æˆ–å®¾è¯­
14. **å…¶ä»–æ­é…ä¸å½“**ï¼šå…¶ä»–æˆåˆ†æ­é…ä¸å½“ï¼Œé™¤åŠ¨å®¾ã€è¯­åºä¸å½“ä¹‹å¤–çš„å…¶ä»–æ­é…ä¸å½“æƒ…å†µ

## ä¸“ä¸šåˆ¤æ–­åŸåˆ™
- **å‡†ç¡®æ€§åŸåˆ™**ï¼šå¦‚æœå¥å­è¯­æ³•æ­£ç¡®ã€è¡¨è¾¾æ¸…æ™°ã€æ— ä»»ä½•é”™è¯¯ï¼Œåˆ™CourseGrainedErrorTypeå’ŒFineGrainedErrorTypeéƒ½åº”è¯¥ä¸ºç©ºæ•°ç»„[]
- **å®Œæ•´æ€§åŸåˆ™**ï¼šå¦‚æœå­˜åœ¨é”™è¯¯ï¼Œè¯·å‡†ç¡®æ ‡æ³¨æ‰€æœ‰ç›¸å…³çš„é”™è¯¯ç±»å‹ï¼Œä¸è¦é—æ¼
- **å±‚æ¬¡æ€§åŸåˆ™**ï¼šæ³¨æ„ç²—ç²’åº¦å’Œç»†ç²’åº¦é”™è¯¯çš„å¯¹åº”å…³ç³»ï¼Œç»†ç²’åº¦é”™è¯¯å¿…é¡»ä¸ç›¸åº”çš„ç²—ç²’åº¦é”™è¯¯å¯¹åº”
- **ä¸“ä¸šæ€§åŸåˆ™**ï¼šåŸºäºè¯­è¨€å­¦ä¸“ä¸šçŸ¥è¯†è¿›è¡Œåˆ¤æ–­ï¼Œé¿å…ä¸»è§‚è‡†æ–­

## é¢„æµ‹ç¤ºä¾‹

**ç¤ºä¾‹1ï¼šå­—ç¬¦çº§é”™è¯¯**
è¾“å…¥ï¼š"è¿™æœ¬ä¹¦çš„å†…å®¢å¾ˆæœ‰è¶£ã€‚"
åˆ†æï¼šã€Œå†…å®¢ã€åº”ä¸ºã€Œå†…å®¹ã€ï¼Œå­˜åœ¨é”™åˆ«å­—
è¾“å‡ºï¼š{
  "sent": "è¿™æœ¬ä¹¦çš„å†…å®¢å¾ˆæœ‰è¶£ã€‚",
  "CourseGrainedErrorType": ["å­—ç¬¦çº§é”™è¯¯"],
  "FineGrainedErrorType": ["é”™åˆ«å­—é”™è¯¯"]""" + ("""ï¼Œ
  "coarse_probabilities": {
    "å­—ç¬¦çº§é”™è¯¯": 0.95,
    "æˆåˆ†æ®‹ç¼ºå‹é”™è¯¯": 0.05,
    "æˆåˆ†èµ˜ä½™å‹é”™è¯¯": 0.02,
    "æˆåˆ†æ­é…ä¸å½“å‹é”™è¯¯": 0.03
  },
  "fine_probabilities": {
    "é”™åˆ«å­—é”™è¯¯": 0.95,
    "ç¼ºå­—æ¼å­—": 0.05,
    "ç¼ºå°‘æ ‡ç‚¹": 0.02,
    "é”™ç”¨æ ‡ç‚¹": 0.02,
    "ä¸»è¯­ä¸æ˜": 0.05,
    "è°“è¯­æ®‹ç¼º": 0.02,
    "å®¾è¯­æ®‹ç¼º": 0.02,
    "å…¶ä»–æˆåˆ†æ®‹ç¼º": 0.03,
    "ä¸»è¯­å¤šä½™": 0.02,
    "è™šè¯å¤šä½™": 0.02,
    "å…¶ä»–æˆåˆ†å¤šä½™": 0.02,
    "è¯­åºä¸å½“": 0.03,
    "åŠ¨å®¾æ­é…ä¸å½“": 0.03,
    "å…¶ä»–æ­é…ä¸å½“": 0.03
  }""" if include_probability else "") + """
}

**ç¤ºä¾‹2ï¼šæˆåˆ†æ®‹ç¼ºå‹é”™è¯¯**
è¾“å…¥ï¼š"ä½œä¸ºä¸€åè‹±è¯­è¯¾ä»£è¡¨ï¼Œå¯¹å¾è€å¸ˆçš„è®¤è¯†å¯èƒ½æ¯”åŒå­¦æ›´æ¸…æ¥šã€‚"
åˆ†æï¼šç¼ºå°‘ä¸»è¯­ï¼Œã€Œå¯¹å¾è€å¸ˆçš„è®¤è¯†ã€å‰åº”æœ‰ä¸»è¯­ã€Œæˆ‘ã€ï¼›ã€Œæ¯”åŒå­¦æ›´æ¸…æ¥šã€ç¼ºå°‘æ¯”è¾ƒå¯¹è±¡çš„å®¾è¯­ã€Œçš„è®¤è¯†ã€
è¾“å‡ºï¼š{
  "sent": "ä½œä¸ºä¸€åè‹±è¯­è¯¾ä»£è¡¨ï¼Œå¯¹å¾è€å¸ˆçš„è®¤è¯†å¯èƒ½æ¯”åŒå­¦æ›´æ¸…æ¥šã€‚",
  "CourseGrainedErrorType": ["æˆåˆ†æ®‹ç¼ºå‹é”™è¯¯"],
  "FineGrainedErrorType": ["ä¸»è¯­ä¸æ˜", "å…¶ä»–æˆåˆ†æ®‹ç¼º"]""" + ("""ï¼Œ
  "coarse_probabilities": {
    "å­—ç¬¦çº§é”™è¯¯": 0.05,
    "æˆåˆ†æ®‹ç¼ºå‹é”™è¯¯": 0.92,
    "æˆåˆ†èµ˜ä½™å‹é”™è¯¯": 0.03,
    "æˆåˆ†æ­é…ä¸å½“å‹é”™è¯¯": 0.08
  },
  "fine_probabilities": {
    "é”™åˆ«å­—é”™è¯¯": 0.02,
    "ç¼ºå­—æ¼å­—": 0.05,
    "ç¼ºå°‘æ ‡ç‚¹": 0.03,
    "é”™ç”¨æ ‡ç‚¹": 0.02,
    "ä¸»è¯­ä¸æ˜": 0.88,
    "è°“è¯­æ®‹ç¼º": 0.05,
    "å®¾è¯­æ®‹ç¼º": 0.05,
    "å…¶ä»–æˆåˆ†æ®‹ç¼º": 0.75,
    "ä¸»è¯­å¤šä½™": 0.02,
    "è™šè¯å¤šä½™": 0.02,
    "å…¶ä»–æˆåˆ†å¤šä½™": 0.03,
    "è¯­åºä¸å½“": 0.05,
    "åŠ¨å®¾æ­é…ä¸å½“": 0.03,
    "å…¶ä»–æ­é…ä¸å½“": 0.05
  }""" if include_probability else "") + """
}

**ç¤ºä¾‹3ï¼šæ­£ç¡®å¥å­**
è¾“å…¥ï¼š"ä»–è·‘å¾—å¾ˆå¿«ï¼Œåƒä¸€é˜µé£ä¸€æ ·ã€‚"
åˆ†æï¼šå¥å­ç»“æ„å®Œæ•´ï¼Œè¯­æ³•æ­£ç¡®ï¼Œè¡¨è¾¾æ¸…æ™°ï¼Œæ— ä»»ä½•é”™è¯¯
è¾“å‡ºï¼š{
  "sent": "ä»–è·‘å¾—å¾ˆå¿«ï¼Œåƒä¸€é˜µé£ä¸€æ ·ã€‚",
  "CourseGrainedErrorType": [],
  "FineGrainedErrorType": []""" + ("""ï¼Œ
  "coarse_probabilities": {
    "å­—ç¬¦çº§é”™è¯¯": 0.02,
    "æˆåˆ†æ®‹ç¼ºå‹é”™è¯¯": 0.03,
    "æˆåˆ†èµ˜ä½™å‹é”™è¯¯": 0.02,
    "æˆåˆ†æ­é…ä¸å½“å‹é”™è¯¯": 0.03
  },
  "fine_probabilities": {
    "é”™åˆ«å­—é”™è¯¯": 0.02,
    "ç¼ºå­—æ¼å­—": 0.02,
    "ç¼ºå°‘æ ‡ç‚¹": 0.01,
    "é”™ç”¨æ ‡ç‚¹": 0.01,
    "ä¸»è¯­ä¸æ˜": 0.02,
    "è°“è¯­æ®‹ç¼º": 0.01,
    "å®¾è¯­æ®‹ç¼º": 0.01,
    "å…¶ä»–æˆåˆ†æ®‹ç¼º": 0.02,
    "ä¸»è¯­å¤šä½™": 0.01,
    "è™šè¯å¤šä½™": 0.01,
    "å…¶ä»–æˆåˆ†å¤šä½™": 0.02,
    "è¯­åºä¸å½“": 0.02,
    "åŠ¨å®¾æ­é…ä¸å½“": 0.02,
    "å…¶ä»–æ­é…ä¸å½“": 0.02
  }""" if include_probability else "") + """
}

## é¢„æµ‹ä»»åŠ¡
è¯·è¿ç”¨ä½ çš„ä¸“ä¸šçŸ¥è¯†ï¼Œå¯¹ä»¥ä¸‹å¥å­è¿›è¡Œå‡†ç¡®çš„ç—…å¥ç±»å‹é¢„æµ‹ï¼š

"""
        
        # æ·»åŠ å¾…é¢„æµ‹çš„å¥å­
        for i, text in enumerate(texts, 1):
            base_prompt += f"{i}. \"{text}\"\n"
        
        base_prompt += f"""
## è¾“å‡ºè¦æ±‚
è¯·è¿”å›JSONæ•°ç»„æ ¼å¼çš„é¢„æµ‹ç»“æœï¼Œæ¯ä¸ªå¥å­ä¸€ä¸ªJSONå¯¹è±¡ï¼ŒåŒ…å«ï¼š
- **sent**: åŸå¥å­
- **CourseGrainedErrorType**: ç²—ç²’åº¦é”™è¯¯ç±»å‹æ•°ç»„ï¼ˆå¦‚æœæ˜¯æ­£ç¡®å¥å­åˆ™ä¸ºç©ºæ•°ç»„[]ï¼‰
- **FineGrainedErrorType**: ç»†ç²’åº¦é”™è¯¯ç±»å‹æ•°ç»„ï¼ˆå¦‚æœæ˜¯æ­£ç¡®å¥å­åˆ™ä¸ºç©ºæ•°ç»„[]ï¼‰"""

        if include_probability:
            base_prompt += """
- **coarse_probabilities**: æ¯ä¸ªç²—ç²’åº¦é”™è¯¯ç±»å‹çš„æ¦‚ç‡ï¼ˆ0-1ä¹‹é—´çš„æµ®ç‚¹æ•°ï¼‰
- **fine_probabilities**: æ¯ä¸ªç»†ç²’åº¦é”™è¯¯ç±»å‹çš„æ¦‚ç‡ï¼ˆ0-1ä¹‹é—´çš„æµ®ç‚¹æ•°ï¼‰"""

        base_prompt += """

## é‡è¦æé†’
1. **å‡†ç¡®åˆ¤æ–­**ï¼šè¿ç”¨ä¸“ä¸šè¯­è¨€å­¦çŸ¥è¯†ï¼Œä»”ç»†åˆ†ææ¯ä¸ªå¥å­çš„è¯­æ³•ç»“æ„
2. **å®Œæ•´æ ‡æ³¨**ï¼šå¦‚æœå¥å­æ²¡æœ‰é”™è¯¯ï¼ŒCourseGrainedErrorTypeå’ŒFineGrainedErrorTypeå¿…é¡»ä¸ºç©ºæ•°ç»„[]
3. **æ ¼å¼è§„èŒƒ**ï¼šåªè¿”å›æ ‡å‡†JSONæ•°ç»„ï¼Œç¡®ä¿å¯ä»¥è¢«Python json.loads()æ­£ç¡®è§£æ
4. **å±‚æ¬¡å¯¹åº”**ï¼šç¡®ä¿ç»†ç²’åº¦é”™è¯¯ç±»å‹ä¸å¯¹åº”çš„ç²—ç²’åº¦é”™è¯¯ç±»å‹åŒ¹é…
5. **ä¸“ä¸šä¸¥è°¨**ï¼šåŸºäºè¯­è¨€å­¦ç†è®ºè¿›è¡Œåˆ¤æ–­ï¼Œä¿æŒé«˜åº¦çš„ä¸“ä¸šæ€§å’Œå‡†ç¡®æ€§

è¯·å¼€å§‹åˆ†æï¼š"""
        
        return base_prompt
        
    def predict_single(self, text: str, include_probability: bool = True, 
                      max_retries: int = 3) -> Dict[str, Any]:
        """é¢„æµ‹å•ä¸ªæ–‡æœ¬çš„æ ‡ç­¾"""
        
        prompt = self.create_prediction_prompt([text], include_probability)
        
        data = {
            "model": "qwen-plus-latest",
            "input": {
                "messages": [
                    {
                        "role": "user", 
                        "content": prompt
                    }
                ]
            },
            "parameters": {
                "temperature": 0.1,
                "top_p": 0.8,
                "max_tokens": 4000
            }
        }
        
        for attempt in range(max_retries):
            try:
                response = requests.post(self.url, headers=self.headers, json=data, timeout=60)
                response.raise_for_status()
                
                result = response.json()
                
                if 'output' in result and 'text' in result['output']:
                    generated_text = result['output']['text']
                    
                    # è§£æJSONå“åº”
                    try:
                        start_idx = generated_text.find('[')
                        end_idx = generated_text.rfind(']') + 1
                        
                        if start_idx != -1 and end_idx != 0:
                            json_text = generated_text[start_idx:end_idx]
                            predictions = json.loads(json_text)
                            
                            if len(predictions) >= 1:
                                return predictions[0]  # è¿”å›ç¬¬ä¸€ä¸ªé¢„æµ‹ç»“æœ
                        else:
                            print(f"æœªæ‰¾åˆ°æœ‰æ•ˆçš„JSONæ ¼å¼ï¼Œå°è¯•ç¬¬ {attempt + 1} æ¬¡")
                            
                    except json.JSONDecodeError as e:
                        print(f"JSONè§£æå¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
                        if attempt < max_retries - 1:
                            time.sleep(2)
                            continue
                        else:
                            print("åŸå§‹å“åº”:", generated_text[:500])
                            
                else:
                    print(f"APIè°ƒç”¨å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}):", result)
                    
            except requests.exceptions.RequestException as e:
                print(f"è¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    time.sleep(5)
                    continue
            
            except Exception as e:
                print(f"å¤„ç†å‡ºé”™ (å°è¯• {attempt + 1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    time.sleep(2)
                    continue
        
        # å¦‚æœæ‰€æœ‰å°è¯•éƒ½å¤±è´¥ï¼Œè¿”å›ç©ºç»“æœ
        print(f"æ‰€æœ‰å°è¯•éƒ½å¤±è´¥ï¼Œä¸ºæ–‡æœ¬è¿”å›ç©ºé¢„æµ‹ç»“æœ: {text[:30]}...")
        return {"sent": text, "CourseGrainedErrorType": [], "FineGrainedErrorType": []}
    
    def load_existing_results(self, output_file: str) -> Dict[str, Any]:
        """åŠ è½½å·²æœ‰çš„é¢„æµ‹ç»“æœ"""
        if os.path.exists(output_file):
            try:
                with open(output_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                print(f"âœ… åŠ è½½å·²æœ‰ç»“æœï¼š{len(data['predictions'])} æ¡è®°å½•")
                return data
            except Exception as e:
                print(f"âš ï¸ åŠ è½½å·²æœ‰ç»“æœå¤±è´¥: {e}ï¼Œå°†é‡æ–°å¼€å§‹")
                return {"predictions": [], "completed_indices": []}
        else:
            return {"predictions": [], "completed_indices": []}
    
    def save_single_result(self, output_file: str, prediction: Dict[str, Any], 
                          completed_index: int):
        """ä¿å­˜å•ä¸ªé¢„æµ‹ç»“æœï¼ˆå¢é‡ä¿å­˜ï¼‰"""
        
        # åŠ è½½ç°æœ‰æ•°æ®
        if os.path.exists(output_file):
            try:
                with open(output_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
            except:
                data = {"predictions": [], "completed_indices": []}
        else:
            data = {"predictions": [], "completed_indices": []}
        
        # æ·»åŠ æ–°ç»“æœ
        data["predictions"].append(prediction)
        data["completed_indices"].append(completed_index)
        data["total_completed"] = len(data["predictions"])
        data["last_update"] = time.strftime("%Y-%m-%d %H:%M:%S")
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    
    def predict_texts_incremental(self, texts: List[str], 
                                 include_probability: bool = True,
                                 output_file: str = "incremental_predictions.json",
                                 sleep_between_requests: float = 1.0) -> List[Dict[str, Any]]:
        """å¢é‡é¢„æµ‹æ–‡æœ¬åˆ—è¡¨çš„æ ‡ç­¾ï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰"""
        
        print(f"ğŸš€ å¼€å§‹å¢é‡é¢„æµ‹ {len(texts)} ä¸ªæ–‡æœ¬")
        print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {output_file}")
        print(f"â±ï¸ è¯·æ±‚é—´éš”: {sleep_between_requests}ç§’")
        
        # åŠ è½½å·²æœ‰ç»“æœ
        existing_data = self.load_existing_results(output_file)
        completed_indices = set(existing_data.get("completed_indices", []))
        all_predictions = existing_data.get("predictions", [])
        
        # åˆ›å»ºéœ€è¦é¢„æµ‹çš„ç´¢å¼•åˆ—è¡¨
        remaining_indices = [i for i in range(len(texts)) if i not in completed_indices]
        
        if not remaining_indices:
            print("âœ… æ‰€æœ‰æ–‡æœ¬éƒ½å·²é¢„æµ‹å®Œæˆï¼")
            return all_predictions
        
        print(f"ğŸ“Š å¾…å¤„ç†æ–‡æœ¬: {len(remaining_indices)} ä¸ª")
        print(f"ğŸ“Š å·²å®Œæˆæ–‡æœ¬: {len(completed_indices)} ä¸ª")
        
        # å¼€å§‹å¢é‡é¢„æµ‹
        for idx in tqdm(remaining_indices, desc="å¢é‡é¢„æµ‹è¿›åº¦"):
            text = texts[idx]
            
            print(f"\nğŸ” é¢„æµ‹ç¬¬ {idx+1}/{len(texts)} ä¸ªæ–‡æœ¬: {text[:50]}...")
            
            # é¢„æµ‹å•ä¸ªæ–‡æœ¬
            prediction = self.predict_single(text, include_probability)
            
            # æ·»åŠ åºå·
            prediction['sent_id'] = idx + 1
            prediction['original_index'] = idx
            
            # ç«‹å³ä¿å­˜ç»“æœ
            self.save_single_result(output_file, prediction, idx)
            
            # æ·»åŠ åˆ°å†…å­˜ä¸­çš„ç»“æœåˆ—è¡¨
            all_predictions.append(prediction)
            
            print(f"âœ… å·²ä¿å­˜é¢„æµ‹ç»“æœ ({len(all_predictions)}/{len(texts)})")
            
            # è¯·æ±‚é—´éš”
            if idx < remaining_indices[-1]:  # ä¸æ˜¯æœ€åä¸€ä¸ª
                time.sleep(sleep_between_requests)
        
        print(f"\nğŸ‰ å¢é‡é¢„æµ‹å®Œæˆï¼å…±å¤„ç† {len(all_predictions)} ä¸ªæ–‡æœ¬")
        print(f"ğŸ“ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        
        return all_predictions
    
    def get_prediction_status(self, output_file: str, total_texts: int):
        """æŸ¥çœ‹é¢„æµ‹è¿›åº¦çŠ¶æ€"""
        if not os.path.exists(output_file):
            print(f"âŒ æ–‡ä»¶ {output_file} ä¸å­˜åœ¨")
            return
        
        try:
            with open(output_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            completed = len(data.get("predictions", []))
            progress = completed / total_texts * 100 if total_texts > 0 else 0
            
            print(f"\nğŸ“Š é¢„æµ‹è¿›åº¦çŠ¶æ€:")
            print(f"   æ€»æ–‡æœ¬æ•°: {total_texts}")
            print(f"   å·²å®Œæˆ: {completed}")
            print(f"   è¿›åº¦: {progress:.1f}%")
            print(f"   æœ€åæ›´æ–°: {data.get('last_update', 'æœªçŸ¥')}")
            
            if completed < total_texts:
                print(f"   å‰©ä½™: {total_texts - completed} ä¸ª")
            else:
                print(f"   âœ… å…¨éƒ¨å®Œæˆï¼")
                
        except Exception as e:
            print(f"âŒ è¯»å–çŠ¶æ€å¤±è´¥: {e}")
    
    def resume_prediction(self, texts: List[str], output_file: str):
        """æ–­ç‚¹ç»­ä¼ é¢„æµ‹"""
        print(f"ğŸ”„ å°è¯•ä» {output_file} æ¢å¤é¢„æµ‹...")
        
        # æ£€æŸ¥å½“å‰çŠ¶æ€
        self.get_prediction_status(output_file, len(texts))
        
        # ç»§ç»­é¢„æµ‹
        return self.predict_texts_incremental(texts, output_file=output_file)

    def analyze_predictions(self, predictions: List[Dict[str, Any]]):
        """åˆ†æé¢„æµ‹ç»“æœ"""
        print(f"\nğŸ“ˆ é¢„æµ‹ç»“æœåˆ†æ:")
        print(f"   æ€»é¢„æµ‹æ•°: {len(predictions)}")
        
        # ç»Ÿè®¡å„ç±»é”™è¯¯ç±»å‹
        coarse_counts = {}
        fine_counts = {}
        
        for pred in predictions:
            for coarse_type in pred.get("CourseGrainedErrorType", []):
                coarse_counts[coarse_type] = coarse_counts.get(coarse_type, 0) + 1
            
            for fine_type in pred.get("FineGrainedErrorType", []):
                fine_counts[fine_type] = fine_counts.get(fine_type, 0) + 1
        
        print(f"\nç²—ç²’åº¦é”™è¯¯ç±»å‹ç»Ÿè®¡:")
        for error_type, count in coarse_counts.items():
            print(f"   {error_type}: {count}")
        
        print(f"\nç»†ç²’åº¦é”™è¯¯ç±»å‹ç»Ÿè®¡:")
        for error_type, count in fine_counts.items():
            print(f"   {error_type}: {count}")


def load_texts_from_json(file_path: str):
    """ä»JSONæ–‡ä»¶åŠ è½½æ–‡æœ¬æ•°æ®"""
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # æå–sentå­—æ®µ
    texts = [item['sent'] for item in data]
    print(f"âœ… ä» {file_path} åŠ è½½äº† {len(texts)} ä¸ªæ–‡æœ¬")
    return texts

def main_with_file():
    """ä»æ–‡ä»¶åŠ è½½æ•°æ®çš„ä¸»å‡½æ•°"""
    
    api_key = "sk-abd72eb342014765b8e401394e870ba6"
    predictor = QwenLabelPredictor(api_key)
    
    # ä»JSONæ–‡ä»¶åŠ è½½æ–‡æœ¬
    input_file = "/mnt/cfs/huangzhiwei/NLP-WED0910/qwen3-256b-data/data_predict_again.json"  # ä½ çš„è¾“å…¥æ–‡ä»¶
    output_file = "/mnt/cfs/huangzhiwei/NLP-WED0910/qwen3-256b-data/val_1.json"  # è¾“å‡ºæ–‡ä»¶
    
    # åŠ è½½æ•°æ®
    texts = load_texts_from_json(input_file)
    
    # æ£€æŸ¥å½“å‰çŠ¶æ€
    predictor.get_prediction_status(output_file, len(texts))
    
    # å¼€å§‹æˆ–æ¢å¤å¢é‡é¢„æµ‹
    predictions = predictor.predict_texts_incremental(
        texts=texts,
        include_probability=True,
        output_file=output_file,
        sleep_between_requests=1.0  # æ¯æ¬¡è¯·æ±‚é—´éš”1ç§’
    )
    
    # åˆ†æç»“æœ
    predictor.analyze_predictions(predictions)
    
    print(f"\nâœ… å¢é‡é¢„æµ‹å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ° {output_file}")

if __name__ == "__main__":
    main_with_file()