# æœ¬åœ°BERTæ¨¡å‹åˆ†è¯éªŒè¯ä»£ç ï¼ˆæ”¯æŒJSONæ–‡ä»¶æ‰¹é‡å¤„ç†ï¼‰
from transformers import AutoTokenizer, BertTokenizer
import torch
import json
import os
from typing import List, Dict, Any
from datetime import datetime

class BertTokenizerTest:
    def __init__(self, model_path: str, tokenizer_type: str = "auto"):
        """
        åˆå§‹åŒ–BERT Tokenizeræµ‹è¯•å™¨
        
        Args:
            model_path: æœ¬åœ°æ¨¡å‹è·¯å¾„æˆ–Hugging Faceæ¨¡å‹åç§°
            tokenizer_type: tokenizerç±»å‹ ("auto", "bert")
        """
        self.model_path = model_path
        
        try:
            if tokenizer_type == "auto":
                self.tokenizer = AutoTokenizer.from_pretrained(model_path)
            elif tokenizer_type == "bert":
                self.tokenizer = BertTokenizer.from_pretrained(model_path)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„tokenizerç±»å‹: {tokenizer_type}")
                
            print(f"âœ… æˆåŠŸåŠ è½½tokenizer: {model_path}")
            print(f"ğŸ“Š è¯æ±‡è¡¨å¤§å°: {self.tokenizer.vocab_size}")
            
        except Exception as e:
            raise Exception(f"åŠ è½½tokenizerå¤±è´¥: {str(e)}")
    
    def test_tokenization(self, text: str, verbose: bool = True) -> Dict[str, Any]:
        """
        æµ‹è¯•åˆ†è¯å’Œç¼–è§£ç è¿‡ç¨‹
        
        Args:
            text: è¦æµ‹è¯•çš„æ–‡æœ¬
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
            
        Returns:
            åŒ…å«æµ‹è¯•ç»“æœçš„å­—å…¸
        """
        try:
            # 1. åˆ†è¯ (tokenize)
            tokens = self.tokenizer.tokenize(text)
            
            # 2. è½¬æ¢ä¸ºtoken IDs (encode)
            token_ids = self.tokenizer.encode(text, add_special_tokens=True)
            
            # 3. ä¸æ·»åŠ ç‰¹æ®Štokensçš„ç¼–ç 
            token_ids_no_special = self.tokenizer.encode(text, add_special_tokens=False)
            
            # 4. è§£ç  (decode)
            decoded_text = self.tokenizer.decode(token_ids, skip_special_tokens=True)
            decoded_text_no_special = self.tokenizer.decode(token_ids_no_special, skip_special_tokens=False)
            
            # 5. æ£€æŸ¥ä¸€è‡´æ€§
            is_consistent = text.strip() == decoded_text.strip()
            is_consistent_no_special = text.strip() == decoded_text_no_special.strip()
            
            # 6. è·å–ç‰¹æ®Štokens
            special_tokens = []
            if hasattr(self.tokenizer, 'cls_token') and self.tokenizer.cls_token:
                special_tokens.append(f"CLS: {self.tokenizer.cls_token}")
            if hasattr(self.tokenizer, 'sep_token') and self.tokenizer.sep_token:
                special_tokens.append(f"SEP: {self.tokenizer.sep_token}")
            if hasattr(self.tokenizer, 'pad_token') and self.tokenizer.pad_token:
                special_tokens.append(f"PAD: {self.tokenizer.pad_token}")
            if hasattr(self.tokenizer, 'unk_token') and self.tokenizer.unk_token:
                special_tokens.append(f"UNK: {self.tokenizer.unk_token}")
            
            result = {
                "original_text": text,
                "tokens": tokens,
                "token_ids": token_ids,
                "token_ids_no_special": token_ids_no_special,
                "decoded_text": decoded_text,
                "decoded_text_no_special": decoded_text_no_special,
                "is_consistent": is_consistent,
                "is_consistent_no_special": is_consistent_no_special,
                "special_tokens": special_tokens,
                "vocab_size": self.tokenizer.vocab_size
            }
            
            if verbose:
                self._print_result(result)
            
            return result
            
        except Exception as e:
            error_result = {
                "original_text": text,
                "error": str(e),
                "success": False
            }
            if verbose:
                print(f"âŒ åˆ†è¯æµ‹è¯•å¤±è´¥: {str(e)}")
            return error_result
    
    def _print_result(self, result: Dict[str, Any]):
        """æ‰“å°æµ‹è¯•ç»“æœ"""
        print(f"ğŸ“ åŸå§‹æ–‡æœ¬: {result['original_text']}")
        print(f"ğŸ”¤ åˆ†è¯ç»“æœ: {result['tokens']}")
        print(f"ğŸ”¢ Token IDs (å«ç‰¹æ®Štokens): {result['token_ids']}")
        print(f"ğŸ”¢ Token IDs (ä¸å«ç‰¹æ®Štokens): {result['token_ids_no_special']}")
        print(f"ğŸ”„ è§£ç æ–‡æœ¬ (å«ç‰¹æ®Štokens): '{result['decoded_text']}'")
        print(f"ğŸ”„ è§£ç æ–‡æœ¬ (ä¸å«ç‰¹æ®Štokens): '{result['decoded_text_no_special']}'")
        print(f"âœ… ä¸€è‡´æ€§æ£€æŸ¥ (å«ç‰¹æ®Štokens): {'é€šè¿‡' if result['is_consistent'] else 'å¤±è´¥'}")
        print(f"âœ… ä¸€è‡´æ€§æ£€æŸ¥ (ä¸å«ç‰¹æ®Štokens): {'é€šè¿‡' if result['is_consistent_no_special'] else 'å¤±è´¥'}")
        print(f"ğŸ¯ ç‰¹æ®Štokens: {', '.join(result['special_tokens'])}")
    
    def process_json_file(self, json_file_path: str, verbose: bool = False, max_samples: int = None) -> List[Dict[str, Any]]:
        """
        å¤„ç†JSONæ–‡ä»¶ä¸­çš„å¥å­è¿›è¡Œåˆ†è¯æµ‹è¯•
        
        Args:
            json_file_path: JSONæ–‡ä»¶è·¯å¾„
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
            max_samples: æœ€å¤§å¤„ç†æ ·æœ¬æ•°ï¼ŒNoneè¡¨ç¤ºå¤„ç†å…¨éƒ¨
            
        Returns:
            æµ‹è¯•ç»“æœåˆ—è¡¨
        """
        try:
            # è¯»å–JSONæ–‡ä»¶
            print(f"ğŸ“‚ æ­£åœ¨è¯»å–JSONæ–‡ä»¶: {json_file_path}")
            with open(json_file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            if not isinstance(data, list):
                raise ValueError("JSONæ–‡ä»¶åº”åŒ…å«ä¸€ä¸ªåˆ—è¡¨")
            
            print(f"ğŸ“Š æ€»å…±æœ‰ {len(data)} æ¡æ•°æ®")
            
            # é™åˆ¶å¤„ç†æ•°é‡
            if max_samples:
                data = data[:max_samples]
                print(f"ğŸ”¢ é™åˆ¶å¤„ç†å‰ {max_samples} æ¡æ•°æ®")
            
            results = []
            failed_count = 0
            
            for i, item in enumerate(data, 1):
                if 'sent' not in item:
                    print(f"âš ï¸  ç¬¬ {i} æ¡æ•°æ®ç¼ºå°‘'sent'å­—æ®µï¼Œè·³è¿‡")
                    continue
                
                sent_text = item['sent']
                sent_id = item.get('sent_id', f'unknown_{i}')
                
                if verbose or i % 50 == 0:  # æ¯50æ¡æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
                    print(f"\nğŸ”„ å¤„ç†ç¬¬ {i}/{len(data)} æ¡ (ID: {sent_id})")
                    if verbose:
                        print(f"ğŸ“ å¥å­: {sent_text}")
                
                # è¿›è¡Œåˆ†è¯æµ‹è¯•
                test_result = self.test_tokenization(sent_text, verbose=verbose)
                
                # æ·»åŠ åŸå§‹æ•°æ®ä¿¡æ¯
                result = {
                    "sent_id": sent_id,
                    "original_item": item,
                    "tokenization_test": test_result,
                    "index": i
                }
                
                results.append(result)
                
                if 'error' in test_result:
                    failed_count += 1
                    if verbose:
                        print(f"âŒ åˆ†è¯æµ‹è¯•å¤±è´¥")
                elif verbose:
                    consistent_no_special = test_result.get('is_consistent_no_special', False)
                    status = "âœ… é€šè¿‡" if consistent_no_special else "âŒ ä¸ä¸€è‡´"
                    print(f"ç»“æœ: {status}")
            
            # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
            total = len(results)
            successful = sum(1 for r in results if 'error' not in r["tokenization_test"])
            consistent = sum(1 for r in results if 'error' not in r["tokenization_test"] and r["tokenization_test"].get("is_consistent_no_special", False))
            consistent_with_special = sum(1 for r in results if 'error' not in r["tokenization_test"] and r["tokenization_test"].get("is_consistent", False))
            
            print(f"\n=== å¤„ç†å®Œæˆç»Ÿè®¡ ===")
            print(f"ğŸ“Š æ€»å¤„ç†æ•°: {total}")
            print(f"âœ… åˆ†è¯æˆåŠŸæ•°: {successful}")
            print(f"âŒ åˆ†è¯å¤±è´¥æ•°: {failed_count}")
            print(f"ğŸ¯ ä¸€è‡´æ€§é€šè¿‡ (ä¸å«ç‰¹æ®Štokens): {consistent}/{successful}")
            print(f"ğŸ¯ ä¸€è‡´æ€§é€šè¿‡ (å«ç‰¹æ®Štokens): {consistent_with_special}/{successful}")
            
            return results
            
        except Exception as e:
            print(f"âŒ å¤„ç†JSONæ–‡ä»¶å¤±è´¥: {str(e)}")
            return []
    
    def save_results(self, results: List[Dict[str, Any]], output_path: str = None):
        """
        ä¿å­˜æµ‹è¯•ç»“æœåˆ°JSONæ–‡ä»¶
        
        Args:
            results: æµ‹è¯•ç»“æœåˆ—è¡¨
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ŒNoneåˆ™è‡ªåŠ¨ç”Ÿæˆ
        """
        if not output_path:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = f"bert_tokenization_results_{timestamp}.json"
        
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
        except Exception as e:
            print(f"âŒ ä¿å­˜ç»“æœå¤±è´¥: {str(e)}")
    
    def generate_summary_report(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        ç”Ÿæˆæµ‹è¯•æ‘˜è¦æŠ¥å‘Š
        
        Args:
            results: æµ‹è¯•ç»“æœåˆ—è¡¨
            
        Returns:
            æ‘˜è¦æŠ¥å‘Šå­—å…¸
        """
        total = len(results)
        if total == 0:
            return {"error": "æ²¡æœ‰æµ‹è¯•ç»“æœ"}
        
        successful = [r for r in results if 'error' not in r["tokenization_test"]]
        failed = [r for r in results if 'error' in r["tokenization_test"]]
        
        consistent_no_special = [r for r in successful if r["tokenization_test"].get("is_consistent_no_special", False)]
        inconsistent_no_special = [r for r in successful if not r["tokenization_test"].get("is_consistent_no_special", False)]
        
        # åˆ†æä¸ä¸€è‡´çš„å¥å­
        problematic_sentences = []
        for result in inconsistent_no_special:
            test_result = result["tokenization_test"]
            problematic_sentences.append({
                "sent_id": result["sent_id"],
                "original": test_result["original_text"],
                "decoded": test_result["decoded_text_no_special"],
                "tokens": test_result["tokens"],
                "error_types": result["original_item"].get("CourseGrainedErrorType", []),
                "fine_error_types": result["original_item"].get("FineGrainedErrorType", [])
            })
        
        summary = {
            "total_samples": total,
            "successful_tokenization": len(successful),
            "failed_tokenization": len(failed),
            "consistent_samples": len(consistent_no_special),
            "inconsistent_samples": len(inconsistent_no_special),
            "consistency_rate": len(consistent_no_special) / len(successful) if successful else 0,
            "problematic_sentences": problematic_sentences[:10],  # åªæ˜¾ç¤ºå‰10ä¸ªé—®é¢˜å¥å­
            "problematic_count": len(problematic_sentences)
        }
        
        return summary
    
    def batch_test(self, texts: List[str]) -> List[Dict[str, Any]]:
        """
        æ‰¹é‡æµ‹è¯•å¤šä¸ªæ–‡æœ¬
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            
        Returns:
            æµ‹è¯•ç»“æœåˆ—è¡¨
        """
        results = []
        for i, text in enumerate(texts, 1):
            print(f"\n=== æµ‹è¯• {i}/{len(texts)} ===")
            result = self.test_tokenization(text)
            results.append(result)
        return results

def test_bert_tokenization_from_json():
    """
    ä»JSONæ–‡ä»¶æµ‹è¯•BERTåˆ†è¯çš„ä¸»å‡½æ•°
    """
    # JSONæ–‡ä»¶è·¯å¾„ - ä¿®æ”¹ä¸ºä½ çš„JSONæ–‡ä»¶è·¯å¾„
    JSON_FILE_PATH = "/mnt/cfs/huangzhiwei/NLP-WED0910/datas/train.json"
    
    # ä½ å¯ä»¥ä¿®æ”¹è¿™é‡Œçš„æ¨¡å‹è·¯å¾„
    model_options = [
        # "bert-base-chinese",           # ä¸­æ–‡BERT
        # "bert-base-uncased",          # è‹±æ–‡BERT
        # "hfl/chinese-bert-wwm-ext",   # ä¸­æ–‡BERT WWM
        # æˆ–è€…ä½ çš„æœ¬åœ°æ¨¡å‹è·¯å¾„
        # "/path/to/your/local/bert/model"
        "/mnt/cfs/huangzhiwei/NLP-WED0910/projects/models/bge-large-zh-v1.5"
    ]
    
    # é€‰æ‹©ä¸€ä¸ªæ¨¡å‹è¿›è¡Œæµ‹è¯•
    model_path = model_options[0]  # é»˜è®¤ä½¿ç”¨ä¸­æ–‡BERT
    
    try:
        # åˆ›å»ºæµ‹è¯•å™¨
        print(f"ğŸš€ æ­£åœ¨åŠ è½½æ¨¡å‹: {model_path}")
        tester = BertTokenizerTest(model_path)
        
        print(f"\n=== BERT JSONæ–‡ä»¶åˆ†è¯ä¸€è‡´æ€§æµ‹è¯• ===")
        print(f"ğŸ·ï¸  æ¨¡å‹: {model_path}")
        print(f"ğŸ“‚ æ–‡ä»¶: {JSON_FILE_PATH}")
        
        # å¤„ç†JSONæ–‡ä»¶
        results = tester.process_json_file(
            json_file_path=JSON_FILE_PATH,
            verbose=False,  # è®¾ä¸ºTrueå¯çœ‹è¯¦ç»†ä¿¡æ¯
            max_samples=100  # é™åˆ¶å¤„ç†å‰100æ¡ï¼Œå¯ä»¥ä¿®æ”¹æˆ–è®¾ä¸ºNoneå¤„ç†å…¨éƒ¨
        )
        
        if results:
            # ç”Ÿæˆæ‘˜è¦æŠ¥å‘Š
            summary = tester.generate_summary_report(results)
            
            print(f"\n=== è¯¦ç»†æµ‹è¯•æ‘˜è¦ ===")
            print(f"ğŸ“Š æ€»æ ·æœ¬æ•°: {summary['total_samples']}")
            print(f"âœ… åˆ†è¯æˆåŠŸ: {summary['successful_tokenization']}")
            print(f"âŒ åˆ†è¯å¤±è´¥: {summary['failed_tokenization']}")
            print(f"ğŸ¯ ä¸€è‡´æ€§é€šè¿‡: {summary['consistent_samples']}")
            print(f"âš ï¸  ä¸€è‡´æ€§å¤±è´¥: {summary['inconsistent_samples']}")
            print(f"ğŸ“ˆ ä¸€è‡´æ€§æˆåŠŸç‡: {summary['consistency_rate']:.2%}")
            
            # ä¿å­˜ç»“æœ
            tester.save_results(results)
            
            # æ˜¾ç¤ºé—®é¢˜å¥å­
            if summary['problematic_sentences']:
                print(f"\n=== å‰{len(summary['problematic_sentences'])}ä¸ªåˆ†è¯ä¸ä¸€è‡´çš„å¥å­ ===")
                for i, item in enumerate(summary['problematic_sentences'], 1):
                    print(f"\n{i}. ID {item['sent_id']}:")
                    print(f"   åŸæ–‡: {item['original']}")
                    print(f"   è§£ç : {item['decoded']}")
                    print(f"   åˆ†è¯: {item['tokens']}")
                    if item['error_types']:
                        print(f"   é”™è¯¯ç±»å‹: {', '.join(item['error_types'])}")
                
                if summary['problematic_count'] > len(summary['problematic_sentences']):
                    print(f"\n... è¿˜æœ‰ {summary['problematic_count'] - len(summary['problematic_sentences'])} ä¸ªé—®é¢˜å¥å­ï¼Œè¯¦è§ä¿å­˜çš„ç»“æœæ–‡ä»¶")
            else:
                print("\nğŸ‰ æ‰€æœ‰å¥å­åˆ†è¯éƒ½ä¸€è‡´ï¼")
        
        return results
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        print("ğŸ’¡ æç¤º: è¯·æ£€æŸ¥æ¨¡å‹è·¯å¾„å’ŒJSONæ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®")
        print("ğŸ’¡ å®‰è£…å‘½ä»¤: pip install transformers torch")
        return None

def test_bert_tokenization():
    """
    æµ‹è¯•BERTåˆ†è¯çš„ä¸»å‡½æ•°ï¼ˆå•å¥æµ‹è¯•ï¼‰
    """
    # ä½ å¯ä»¥ä¿®æ”¹è¿™é‡Œçš„æ¨¡å‹è·¯å¾„
    # å¯ä»¥æ˜¯æœ¬åœ°è·¯å¾„ï¼Œä¹Ÿå¯ä»¥æ˜¯Hugging Faceæ¨¡å‹åç§°
    model_options = [
        # "bert-base-chinese",           # ä¸­æ–‡BERT
        # "bert-base-uncased",          # è‹±æ–‡BERT
        # "hfl/chinese-bert-wwm-ext",   # ä¸­æ–‡BERT WWM
        # æˆ–è€…ä½ çš„æœ¬åœ°æ¨¡å‹è·¯å¾„
        # "/path/to/your/local/bert/model"
        '/mnt/cfs/huangzhiwei/NLP-WED0910/projects/models/bge-large-zh-v1.5'
    ]
    
    # é€‰æ‹©ä¸€ä¸ªæ¨¡å‹è¿›è¡Œæµ‹è¯•
    model_path = model_options[0]  # é»˜è®¤ä½¿ç”¨ä¸­æ–‡BERT
    
    try:
        # åˆ›å»ºæµ‹è¯•å™¨
        print(f"ğŸš€ æ­£åœ¨åŠ è½½æ¨¡å‹: {model_path}")
        tester = BertTokenizerTest(model_path)
        
        # æµ‹è¯•ç”¨ä¾‹
        test_cases = [
            "ä½ å¥½ï¼Œä¸–ç•Œï¼",
            "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•å¥å­ã€‚",
            "Hello, how are you today?",
            "æ··åˆè¯­è¨€æµ‹è¯•ï¼šHelloä¸–ç•Œ123ï¼",
            "ç‰¹æ®Šå­—ç¬¦ï¼š@#$%^&*()",
            "å¾ˆé•¿çš„å¥å­æµ‹è¯•ï¼šäººå·¥æ™ºèƒ½æŠ€æœ¯åœ¨è¿‘å¹´æ¥å–å¾—äº†æ˜¾è‘—çš„è¿›å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸã€‚",
            "[UNK]æµ‹è¯•æœªçŸ¥è¯æ±‡",
        ]
        
        print(f"\n=== BERTæœ¬åœ°åˆ†è¯ä¸€è‡´æ€§æµ‹è¯• ===")
        print(f"ğŸ·ï¸  æ¨¡å‹: {model_path}")
        
        # æ‰¹é‡æµ‹è¯•
        results = tester.batch_test(test_cases)
        
        # ç»Ÿè®¡ç»“æœ
        total_tests = len(results)
        consistent_with_special = sum(1 for r in results if r.get('is_consistent', False))
        consistent_no_special = sum(1 for r in results if r.get('is_consistent_no_special', False))
        
        print(f"\n=== æµ‹è¯•æ€»ç»“ ===")
        print(f"ğŸ“Š æ€»æµ‹è¯•æ•°: {total_tests}")
        print(f"âœ… ä¸€è‡´æ€§é€šè¿‡ (å«ç‰¹æ®Štokens): {consistent_with_special}/{total_tests}")
        print(f"âœ… ä¸€è‡´æ€§é€šè¿‡ (ä¸å«ç‰¹æ®Štokens): {consistent_no_special}/{total_tests}")
        
        return results
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        print("ğŸ’¡ æç¤º: è¯·æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦æ­£ç¡®ï¼Œæˆ–ç¡®ä¿å·²å®‰è£…transformersåº“")
        print("ğŸ’¡ å®‰è£…å‘½ä»¤: pip install transformers torch")
        return None

# é¢å¤–çš„å®ç”¨å‡½æ•°
def compare_tokenizers(text: str, model_paths: List[str]):
    """
    æ¯”è¾ƒä¸åŒtokenizerçš„åˆ†è¯ç»“æœ
    
    Args:
        text: è¦æ¯”è¾ƒçš„æ–‡æœ¬
        model_paths: æ¨¡å‹è·¯å¾„åˆ—è¡¨
    """
    print(f"ğŸ” æ¯”è¾ƒä¸åŒtokenizerå¯¹æ–‡æœ¬çš„åˆ†è¯ç»“æœ")
    print(f"ğŸ“ æµ‹è¯•æ–‡æœ¬: {text}")
    print("=" * 60)
    
    for model_path in model_paths:
        try:
            print(f"\nğŸ·ï¸  æ¨¡å‹: {model_path}")
            tester = BertTokenizerTest(model_path)
            result = tester.test_tokenization(text, verbose=False)
            
            print(f"ğŸ”¤ åˆ†è¯: {result['tokens']}")
            print(f"ğŸ”¢ Tokenæ•°é‡: {len(result['tokens'])}")
            print(f"âœ… ä¸€è‡´æ€§: {'é€šè¿‡' if result['is_consistent_no_special'] else 'å¤±è´¥'}")
            
        except Exception as e:
            print(f"âŒ {model_path}: åŠ è½½å¤±è´¥ - {str(e)}")

if __name__ == "__main__":
    # é€‰æ‹©æµ‹è¯•æ¨¡å¼
    print("è¯·é€‰æ‹©æµ‹è¯•æ¨¡å¼:")
    print("1. å•å¥æµ‹è¯•")
    print("2. JSONæ–‡ä»¶æ‰¹é‡æµ‹è¯•")
    print("3. æ¯”è¾ƒä¸åŒæ¨¡å‹")
    
    choice = input("è¯·è¾“å…¥é€‰æ‹© (1, 2 æˆ– 3): ").strip()
    
    if choice == "1":
        test_bert_tokenization()
    elif choice == "2":
        test_bert_tokenization_from_json()
    elif choice == "3":
        # æ¯”è¾ƒä¸åŒæ¨¡å‹çš„åˆ†è¯ç»“æœ
        compare_tokenizers(
            "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•å¥å­",
            ["bert-base-chinese", "hfl/chinese-bert-wwm-ext"]
        )
    else:
        print("æ— æ•ˆé€‰æ‹©ï¼Œé»˜è®¤æ‰§è¡Œå•å¥æµ‹è¯•")
        test_bert_tokenization()