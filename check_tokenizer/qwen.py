# Qwen API åˆ†è¯éªŒè¯ä»£ç ï¼ˆæ”¯æŒJSONæ–‡ä»¶æ‰¹é‡å¤„ç†ï¼‰
import requests
import json
import os
import time
from typing import Optional, List, Dict, Any
from datetime import datetime

class QwenTokenizerTest:
    def __init__(self, api_key: str, base_url: str = "https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation"):
        """
        åˆå§‹åŒ–Qwen APIè°ƒç”¨å™¨
        
        Args:
            api_key: ä½ çš„APIå¯†é’¥
            base_url: APIåŸºç¡€URL
        """
        self.api_key = api_key
        self.base_url = base_url
        self.headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
    
    def test_tokenization_via_api(self, text: str, model: str = "qwen-turbo") -> dict:
        """
        é€šè¿‡APIè°ƒç”¨æµ‹è¯•åˆ†è¯
        
        Args:
            text: è¦æµ‹è¯•çš„æ–‡æœ¬
            model: æ¨¡å‹åç§°
            
        Returns:
            åŒ…å«æµ‹è¯•ç»“æœçš„å­—å…¸
        """
        # æ„é€ è¯·æ±‚ï¼Œè¦æ±‚æ¨¡å‹è¿”å›åˆ†è¯ä¿¡æ¯
        prompt = f"""è¯·å¯¹ä»¥ä¸‹æ–‡æœ¬è¿›è¡Œåˆ†è¯å¤„ç†ï¼Œå¹¶è¿”å›ä»¥ä¸‹ä¿¡æ¯ï¼š
1. åŸå§‹æ–‡æœ¬
2. åˆ†è¯åçš„tokensåˆ—è¡¨
3. å¯¹tokensè¿›è¡Œdecodeåçš„æ–‡æœ¬

æ–‡æœ¬ï¼š{text}

è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¿”å›ï¼š
åŸå§‹æ–‡æœ¬: [åŸå§‹æ–‡æœ¬]
åˆ†è¯tokens: [tokenåˆ—è¡¨]
è§£ç åæ–‡æœ¬: [è§£ç æ–‡æœ¬]
"""
        
        payload = {
            "model": model,
            "input": {
                "messages": [
                    {
                        "role": "user",
                        "content": prompt
                    }
                ]
            },
            "parameters": {
                "result_format": "message"
            }
        }
        
        try:
            response = requests.post(self.base_url, headers=self.headers, json=payload)
            response.raise_for_status()
            
            result = response.json()
            output_text = result["output"]["choices"][0]["message"]["content"]
            
            # è§£æè¿”å›çš„ç»“æœ
            lines = output_text.strip().split('\n')
            parsed_result = {}
            
            for line in lines:
                if line.startswith("åŸå§‹æ–‡æœ¬:"):
                    parsed_result["original"] = line.split(":", 1)[1].strip()
                elif line.startswith("è§£ç åæ–‡æœ¬:"):
                    parsed_result["decoded"] = line.split(":", 1)[1].strip()
            
            # æ£€æŸ¥æ˜¯å¦ä¸€è‡´
            is_consistent = parsed_result.get("original", "").strip() == parsed_result.get("decoded", "").strip()
            
            return {
                "success": True,
                "original_text": text,
                "api_response": output_text,
                "parsed_result": parsed_result,
                "is_consistent": is_consistent,
                "error": None
            }
            
        except Exception as e:
            return {
                "success": False,
                "original_text": text,
                "api_response": None,
                "parsed_result": None,
                "is_consistent": False,
                "error": str(e)
            }
    
    def process_json_file(self, json_file_path: str, model: str = "qwen-plus-latest", max_samples: int = None) -> List[Dict[str, Any]]:
        """
        å¤„ç†JSONæ–‡ä»¶ä¸­çš„å¥å­è¿›è¡Œåˆ†è¯æµ‹è¯•
        
        Args:
            json_file_path: JSONæ–‡ä»¶è·¯å¾„
            model: æ¨¡å‹åç§°
            max_samples: æœ€å¤§å¤„ç†æ ·æœ¬æ•°ï¼ŒNoneè¡¨ç¤ºå¤„ç†å…¨éƒ¨
            
        Returns:
            æµ‹è¯•ç»“æœåˆ—è¡¨
        """
        try:
            # è¯»å–JSONæ–‡ä»¶
            print(f"ğŸ“‚ æ­£åœ¨è¯»å–JSONæ–‡ä»¶: {json_file_path}")
            with open(json_file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            if not isinstance(data, list):
                raise ValueError("JSONæ–‡ä»¶åº”åŒ…å«ä¸€ä¸ªåˆ—è¡¨")
            
            print(f"ğŸ“Š æ€»å…±æœ‰ {len(data)} æ¡æ•°æ®")
            
            # é™åˆ¶å¤„ç†æ•°é‡
            if max_samples:
                data = data[:max_samples]
                print(f"ğŸ”¢ é™åˆ¶å¤„ç†å‰ {max_samples} æ¡æ•°æ®")
            
            results = []
            failed_count = 0
            
            for i, item in enumerate(data, 1):
                if 'sent' not in item:
                    print(f"âš ï¸  ç¬¬ {i} æ¡æ•°æ®ç¼ºå°‘'sent'å­—æ®µï¼Œè·³è¿‡")
                    continue
                
                sent_text = item['sent']
                sent_id = item.get('sent_id', f'unknown_{i}')
                
                print(f"\nğŸ”„ å¤„ç†ç¬¬ {i}/{len(data)} æ¡ (ID: {sent_id})")
                print(f"ğŸ“ å¥å­: {sent_text}")
                
                # è¿›è¡Œåˆ†è¯æµ‹è¯•
                test_result = self.test_tokenization_via_api(sent_text, model)
                
                # æ·»åŠ åŸå§‹æ•°æ®ä¿¡æ¯
                result = {
                    "sent_id": sent_id,
                    "original_item": item,
                    "tokenization_test": test_result,
                    "index": i
                }
                
                results.append(result)
                
                if test_result["success"]:
                    status = "âœ… é€šè¿‡" if test_result["is_consistent"] else "âŒ ä¸ä¸€è‡´"
                    print(f"ç»“æœ: {status}")
                else:
                    failed_count += 1
                    print(f"âŒ APIè°ƒç”¨å¤±è´¥: {test_result['error']}")
                
                # é¿å…APIè°ƒç”¨è¿‡å¿«
                time.sleep(1)  # ç­‰å¾…1ç§’
            
            # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
            total = len(results)
            successful = sum(1 for r in results if r["tokenization_test"]["success"])
            consistent = sum(1 for r in results if r["tokenization_test"]["success"] and r["tokenization_test"]["is_consistent"])
            
            print(f"\n=== å¤„ç†å®Œæˆç»Ÿè®¡ ===")
            print(f"ğŸ“Š æ€»å¤„ç†æ•°: {total}")
            print(f"âœ… APIæˆåŠŸæ•°: {successful}")
            print(f"âŒ APIå¤±è´¥æ•°: {failed_count}")
            print(f"ğŸ¯ ä¸€è‡´æ€§é€šè¿‡: {consistent}/{successful}")
            
            return results
            
        except Exception as e:
            print(f"âŒ å¤„ç†JSONæ–‡ä»¶å¤±è´¥: {str(e)}")
            return []
    
    def save_results(self, results: List[Dict[str, Any]], output_path: str = None):
        """
        ä¿å­˜æµ‹è¯•ç»“æœåˆ°JSONæ–‡ä»¶
        
        Args:
            results: æµ‹è¯•ç»“æœåˆ—è¡¨
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ŒNoneåˆ™è‡ªåŠ¨ç”Ÿæˆ
        """
        if not output_path:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = f"qwen_tokenization_results_{timestamp}.json"
        
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
        except Exception as e:
            print(f"âŒ ä¿å­˜ç»“æœå¤±è´¥: {str(e)}")

def test_qwen_tokenization_from_json():
    """
    ä»JSONæ–‡ä»¶æµ‹è¯•Qwenåˆ†è¯çš„ä¸»å‡½æ•°
    """
    # ä½ éœ€è¦åœ¨è¿™é‡Œå¡«å…¥ä½ çš„APIå¯†é’¥
    API_KEY = "sk-abd72eb342014765b8e401394e870ba6"
    
    # JSONæ–‡ä»¶è·¯å¾„ - ä¿®æ”¹ä¸ºä½ çš„JSONæ–‡ä»¶è·¯å¾„
    JSON_FILE_PATH = "/mnt/cfs/huangzhiwei/NLP-WED0910/datas/train.json"
    
    # åˆ›å»ºæµ‹è¯•å™¨
    tester = QwenTokenizerTest(API_KEY)
    
    print("=== Qwen API JSONæ–‡ä»¶åˆ†è¯ä¸€è‡´æ€§æµ‹è¯• ===\n")
    
    # å¤„ç†JSONæ–‡ä»¶
    results = tester.process_json_file(
        json_file_path=JSON_FILE_PATH,
        max_samples=10  # é™åˆ¶å¤„ç†å‰10æ¡ï¼Œå¯ä»¥ä¿®æ”¹æˆ–è®¾ä¸ºNoneå¤„ç†å…¨éƒ¨
    )
    
    if results:
        # ä¿å­˜ç»“æœ
        tester.save_results(results)
        
        # æ˜¾ç¤ºé—®é¢˜å¥å­
        problematic_sentences = []
        for result in results:
            test_result = result["tokenization_test"]
            if test_result["success"] and not test_result["is_consistent"]:
                problematic_sentences.append({
                    "sent_id": result["sent_id"],
                    "original": test_result["original_text"],
                    "decoded": test_result.get("parsed_result", {}).get("decoded", ""),
                    "error_types": result["original_item"].get("CourseGrainedErrorType", []),
                    "fine_error_types": result["original_item"].get("FineGrainedErrorType", [])
                })
        
        if problematic_sentences:
            print(f"\n=== å‘ç° {len(problematic_sentences)} ä¸ªåˆ†è¯ä¸ä¸€è‡´çš„å¥å­ ===")
            for item in problematic_sentences:
                print(f"ID {item['sent_id']}:")
                print(f"  åŸæ–‡: {item['original']}")
                print(f"  è§£ç : {item['decoded']}")
                if item['error_types']:
                    print(f"  é”™è¯¯ç±»å‹: {', '.join(item['error_types'])}")
                print()
        else:
            print("\nğŸ‰ æ‰€æœ‰å¥å­åˆ†è¯éƒ½ä¸€è‡´ï¼")

def test_qwen_tokenization():
    """
    æµ‹è¯•Qwenåˆ†è¯çš„ä¸»å‡½æ•°ï¼ˆå•å¥æµ‹è¯•ï¼‰
    """
    # ä½ éœ€è¦åœ¨è¿™é‡Œå¡«å…¥ä½ çš„APIå¯†é’¥
    API_KEY = "your_api_key_here"
    
    # åˆ›å»ºæµ‹è¯•å™¨
    tester = QwenTokenizerTest(API_KEY)
    
    # æµ‹è¯•ç”¨ä¾‹
    test_cases = [
        "ä½ å¥½ï¼Œä¸–ç•Œï¼",
        "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•å¥å­ã€‚",
        "Hello, how are you today?",
        "æ··åˆè¯­è¨€æµ‹è¯•ï¼šHelloä¸–ç•Œ123ï¼",
        "ç‰¹æ®Šå­—ç¬¦ï¼š@#$%^&*()",
    ]
    
    print("=== Qwen API åˆ†è¯ä¸€è‡´æ€§æµ‹è¯• ===\n")
    
    for i, text in enumerate(test_cases, 1):
        print(f"æµ‹è¯• {i}: {text}")
        result = tester.test_tokenization_via_api(text)
        
        if result["success"]:
            print(f"âœ… APIè°ƒç”¨æˆåŠŸ")
            print(f"ğŸ“ APIè¿”å›:\n{result['api_response']}")
            print(f"ğŸ” ä¸€è‡´æ€§æ£€æŸ¥: {'âœ… é€šè¿‡' if result['is_consistent'] else 'âŒ å¤±è´¥'}")
        else:
            print(f"âŒ APIè°ƒç”¨å¤±è´¥: {result['error']}")
        
        print("-" * 50)

if __name__ == "__main__":
    # é€‰æ‹©æµ‹è¯•æ¨¡å¼
    print("è¯·é€‰æ‹©æµ‹è¯•æ¨¡å¼:")
    print("1. å•å¥æµ‹è¯•")
    print("2. JSONæ–‡ä»¶æ‰¹é‡æµ‹è¯•")
    
    choice = input("è¯·è¾“å…¥é€‰æ‹© (1 æˆ– 2): ").strip()
    
    if choice == "1":
        test_qwen_tokenization()
    elif choice == "2":
        test_qwen_tokenization_from_json()
    else:
        print("æ— æ•ˆé€‰æ‹©ï¼Œé»˜è®¤æ‰§è¡Œå•å¥æµ‹è¯•")
        test_qwen_tokenization()