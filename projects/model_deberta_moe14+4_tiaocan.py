import torch
import torch.nn as nn
import torch.nn.functional as F
import os
import random
import argparse
import numpy as np
import json
from tqdm import tqdm
from transformers import BertModel, AutoModel
from torch.optim import AdamW
from torch.utils.data import Dataset
from torch.utils.data import DataLoader
from transformers import AutoTokenizer
from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score
import time
from datetime import datetime

# ä½¿ç”¨Optunaè¿›è¡Œè´å¶æ–¯ä¼˜åŒ–
import optuna
from optuna.samplers import TPESampler
import sqlite3

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ä½ çš„åŸæœ‰ç±»å®šä¹‰ä¿æŒä¸å˜
class ErrorDetectionDataset(Dataset):
    def __init__(
            self,
            data_path,
            coarse_labels={
                "å­—ç¬¦çº§é”™è¯¯": 0,
                "æˆåˆ†æ®‹ç¼ºå‹é”™è¯¯": 1, 
                "æˆåˆ†èµ˜ä½™å‹é”™è¯¯": 2,
                "æˆåˆ†æ­é…ä¸å½“å‹é”™è¯¯": 3
            },
            fine_labels={
                "ç¼ºå­—æ¼å­—": 0,
                "é”™åˆ«å­—é”™è¯¯": 1,
                "ç¼ºå°‘æ ‡ç‚¹": 2,
                "é”™ç”¨æ ‡ç‚¹": 3,
                "ä¸»è¯­ä¸æ˜": 4,
                "è°“è¯­æ®‹ç¼º": 5,
                "å®¾è¯­æ®‹ç¼º": 6,
                "å…¶ä»–æˆåˆ†æ®‹ç¼º": 7,
                "ä¸»è¯­å¤šä½™": 8,
                "è™šè¯å¤šä½™": 9,
                "å…¶ä»–æˆåˆ†å¤šä½™": 10,
                "è¯­åºä¸å½“": 11,
                "åŠ¨å®¾æ­é…ä¸å½“": 12,
                "å…¶ä»–æ­é…ä¸å½“": 13
            }
    ):
        self.data_path = data_path
        self.coarse_labels = coarse_labels
        self.fine_labels = fine_labels
        self._get_data()
    
    def _get_data(self):
        with open(self.data_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        self.data = []
        for item in data:
            sent = item['sent']
            coarse_types = item['CourseGrainedErrorType']
            fine_types = item['FineGrainedErrorType']
            
            # æ„å»ºç²—ç²’åº¦æ ‡ç­¾ï¼ˆå¤šæ ‡ç­¾ï¼‰
            coarse_label = [0] * len(self.coarse_labels)
            for c_type in coarse_types:
                if c_type in self.coarse_labels:
                    coarse_label[self.coarse_labels[c_type]] = 1
            
            # æ„å»ºç»†ç²’åº¦æ ‡ç­¾ï¼ˆå¤šæ ‡ç­¾ï¼‰
            fine_label = [0] * len(self.fine_labels)
            for f_type in fine_types:
                if f_type in self.fine_labels:
                    fine_label[self.fine_labels[f_type]] = 1
            
            self.data.append((sent, coarse_label, fine_label, item.get('sent_id', -1)))
    
    def __len__(self):
        return len(self.data)
    
    def get_coarse_labels(self):
        return self.coarse_labels
    
    def get_fine_labels(self):
        return self.fine_labels
    
    def __getitem__(self, idx):
        return self.data[idx]


class ErrorDetectionDataLoader:
    def __init__(
        self,
        dataset,
        batch_size=16,
        max_length=128,
        shuffle=True,
        drop_last=True,
        device=None,
        tokenizer_name='/mnt/cfs/huangzhiwei/NLP-WED0910/projects/models/bge-large-zh-v1.5'
    ):
        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
        self.dataset = dataset
        self.batch_size = batch_size
        self.max_length = max_length
        self.shuffle = shuffle
        self.drop_last = drop_last
        
        if device is None:
            self.device = torch.device(
                'cuda' if torch.cuda.is_available() else 'cpu'
            )
        else:
            self.device = device
        
        self.loader = DataLoader(
            dataset=self.dataset,
            batch_size=self.batch_size,
            collate_fn=self.collate_fn,
            shuffle=self.shuffle,
            drop_last=self.drop_last
        )
    
    def collate_fn(self, data):
        sents = [item[0] for item in data]
        coarse_labels = [item[1] for item in data]
        fine_labels = [item[2] for item in data]
        sent_ids = [item[3] for item in data]
        
        # ç¼–ç æ–‡æœ¬
        encoded = self.tokenizer.batch_encode_plus(
            batch_text_or_text_pairs=sents,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt',
            return_length=True
        )
        
        input_ids = encoded['input_ids'].to(self.device)
        attention_mask = encoded['attention_mask'].to(self.device)
        
        # å¤„ç†æ ‡ç­¾
        if coarse_labels[0] == -1:
            coarse_labels = None
            fine_labels = None
        else:
            coarse_labels = torch.tensor(coarse_labels, dtype=torch.float).to(self.device)
            fine_labels = torch.tensor(fine_labels, dtype=torch.float).to(self.device)
        
        return input_ids, attention_mask, coarse_labels, fine_labels, sent_ids
    
    def __iter__(self):
        for data in self.loader:
            yield data
    
    def __len__(self):
        return len(self.loader)


class ExpertSpecializedMoEClassifier(nn.Module):
    """ä¸“å®¶ä¸“é—¨åŒ–çš„MoEåˆ†ç±»å™¨ - æ¯ä¸ªä¸“å®¶ä¸“é—¨å¤„ç†ç‰¹å®šç±»å‹çš„é”™è¯¯"""
    def __init__(
        self, 
        pretrained_model_name, 
        num_coarse_labels=4, 
        num_fine_labels=14, 
        dropout=0.2,
        expert_dim=512
    ):
        super().__init__()
        
        self.bert = AutoModel.from_pretrained(pretrained_model_name)
        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()
        
        hidden_size = self.bert.config.hidden_size
        
        # ä¸ºæ¯ä¸ªç²—ç²’åº¦ç±»åˆ«åˆ›å»ºä¸“é—¨çš„ä¸“å®¶
        self.coarse_gate = nn.Linear(hidden_size, num_coarse_labels)
        
        # æ¯ä¸ªç²—ç²’åº¦ç±»åˆ«å¯¹åº”ä¸€ä¸ªä¸“å®¶
        self.coarse_experts = nn.ModuleList([
            nn.Sequential(
                nn.Linear(hidden_size, expert_dim),
                nn.ReLU(),
                nn.Dropout(dropout),
                nn.Linear(expert_dim, hidden_size),
                nn.Dropout(dropout)
            ) for _ in range(num_coarse_labels)
        ])
        
        # ç²—ç²’åº¦åˆ†ç±»å¤´
        self.coarse_classifier = nn.Linear(hidden_size, num_coarse_labels)
        
        # ä¸ºæ¯ä¸ªç²—ç²’åº¦ç±»åˆ«çš„ç»†ç²’åº¦åˆ†ç±»åˆ›å»ºä¸“é—¨çš„åˆ†ç±»å¤´
        self.fine_classifiers = nn.ModuleList([
            nn.Linear(hidden_size, len(fine_indices)) 
            for fine_indices in [
                [0, 1, 2, 3],    # å­—ç¬¦çº§é”™è¯¯ -> 4ä¸ªç»†ç²’åº¦ç±»åˆ«
                [4, 5, 6, 7],    # æˆåˆ†æ®‹ç¼ºå‹é”™è¯¯ -> 4ä¸ªç»†ç²’åº¦ç±»åˆ«
                [8, 9, 10],      # æˆåˆ†å†—ä½™å‹é”™è¯¯ -> 3ä¸ªç»†ç²’åº¦ç±»åˆ«
                [11, 12, 13]     # æˆåˆ†æ­é…ä¸å½“å‹é”™è¯¯ -> 3ä¸ªç»†ç²’åº¦ç±»åˆ«
            ]
        ])
        
        # å®šä¹‰ç²—ç²’åº¦ç±»åˆ«å’Œå¯¹åº”çš„ç»†ç²’åº¦ç´¢å¼•çš„æ˜ å°„
        self.coarse_to_fine_indices = {
            0: [0, 1, 2, 3],    # å­—ç¬¦çº§é”™è¯¯
            1: [4, 5, 6, 7],    # æˆåˆ†æ®‹ç¼ºå‹é”™è¯¯
            2: [8, 9, 10],      # æˆåˆ†å†—ä½™å‹é”™è¯¯
            3: [11, 12, 13]     # æˆåˆ†æ­é…ä¸å½“å‹é”™è¯¯
        }
        
        self.num_fine_labels = num_fine_labels
        
    def forward(self, input_ids, attention_mask, token_type_ids=None):
        # BERTç¼–ç 
        if token_type_ids is not None:
            outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)
        else:
            outputs = self.bert(input_ids, attention_mask=attention_mask)
        
        pooled_output = outputs.last_hidden_state[:, 0, :]  # å–[CLS]çš„è¾“å‡º
        pooled_output = self.dropout(pooled_output)
        
        # ç²—ç²’åº¦Gateç½‘ç»œ
        gate_logits = self.coarse_gate(pooled_output)
        gate_probs = F.softmax(gate_logits, dim=-1)
        
        # è®¡ç®—æ¯ä¸ªä¸“å®¶çš„è¾“å‡º
        expert_outputs = []
        for i, expert in enumerate(self.coarse_experts):
            expert_output = expert(pooled_output)
            expert_outputs.append(expert_output)
        
        expert_outputs = torch.stack(expert_outputs, dim=1)  # (batch_size, num_experts, hidden_size)
        
        # åŸºäºgateæƒé‡ç»„åˆä¸“å®¶è¾“å‡º
        weighted_output = torch.sum(
            gate_probs.unsqueeze(-1) * expert_outputs, dim=1
        )  # (batch_size, hidden_size)
        
        # ç²—ç²’åº¦åˆ†ç±»
        coarse_logits = self.coarse_classifier(weighted_output)
        coarse_probs = torch.sigmoid(coarse_logits)
        
        # ç»†ç²’åº¦åˆ†ç±» - ä¸ºæ¯ä¸ªç²—ç²’åº¦ç±»åˆ«è®¡ç®—å¯¹åº”çš„ç»†ç²’åº¦é¢„æµ‹
        fine_logits_list = []
        batch_size = pooled_output.size(0)
        
        for i, (coarse_idx, fine_indices) in enumerate(self.coarse_to_fine_indices.items()):
            # ä½¿ç”¨å¯¹åº”ä¸“å®¶çš„è¾“å‡ºè¿›è¡Œç»†ç²’åº¦åˆ†ç±»
            expert_output = expert_outputs[:, i, :]  # (batch_size, hidden_size)
            fine_logits = self.fine_classifiers[i](expert_output)  # (batch_size, num_fine_for_this_coarse)
            fine_logits_list.append(fine_logits)
        
        # ç»„åˆæ‰€æœ‰ç»†ç²’åº¦é¢„æµ‹
        fine_probs = torch.zeros(batch_size, self.num_fine_labels).to(pooled_output.device)
        for i, (coarse_idx, fine_indices) in enumerate(self.coarse_to_fine_indices.items()):
            fine_logits = fine_logits_list[i]
            fine_probs_part = torch.sigmoid(fine_logits)
            fine_probs[:, fine_indices] = fine_probs_part
        
        # è®¡ç®—è´Ÿè½½å‡è¡¡æŸå¤±
        load_balance_loss = self.compute_load_balance_loss(gate_probs)
        
        return coarse_probs, fine_probs, load_balance_loss
    
    def compute_load_balance_loss(self, gate_probs):
        """è®¡ç®—è´Ÿè½½å‡è¡¡æŸå¤±"""
        expert_usage = gate_probs.mean(dim=0)
        ideal_usage = 1.0 / gate_probs.size(1)
        load_balance_loss = torch.sum((expert_usage - ideal_usage) ** 2)
        return 0.01 * load_balance_loss
    
    def apply_hierarchical_constraint(self, coarse_preds, fine_preds):
        """åº”ç”¨å±‚æ¬¡çº¦æŸ"""
        constrained_fine_preds = fine_preds.clone()
        
        for i in range(coarse_preds.size(0)):
            for coarse_idx, fine_indices in self.coarse_to_fine_indices.items():
                if coarse_preds[i, coarse_idx] == 0:
                    constrained_fine_preds[i, fine_indices] = 0
        
        return constrained_fine_preds


def calculate_metrics(labels, predictions, average='micro'):
    """è®¡ç®—å„ç§è¯„ä¼°æŒ‡æ ‡"""
    labels = np.array(labels)
    predictions = np.array(predictions)
    
    micro_f1 = f1_score(labels, predictions, average='micro')
    macro_f1 = f1_score(labels, predictions, average='macro')
    sample_acc = accuracy_score(labels, predictions)
    
    return {
        'micro_f1': micro_f1 * 100,
        'macro_f1': macro_f1 * 100,
        'accuracy': sample_acc * 100
    }


def train_single_config_for_optuna(trial, base_config, results_dir):
    """ä¸ºOptunaä¼˜åŒ–è®­ç»ƒå•ä¸ªé…ç½®ï¼Œè¿”å›å®Œæ•´çš„6ä¸ªæŒ‡æ ‡"""
    
    # ä»trialä¸­è·å–è¶…å‚æ•° - ç§»é™¤äº†åŸMoEç›¸å…³çš„å‚æ•°ï¼Œåªä¿ç•™expert_dim
    params = {
        'dropout': trial.suggest_float('dropout', 0.0, 0.4),
        'batch_size': trial.suggest_categorical('batch_size', [8, 16, 32]),
        'lr': trial.suggest_float('lr', 1e-6, 5e-5, log=True),
        'threshold': trial.suggest_float('threshold', 0.3, 0.8),
        'seed': trial.suggest_categorical('seed', [42, 3407, 2023]),
        'expert_dim': trial.suggest_categorical('expert_dim', [256, 512, 768, 1024])
    }
    
    # è®¾ç½®éšæœºç§å­
    seed = params['seed']
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    
    # åˆ›å»ºé…ç½®å¯¹è±¡
    class Config:
        pass
    
    configs = Config()
    
    # è®¾ç½®åŸºç¡€é…ç½®
    for key, value in base_config.items():
        setattr(configs, key, value)
    
    # è®¾ç½®ä¼˜åŒ–å‚æ•°
    for key, value in params.items():
        setattr(configs, key, value)
    
    # åˆ›å»ºæ£€æŸ¥ç‚¹ç›®å½•
    checkpoint_dir = os.path.join(configs.checkpoint_dir, f"optuna_trial_{trial.number}")
    os.makedirs(checkpoint_dir, exist_ok=True)
    configs.checkpoint_dir = checkpoint_dir
    
    try:
        # åŠ è½½æ•°æ®é›†
        train_dataset = ErrorDetectionDataset(configs.data_path)
        val_dataset = ErrorDetectionDataset(configs.val_data_path)    

        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_dataloader = ErrorDetectionDataLoader(
            dataset=train_dataset,
            batch_size=configs.batch_size,
            max_length=configs.max_length,
            shuffle=True,
            drop_last=True,
            device=configs.device,
            tokenizer_name=configs.model_name
        )

        val_dataloader = ErrorDetectionDataLoader(
            dataset=val_dataset,
            batch_size=configs.batch_size,
            max_length=configs.max_length,
            shuffle=False,
            drop_last=False,
            device=configs.device,
            tokenizer_name=configs.model_name
        )

        # åˆ›å»ºæ¨¡å‹ - ä½¿ç”¨æ–°çš„ExpertSpecializedMoEClassifier
        model = ExpertSpecializedMoEClassifier(
            pretrained_model_name=configs.model_name,
            num_coarse_labels=configs.num_coarse_labels,
            num_fine_labels=configs.num_fine_labels,
            dropout=configs.dropout,
            expert_dim=configs.expert_dim
        ).to(configs.device)

        # å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
        criterion = nn.BCELoss()
        optimizer = AdamW(
            filter(lambda p: p.requires_grad, model.parameters()),
            lr=configs.lr,
            weight_decay=getattr(configs, 'weight_decay', 0.01)
        )

        # åˆå§‹åŒ–æœ€ä½³éªŒè¯æ€§èƒ½å’Œ6ä¸ªæŒ‡æ ‡
        best_metrics = {
            'hierarchical_final_micro_f1': 0,
            'hierarchical_final_macro_f1': 0,
            'hierarchical_coarse_micro_f1': 0,
            'hierarchical_fine_micro_f1': 0,
            'hierarchical_coarse_macro_f1': 0,
            'hierarchical_fine_macro_f1': 0,
            'epoch': 0
        }
        
        best_val_f1 = 0
        patience_counter = 0
        
        # è®­ç»ƒå¾ªç¯
        for epoch in range(configs.epochs):
            # è®­ç»ƒé˜¶æ®µ
            model.train()
            train_loss = 0.0
            
            for input_ids, attention_mask, coarse_labels, fine_labels, sent_ids in train_dataloader:
                optimizer.zero_grad()
                
                # å‰å‘ä¼ æ’­
                coarse_probs, fine_probs, load_balance_loss = model(input_ids, attention_mask)
                
                # è®¡ç®—åˆ†ç±»æŸå¤±
                coarse_loss = criterion(coarse_probs, coarse_labels)
                fine_loss = criterion(fine_probs, fine_labels)
                classification_loss = coarse_loss + fine_loss
                
                # æ€»æŸå¤± = åˆ†ç±»æŸå¤± + è´Ÿè½½å‡è¡¡æŸå¤±
                total_loss = classification_loss + load_balance_loss
                
                # åå‘ä¼ æ’­
                total_loss.backward()
                optimizer.step()
                
                train_loss += classification_loss.item()

            # éªŒè¯é˜¶æ®µ
            model.eval()
            all_coarse_preds = []
            all_coarse_labels = []
            all_fine_labels = []
            all_constrained_fine_preds = []

            with torch.no_grad():
                for input_ids, attention_mask, coarse_labels, fine_labels, sent_ids in val_dataloader:
                    # å‰å‘ä¼ æ’­
                    coarse_probs, fine_probs, load_balance_loss = model(input_ids, attention_mask)
                    
                    # åº”ç”¨å±‚æ¬¡çº¦æŸ
                    coarse_preds = (coarse_probs > configs.threshold).float()
                    fine_preds = (fine_probs > configs.threshold).float()
                    constrained_fine_preds = model.apply_hierarchical_constraint(coarse_preds, fine_preds)
                    
                    # æ”¶é›†é¢„æµ‹ç»“æœ
                    all_coarse_preds.extend(coarse_preds.cpu().numpy())
                    all_coarse_labels.extend(coarse_labels.cpu().numpy())
                    all_constrained_fine_preds.extend(constrained_fine_preds.cpu().numpy())
                    all_fine_labels.extend(fine_labels.cpu().numpy())
            
            # è®¡ç®—éªŒè¯æŒ‡æ ‡
            val_coarse_metrics_micro = calculate_metrics(all_coarse_labels, all_coarse_preds, average='micro')
            val_coarse_metrics_macro = calculate_metrics(all_coarse_labels, all_coarse_preds, average='macro')
            val_constrained_fine_metrics_micro = calculate_metrics(all_fine_labels, all_constrained_fine_preds, average='micro')
            val_constrained_fine_metrics_macro = calculate_metrics(all_fine_labels, all_constrained_fine_preds, average='macro')
            
            # è®¡ç®—6ä¸ªæŒ‡æ ‡
            hierarchical_final_micro_f1 = (val_constrained_fine_metrics_micro['micro_f1'] + val_coarse_metrics_micro['micro_f1']) / 2
            hierarchical_final_macro_f1 = (val_constrained_fine_metrics_macro['macro_f1'] + val_coarse_metrics_macro['macro_f1']) / 2
            hierarchical_coarse_micro_f1 = val_coarse_metrics_micro['micro_f1']
            hierarchical_fine_micro_f1 = val_constrained_fine_metrics_micro['micro_f1']
            hierarchical_coarse_macro_f1 = val_coarse_metrics_macro['macro_f1']
            hierarchical_fine_macro_f1 = val_constrained_fine_metrics_macro['macro_f1']
            
            current_metrics = {
                'hierarchical_final_micro_f1': hierarchical_final_micro_f1,
                'hierarchical_final_macro_f1': hierarchical_final_macro_f1,
                'hierarchical_coarse_micro_f1': hierarchical_coarse_micro_f1,
                'hierarchical_fine_micro_f1': hierarchical_fine_micro_f1,
                'hierarchical_coarse_macro_f1': hierarchical_coarse_macro_f1,
                'hierarchical_fine_macro_f1': hierarchical_fine_macro_f1,
                'epoch': epoch + 1
            }
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºæœ€ä½³æ€§èƒ½ï¼ˆä½¿ç”¨Final micro f1ä½œä¸ºä¸»è¦æŒ‡æ ‡ï¼‰
            if hierarchical_final_micro_f1 > best_val_f1:
                best_val_f1 = hierarchical_final_micro_f1
                best_metrics = current_metrics.copy()
                patience_counter = 0
            else:
                patience_counter += 1
                
            # æ—©åœæ£€æŸ¥
            if patience_counter >= configs.patience:
                break
            
            # æŠ¥å‘Šä¸­é—´ç»“æœç»™Optunaï¼ˆç”¨äºå‰ªæï¼‰
            trial.report(hierarchical_final_micro_f1, epoch)
            
            # æ£€æŸ¥æ˜¯å¦åº”è¯¥å‰ªæ
            if trial.should_prune():
                raise optuna.exceptions.TrialPruned()
        
        # ä¿å­˜å½“å‰è¯•éªŒç»“æœåˆ°JSON
        trial_result = {
            'trial_number': trial.number,
            'params': params,
            'metrics': best_metrics,
            'status': 'completed',
            'timestamp': datetime.now().isoformat()
        }
        
        # ä¿å­˜æ¯è½®ç»“æœ
        with open(os.path.join(results_dir, f'trial_{trial.number}_result.json'), 'w', encoding='utf-8') as f:
            json.dump(trial_result, f, ensure_ascii=False, indent=2)
        
        # æ¸…ç†æ£€æŸ¥ç‚¹ç›®å½•
        import shutil
        if os.path.exists(checkpoint_dir):
            shutil.rmtree(checkpoint_dir)
        
        return best_val_f1, best_metrics
        
    except Exception as e:
        print(f"Trial {trial.number} å¤±è´¥: {str(e)}")
        
        # ä¿å­˜å¤±è´¥çš„è¯•éªŒç»“æœ
        failed_result = {
            'trial_number': trial.number,
            'params': params,
            'metrics': None,
            'status': 'failed',
            'error': str(e),
            'timestamp': datetime.now().isoformat()
        }
        
        with open(os.path.join(results_dir, f'trial_{trial.number}_result.json'), 'w', encoding='utf-8') as f:
            json.dump(failed_result, f, ensure_ascii=False, indent=2)
        
        # æ¸…ç†æ£€æŸ¥ç‚¹ç›®å½•
        import shutil
        if os.path.exists(checkpoint_dir):
            shutil.rmtree(checkpoint_dir)
        
        raise optuna.exceptions.TrialPruned()


def optuna_hyperparameter_optimization():
    """ä½¿ç”¨Optunaè¿›è¡Œè¶…å‚æ•°ä¼˜åŒ–"""
    
    # å›ºå®šå‚æ•° - è¯·æ ¹æ®ä½ çš„å®é™…è·¯å¾„ä¿®æ”¹
    base_config = {
        'model_name': '/mnt/cfs/huangzhiwei/NLP-WED0910/projects/models/bge-large-zh-v1.5',
        'num_coarse_labels': 4,
        'num_fine_labels': 14,
        'max_length': 128,
        'epochs': 40,  # å‡å°‘epochä»¥åŠ å¿«è°ƒè¯•
        'patience': 5,
        # è¯·ä¿®æ”¹ä¸ºä½ çš„å®é™…æ•°æ®è·¯å¾„
        'data_path': '/mnt/cfs/huangzhiwei/NLP-WED0910/datas/train.json',
        'val_data_path': '/mnt/cfs/huangzhiwei/NLP-WED0910/datas/val.json',
        'checkpoint_dir': 'optuna_checkpoints_expert_specialized_0527',
        'device': device,
        'weight_decay': 0.01
    }
    
    # åˆ›å»ºç»“æœä¿å­˜ç›®å½•
    results_dir = 'hyperparameter_results_expert_specialized_0527'
    os.makedirs(results_dir, exist_ok=True)
    
    # åˆ›å»ºæ•°æ®åº“å­˜å‚¨ä¼˜åŒ–å†å²
    storage_name = f"sqlite:///{results_dir}/optuna_study.db"
    
    # åˆ›å»ºstudyå¯¹è±¡ - è´å¶æ–¯ä¼˜åŒ–é…ç½®
    study = optuna.create_study(
        direction='maximize',  # æœ€å¤§åŒ–ç›®æ ‡å‡½æ•°
        sampler=TPESampler(
            seed=42,
            n_startup_trials=10,  # å‰10æ¬¡éšæœºè¯•éªŒç”¨äºåˆå§‹åŒ–
            n_ei_candidates=24,   # æ¯æ¬¡è€ƒè™‘24ä¸ªå€™é€‰ç‚¹
            multivariate=True,    # è€ƒè™‘å‚æ•°é—´ç›¸å…³æ€§
        ),
        pruner=optuna.pruners.MedianPruner(  # ä¸­ä½æ•°å‰ªæå™¨
            n_startup_trials=5,    # å‰5ä¸ªepochä¸å‰ªæ
            n_warmup_steps=10,     # é¢„çƒ­10æ­¥
            interval_steps=1,      # æ¯æ­¥æ£€æŸ¥ä¸€æ¬¡
        ),
        study_name='expert_specialized_error_classification',
        storage=storage_name,
        load_if_exists=True  # å¦‚æœå­˜åœ¨åˆ™åŠ è½½
    )
    
    print("å¼€å§‹Optunaè¶…å‚æ•°ä¼˜åŒ–...")
    print("ğŸ”§ ä½¿ç”¨ä¸“å®¶ä¸“é—¨åŒ–MoEåˆ†ç±»å™¨")
    print(f"æ•°æ®åº“: {storage_name}")
    print("ä¼˜åŒ–ç›®æ ‡: Hierarchical Final Micro F1")
    print("è·Ÿè¸ª6ä¸ªæŒ‡æ ‡:")
    print("  1. hierarchical_final_micro_f1 (ä¸»è¦æŒ‡æ ‡)")
    print("  2. hierarchical_final_macro_f1")
    print("  3. hierarchical_coarse_micro_f1")
    print("  4. hierarchical_fine_micro_f1")
    print("  5. hierarchical_coarse_macro_f1")
    print("  6. hierarchical_fine_macro_f1")
    
    start_time = time.time()
    
    # è®°å½•æ‰€æœ‰è¯•éªŒç»“æœ
    all_trials_results = []
    best_overall_result = {
        'best_score': 0,
        'best_params': {},
        'best_metrics': {},
        'trial_number': -1
    }
    
    # å®šä¹‰å›è°ƒå‡½æ•°æ¥ä¿å­˜ä¸­é—´ç»“æœ
    def save_callback(study, trial):
        nonlocal best_overall_result
        
        if trial.state == optuna.trial.TrialState.COMPLETE:
            # è¯»å–è¯¥è¯•éªŒçš„è¯¦ç»†ç»“æœ
            trial_file = os.path.join(results_dir, f'trial_{trial.number}_result.json')
            if os.path.exists(trial_file):
                with open(trial_file, 'r', encoding='utf-8') as f:
                    trial_data = json.load(f)
                    all_trials_results.append(trial_data)
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯æ–°çš„æœ€ä½³ç»“æœ
                    if trial.value > best_overall_result['best_score']:
                        best_overall_result = {
                            'best_score': trial.value,
                            'best_params': trial.params.copy(),
                            'best_metrics': trial_data['metrics'].copy(),
                            'trial_number': trial.number
                        }
            
            print(f"âœ… Trial {trial.number} å®Œæˆ: å¾—åˆ† = {trial.value:.2f}%")
            print(f"ğŸ“Š å½“å‰æœ€ä½³: {study.best_value:.2f}% (Trial {study.best_trial.number})")
            
            # ä¿å­˜å½“å‰æœ€ä½³ç»“æœ
            with open(os.path.join(results_dir, 'current_best_result.json'), 'w', encoding='utf-8') as f:
                json.dump(best_overall_result, f, ensure_ascii=False, indent=2)
        
        elif trial.state == optuna.trial.TrialState.PRUNED:
            print(f"âœ‚ï¸ Trial {trial.number} è¢«å‰ªæ")
    
    try:
        # å¼€å§‹ä¼˜åŒ–
        def objective(trial):
            score, metrics = train_single_config_for_optuna(trial, base_config, results_dir)
            return score
        
        study.optimize(
            objective,
            n_trials=200,  # å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´è¯•éªŒæ¬¡æ•°
            timeout=None,  # ä¸è®¾ç½®æ—¶é—´é™åˆ¶
            callbacks=[save_callback]
        )
        
    except KeyboardInterrupt:
        print("ä¼˜åŒ–è¢«ç”¨æˆ·ä¸­æ–­")
    
    total_time = time.time() - start_time
    
    # ä¿å­˜æ‰€æœ‰è¯•éªŒçš„æ±‡æ€»ç»“æœ
    all_trials_summary = {
        'model_type': 'ExpertSpecializedMoEClassifier',
        'optimization_completed': datetime.now().isoformat(),
        'total_time_hours': total_time / 3600,
        'total_trials': len(study.trials),
        'completed_trials': len([t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]),
        'pruned_trials': len([t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]),
        'best_trial_number': study.best_trial.number if study.best_trial else -1,
        'best_score': study.best_value if study.best_value else 0,
        'all_trials_summary': all_trials_results
    }
    
    with open(os.path.join(results_dir, 'all_trials_summary.json'), 'w', encoding='utf-8') as f:
        json.dump(all_trials_summary, f, ensure_ascii=False, indent=2)
    
    # ä¿å­˜æœ€ä½³å‚æ•°ç»„åˆå’Œ6ä¸ªæŒ‡æ ‡
    if best_overall_result['trial_number'] != -1:
        final_best_result = {
            'model_type': 'ExpertSpecializedMoEClassifier',
            'best_hyperparameters': best_overall_result['best_params'],
            'best_6_metrics': best_overall_result['best_metrics'],
            'trial_number': best_overall_result['trial_number'],
            'optimization_summary': {
                'total_trials': len(study.trials),
                'total_time_hours': total_time / 3600,
                'optimization_date': datetime.now().isoformat()
            }
        }
        
        with open(os.path.join(results_dir, 'BEST_RESULT.json'), 'w', encoding='utf-8') as f:
            json.dump(final_best_result, f, ensure_ascii=False, indent=2)
    
    # æ‰“å°æœ€ç»ˆç»“æœ
    print(f"\n{'='*80}")
    print("ğŸ‰ ä¸“å®¶ä¸“é—¨åŒ–MoEåˆ†ç±»å™¨è¶…å‚æ•°ä¼˜åŒ–å®Œæˆ!")
    print(f"â±ï¸  æ€»è€—æ—¶: {total_time/3600:.2f} å°æ—¶")
    print(f"ğŸ§ª æ€»è¯•éªŒæ•°: {len(study.trials)}")
    print(f"âœ… å®Œæˆè¯•éªŒæ•°: {len([t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE])}")
    print(f"âœ‚ï¸ å‰ªæè¯•éªŒæ•°: {len([t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED])}")
    
    if best_overall_result['trial_number'] != -1:
        print(f"\nğŸ† æœ€ä½³ç»“æœ (Trial {best_overall_result['trial_number']}):")
        print(f"ğŸ“ˆ Final Micro F1: {best_overall_result['best_score']:.2f}%")
        
        print(f"\nğŸ”§ æœ€ä½³è¶…å‚æ•°:")
        for param, value in best_overall_result['best_params'].items():
            print(f"   {param}: {value}")
        
        print(f"\nğŸ“Š å®Œæ•´6ä¸ªæŒ‡æ ‡:")
        metrics = best_overall_result['best_metrics']
        print(f"   1. Final Micro F1: {metrics['hierarchical_final_micro_f1']:.2f}%")
        print(f"   2. Final Macro F1: {metrics['hierarchical_final_macro_f1']:.2f}%")
        print(f"   3. Coarse Micro F1: {metrics['hierarchical_coarse_micro_f1']:.2f}%")
        print(f"   4. Fine Micro F1: {metrics['hierarchical_fine_micro_f1']:.2f}%")
        print(f"   5. Coarse Macro F1: {metrics['hierarchical_coarse_macro_f1']:.2f}%")
        print(f"   6. Fine Macro F1: {metrics['hierarchical_fine_macro_f1']:.2f}%")
        print(f"   ğŸ¯ æœ€ä½³epoch: {metrics['epoch']}")
    
    print(f"\nğŸ“ ç»“æœä¿å­˜åœ¨: {results_dir}/")
    print(f"   - BEST_RESULT.json: æœ€ä½³å‚æ•°å’Œ6ä¸ªæŒ‡æ ‡")
    print(f"   - all_trials_summary.json: æ‰€æœ‰è¯•éªŒæ±‡æ€»")
    print(f"   - trial_X_result.json: æ¯ä¸ªè¯•éªŒçš„è¯¦ç»†ç»“æœ")
    print(f"   - current_best_result.json: å®æ—¶æœ€ä½³ç»“æœ")
    
    return best_overall_result, study


if __name__ == '__main__':
    print("ğŸš€ å¯åŠ¨ä¸“å®¶ä¸“é—¨åŒ–MoEå±‚æ¬¡åŒ–é”™è¯¯åˆ†ç±»è¶…å‚æ•°ä¼˜åŒ–")
    print("ğŸ¯ ä¸»è¦è¯„åˆ¤æŒ‡æ ‡: Final Micro F1")
    print("ğŸ“Š è·Ÿè¸ª6ä¸ªå…³é”®æŒ‡æ ‡")
    print("ğŸ’¾ æ¯è½®ç»“æœä¿å­˜åˆ°JSONæ–‡ä»¶")
    print("ğŸ”§ ä½¿ç”¨ä¸“å®¶ä¸“é—¨åŒ–MoEåˆ†ç±»å™¨ - æ¯ä¸ªä¸“å®¶ä¸“é—¨å¤„ç†ç‰¹å®šç±»å‹çš„é”™è¯¯")
    
    # æ£€æŸ¥æ•°æ®è·¯å¾„
    data_paths = [
        '/mnt/cfs/huangzhiwei/NLP-WED0910/datas/train.json',
        '/mnt/cfs/huangzhiwei/NLP-WED0910/datas/val.json'
    ]
    
    print("\nğŸ” æ£€æŸ¥æ•°æ®æ–‡ä»¶...")
    for path in data_paths:
        if os.path.exists(path):
            print(f"âœ… {path} - å­˜åœ¨")
        else:
            print(f"âŒ {path} - ä¸å­˜åœ¨")
            print(f"è¯·ä¿®æ”¹ä»£ç ä¸­çš„æ•°æ®è·¯å¾„æˆ–ç¡®ä¿æ–‡ä»¶å­˜åœ¨")
            exit(1)
    
    # æ£€æŸ¥Optunaæ˜¯å¦å®‰è£…
    try:
        import optuna
        print(f"âœ… Optunaç‰ˆæœ¬: {optuna.__version__}")
    except ImportError:
        print("âŒ è¯·å…ˆå®‰è£…Optuna: pip install optuna")
        exit(1)
    
    # å¼€å§‹ä¼˜åŒ–
    best_result, study = optuna_hyperparameter_optimization()
    
    print(f"\nğŸŠ ä¼˜åŒ–å®Œæˆ! æœ€ä½³é…ç½®å·²ä¿å­˜åˆ° hyperparameter_results_expert_specialized_0527/BEST_RESULT.json")