import torch
import torch.nn as nn
import torch.nn.functional as F
import os
import random
import argparse
import numpy as np
import json
from tqdm import tqdm
from transformers import BertModel, AutoModel
from torch.optim import AdamW
from torch.utils.data import Dataset
from torch.utils.data import DataLoader
from transformers import AutoTokenizer
from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score
import time
from datetime import datetime

# ä½¿ç”¨Optunaè¿›è¡Œè´å¶æ–¯ä¼˜åŒ–
import optuna
from optuna.samplers import TPESampler
import sqlite3
from optuna.distributions import UniformDistribution, LogUniformDistribution

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ä½ çš„åŸæœ‰ç±»å®šä¹‰ä¿æŒä¸å˜
class ErrorDetectionDataset(Dataset):
    def __init__(
            self,
            data_path,
            coarse_labels={
                "å­—ç¬¦çº§é”™è¯¯": 0,
                "æˆåˆ†æ®‹ç¼ºå‹é”™è¯¯": 1, 
                "æˆåˆ†èµ˜ä½™å‹é”™è¯¯": 2,
                "æˆåˆ†æ­é…ä¸å½“å‹é”™è¯¯": 3
            },
            fine_labels={
                "ç¼ºå­—æ¼å­—": 0,
                "é”™åˆ«å­—é”™è¯¯": 1,
                "ç¼ºå°‘æ ‡ç‚¹": 2,
                "é”™ç”¨æ ‡ç‚¹": 3,
                "ä¸»è¯­ä¸æ˜": 4,
                "è°“è¯­æ®‹ç¼º": 5,
                "å®¾è¯­æ®‹ç¼º": 6,
                "å…¶ä»–æˆåˆ†æ®‹ç¼º": 7,
                "ä¸»è¯­å¤šä½™": 8,
                "è™šè¯å¤šä½™": 9,
                "å…¶ä»–æˆåˆ†å¤šä½™": 10,
                "è¯­åºä¸å½“": 11,
                "åŠ¨å®¾æ­é…ä¸å½“": 12,
                "å…¶ä»–æ­é…ä¸å½“": 13
            }
    ):
        self.data_path = data_path
        self.coarse_labels = coarse_labels
        self.fine_labels = fine_labels
        self._get_data()
    
    # ä¿®æ”¹1: ErrorDetectionDatasetç±»çš„_get_dataæ–¹æ³•
    def _get_data(self):
        with open(self.data_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        self.data = []
        for item in data:
            sent = item['sent']
            coarse_types = item['CourseGrainedErrorType']
            fine_types = item['FineGrainedErrorType']
            
            # æ„å»ºç²—ç²’åº¦æ ‡ç­¾ï¼ˆå¤šæ ‡ç­¾ï¼‰
            coarse_label = [0] * len(self.coarse_labels)
            for c_type in coarse_types:
                if c_type in self.coarse_labels:
                    coarse_label[self.coarse_labels[c_type]] = 1
            
            # æ„å»ºç»†ç²’åº¦æ ‡ç­¾ï¼ˆå¤šæ ‡ç­¾ï¼‰
            fine_label = [0] * len(self.fine_labels)
            for f_type in fine_types:
                if f_type in self.fine_labels:
                    fine_label[self.fine_labels[f_type]] = 1
            
            # è·å–Qwençš„è½¯æ ‡ç­¾ï¼ˆæ¦‚ç‡ï¼‰
            coarse_soft_label = None
            fine_soft_label = None
            
            if 'coarse_probabilities_qwen256' in item:
                coarse_soft_label = [0.0] * len(self.coarse_labels)
                for label_name, prob in item['coarse_probabilities_qwen256'].items():
                    if label_name in self.coarse_labels:
                        coarse_soft_label[self.coarse_labels[label_name]] = prob
            
            if 'fine_probabilities_qwen256' in item:
                fine_soft_label = [0.0] * len(self.fine_labels)
                for label_name, prob in item['fine_probabilities_qwen256'].items():
                    if label_name in self.fine_labels:
                        fine_soft_label[self.fine_labels[label_name]] = prob
            
            self.data.append((
                sent, 
                coarse_label, 
                fine_label, 
                item.get('sent_id', -1),
                coarse_soft_label,  # æ–°å¢ï¼šç²—ç²’åº¦è½¯æ ‡ç­¾
                fine_soft_label     # æ–°å¢ï¼šç»†ç²’åº¦è½¯æ ‡ç­¾
            ))
    
    def __len__(self):
        return len(self.data)
    
    def get_coarse_labels(self):
        return self.coarse_labels
    
    def get_fine_labels(self):
        return self.fine_labels
    
    def __getitem__(self, idx):
        return self.data[idx]


class ErrorDetectionDataLoader:
    def __init__(
        self,
        dataset,
        batch_size=16,
        max_length=128,
        shuffle=True,
        drop_last=True,
        device=None,
        tokenizer_name='/mnt/cfs/huangzhiwei/NLP-WED0910/projects/models/bge-large-zh-v1.5'
    ):
        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
        self.dataset = dataset
        self.batch_size = batch_size
        self.max_length = max_length
        self.shuffle = shuffle
        self.drop_last = drop_last
        
        if device is None:
            self.device = torch.device(
                'cuda' if torch.cuda.is_available() else 'cpu'
            )
        else:
            self.device = device
        
        self.loader = DataLoader(
            dataset=self.dataset,
            batch_size=self.batch_size,
            collate_fn=self.collate_fn,
            shuffle=self.shuffle,
            drop_last=self.drop_last
        )
    
    # ä¿®æ”¹2: ErrorDetectionDataLoaderç±»çš„collate_fnæ–¹æ³•
    def collate_fn(self, data):
        sents = [item[0] for item in data]
        coarse_labels = [item[1] for item in data]
        fine_labels = [item[2] for item in data]
        sent_ids = [item[3] for item in data]
        coarse_soft_labels = [item[4] for item in data]  # æ–°å¢
        fine_soft_labels = [item[5] for item in data]    # æ–°å¢
        
        # ç¼–ç æ–‡æœ¬
        encoded = self.tokenizer.batch_encode_plus(
            batch_text_or_text_pairs=sents,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt',
            return_length=True
        )
        
        input_ids = encoded['input_ids'].to(self.device)
        attention_mask = encoded['attention_mask'].to(self.device)
        
        # å¤„ç†ç¡¬æ ‡ç­¾
        if coarse_labels[0] == -1:
            coarse_labels = None
            fine_labels = None
            coarse_soft_labels = None
            fine_soft_labels = None
        else:
            coarse_labels = torch.tensor(coarse_labels, dtype=torch.float).to(self.device)
            fine_labels = torch.tensor(fine_labels, dtype=torch.float).to(self.device)
            
            # å¤„ç†è½¯æ ‡ç­¾
            if coarse_soft_labels[0] is not None:
                coarse_soft_labels = torch.tensor(coarse_soft_labels, dtype=torch.float).to(self.device)
            else:
                coarse_soft_labels = None
                
            if fine_soft_labels[0] is not None:
                fine_soft_labels = torch.tensor(fine_soft_labels, dtype=torch.float).to(self.device)
            else:
                fine_soft_labels = None
        
        return input_ids, attention_mask, coarse_labels, fine_labels, sent_ids, coarse_soft_labels, fine_soft_labels
    
    def __iter__(self):
        for data in self.loader:
            yield data
    
    def __len__(self):
        return len(self.loader)

class MoELayer(nn.Module):
    """Mixture of Experts Layer"""
    def __init__(self, input_dim, expert_dim, num_experts=8, top_k=2, dropout=0.1, load_balance_weight=0.01):
        super().__init__()
        self.num_experts = num_experts
        self.top_k = top_k
        self.input_dim = input_dim
        self.expert_dim = expert_dim
        
        # Gateç½‘ç»œï¼Œç”¨äºé€‰æ‹©ä¸“å®¶
        self.gate = nn.Linear(input_dim, num_experts)

        # ä¸“å®¶ç½‘ç»œï¼šæ¯ä¸€ä¸ªä¸“å®¶éƒ½æ˜¯ä¸¤å±‚çš„MLP
        self.experts = nn.ModuleList([
            nn.Sequential(
                nn.Linear(input_dim, expert_dim),
                nn.ReLU(),
                nn.Dropout(dropout),
                nn.Linear(expert_dim, input_dim),
                nn.Dropout(dropout)
            ) for _ in range(num_experts)
        ])

        # è´Ÿè½½å‡è¡¡æŸå¤±çš„æƒé‡
        self.load_balance_weight = load_balance_weight
    
    def forward(self, x):
        batch_size, seq_len = x.size(0), x.size(1) if len(x.shape) > 2 else 1

        # å¦‚æœè¾“å…¥æ˜¯3D (batch, seq, hidden)ï¼Œæˆ‘ä»¬åªç”¨[CLS]ä½ç½®
        if len(x.shape) == 3:
            x = x[:, 0, :]  # å–[CLS]ä½ç½®

        # Gateç½‘ç»œè¾“å‡º: é€‰æ‹©ä¸“å®¶çš„æ¦‚ç‡
        gate_logits = self.gate(x) # (batch_size, num_experts)
        gate_probs = F.softmax(gate_logits, dim=-1)

        # é€‰æ‹©å‰top_kä¸ªä¸“å®¶
        top_k_probs, top_k_indices = torch.topk(gate_probs, self.top_k, dim=-1)
        top_k_probs = top_k_probs / top_k_probs.sum(dim=-1, keepdim=True)  # å½’ä¸€åŒ–

        # è®¡ç®—ä¸“å®¶è¾“å‡º
        expert_outputs = []
        for i in range(self.num_experts):
            expert_output = self.experts[i](x) # (batch_size, input_dim)
            expert_outputs.append(expert_output)

        expert_outputs = torch.stack(expert_outputs, dim=1)  # (batch_size, num_experts, input_dim)

        # ç»„åˆtop-kä¸“å®¶çš„è¾“å‡º
        final_output = torch.zeros_like(x) # (batch_size, input_dim)
        for i in range(self.top_k):
            expert_idx = top_k_indices[:, i]  # ä¿®å¤è¯­æ³•é”™è¯¯
            expert_weight = top_k_probs[:, i:i+1]  # (batch_size, 1)

            # é€‰æ‹©å¯¹åº”ä¸“å®¶çš„è¾“å‡º
            selected_output = expert_outputs[torch.arange(batch_size), expert_idx]  # (batch_size, input_dim)
            final_output += expert_weight * selected_output

        # è®¡ç®—è´Ÿè½½å‡è¡¡æŸå¤±
        load_balance_loss = self.compute_load_balance_loss(gate_probs)
        
        return final_output, load_balance_loss

    
    def compute_load_balance_loss(self, gate_probs):
        """è®¡ç®—è´Ÿè½½å‡è¡¡æŸå¤±ï¼Œé¼“åŠ±ä¸“å®¶ä½¿ç”¨çš„å‡è¡¡æ€§"""
        # è®¡ç®—æ¯ä¸ªä¸“å®¶è¢«é€‰æ‹©çš„å¹³å‡æ¦‚ç‡
        expert_usage = gate_probs.mean(dim=0)  # (num_experts,)
        
        # ç†æƒ³æƒ…å†µä¸‹æ¯ä¸ªä¸“å®¶è¢«ä½¿ç”¨çš„æ¦‚ç‡åº”è¯¥æ˜¯ 1/num_experts
        ideal_usage = 1.0 / self.num_experts
        
        # è®¡ç®—è´Ÿè½½å‡è¡¡æŸå¤±ï¼ˆä½¿ç”¨L2è·ç¦»ï¼‰
        load_balance_loss = torch.sum((expert_usage - ideal_usage) ** 2)
        
        return self.load_balance_weight * load_balance_loss


class HierarchicalErrorClassifier(nn.Module):
    def __init__(
        self, 
        pretrained_model_name, 
        num_coarse_labels=4, 
        num_fine_labels=14, 
        dropout=0.2,
        num_experts=8,
        expert_dim=512,
        top_k=2,
        use_separate_moe=True,  # æ˜¯å¦ä¸ºç²—ç²’åº¦å’Œç»†ç²’åº¦ä½¿ç”¨ä¸åŒçš„MoE
        load_balance_weight=0.01  # æ·»åŠ load_balance_weightå‚æ•°
    ):
        super().__init__()
        
        self.bert = AutoModel.from_pretrained(pretrained_model_name)
        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()
        self.use_separate_moe = use_separate_moe
        
        hidden_size = self.bert.config.hidden_size
        
        if use_separate_moe:
            # ä¸ºç²—ç²’åº¦å’Œç»†ç²’åº¦åˆ†ç±»åˆ†åˆ«ä½¿ç”¨ä¸åŒçš„MoE
            self.coarse_moe = MoELayer(
                input_dim=hidden_size,
                expert_dim=expert_dim,
                num_experts=num_experts,
                top_k=top_k,
                dropout=dropout,
                load_balance_weight=load_balance_weight
            )
            self.fine_moe = MoELayer(
                input_dim=hidden_size,
                expert_dim=expert_dim,
                num_experts=num_experts,
                top_k=top_k,
                dropout=dropout,
                load_balance_weight=load_balance_weight
            )
        else:
            # å…±äº«ä¸€ä¸ªMoEå±‚
            self.shared_moe = MoELayer(
                input_dim=hidden_size,
                expert_dim=expert_dim,
                num_experts=num_experts,
                top_k=top_k,
                dropout=dropout,
                load_balance_weight=load_balance_weight
            )
        
        # ç²—ç²’åº¦åˆ†ç±»å™¨
        self.coarse_classifier = nn.Linear(self.bert.config.hidden_size, num_coarse_labels)
        
        # ç»†ç²’åº¦åˆ†ç±»å™¨
        self.fine_classifier = nn.Linear(self.bert.config.hidden_size, num_fine_labels)
        
        # å®šä¹‰ç²—ç²’åº¦ç±»åˆ«å’Œå¯¹åº”çš„ç»†ç²’åº¦ç´¢å¼•çš„æ˜ å°„
        self.coarse_to_fine_indices = {
            0: [0, 1, 2, 3],    # å­—ç¬¦çº§é”™è¯¯
            1: [4, 5, 6, 7],    # æˆåˆ†æ®‹ç¼ºå‹é”™è¯¯
            2: [8, 9, 10],      # æˆåˆ†å†—ä½™å‹é”™è¯¯
            3: [11, 12, 13]     # æˆåˆ†æ­é…ä¸å½“å‹é”™è¯¯
        }
    
    def forward(self, input_ids, attention_mask, token_type_ids=None):
        # BERTç¼–ç 
        if token_type_ids is not None:
            outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)
        else:
            outputs = self.bert(input_ids, attention_mask=attention_mask)
        
        pooled_output = outputs.last_hidden_state[:,0,:]  # å–[CLS]çš„è¾“å‡º
        pooled_output = self.dropout(pooled_output)

        total_load_balance_loss = 0
        
        if self.use_separate_moe:
            # ä½¿ç”¨ä¸åŒçš„MoEå¤„ç†ç²—ç²’åº¦å’Œç»†ç²’åº¦åˆ†ç±»
            coarse_features, coarse_lb_loss = self.coarse_moe(pooled_output)
            fine_features, fine_lb_loss = self.fine_moe(pooled_output)
            total_load_balance_loss = coarse_lb_loss + fine_lb_loss
        else:
            # ä½¿ç”¨å…±äº«MoE
            shared_features, shared_lb_loss = self.shared_moe(pooled_output)
            coarse_features = fine_features = shared_features
            total_load_balance_loss = shared_lb_loss
        
        # ç²—ç²’åº¦åˆ†ç±»
        # coarse_logits = self.coarse_classifier(pooled_output)
        coarse_logits = self.coarse_classifier(coarse_features)  # ä½¿ç”¨MoEè¾“å‡º
        coarse_probs = torch.sigmoid(coarse_logits)
        
        # ç»†ç²’åº¦åˆ†ç±»
        # fine_logits = self.fine_classifier(pooled_output)
        fine_logits = self.fine_classifier(fine_features)  # ä½¿ç”¨MoEè¾“å‡º
        fine_probs = torch.sigmoid(fine_logits)
        
        return coarse_probs, fine_probs, total_load_balance_loss
    
    def apply_hierarchical_constraint(self, coarse_preds, fine_preds):
        """
        åº”ç”¨å±‚æ¬¡çº¦æŸï¼šå¦‚æœç²—ç²’åº¦ç±»åˆ«é¢„æµ‹ä¸ºè´Ÿï¼Œåˆ™è¯¥ç²—ç²’åº¦ä¸‹çš„æ‰€æœ‰ç»†ç²’åº¦ç±»åˆ«å‡è®¾ä¸ºè´Ÿ
        """
        constrained_fine_preds = fine_preds.clone()
        
        # éå†æ¯ä¸ªæ ·æœ¬
        for i in range(coarse_preds.size(0)):
            # å¯¹æ¯ä¸ªç²—ç²’åº¦ç±»åˆ«
            for coarse_idx, fine_indices in self.coarse_to_fine_indices.items():
                # å¦‚æœç²—ç²’åº¦ä¸ºè´Ÿï¼Œåˆ™å¯¹åº”çš„ç»†ç²’åº¦å…¨éƒ¨è®¾ä¸ºè´Ÿ
                if coarse_preds[i, coarse_idx] == 0:
                    constrained_fine_preds[i, fine_indices] = 0
        
        return constrained_fine_preds


# ä¿®æ”¹3: æ–°å¢è’¸é¦æŸå¤±å‡½æ•°ï¼ˆä¿®æ­£ç‰ˆï¼‰
class DistillationLoss(nn.Module):
    def __init__(self, alpha=0.7, temperature=4.0):
        super().__init__()
        self.alpha = alpha
        self.temperature = temperature
        self.hard_loss = nn.BCELoss()
        self.soft_loss = nn.MSELoss()
    
    def forward(self, student_probs, hard_labels, soft_labels=None):
        hard_loss = self.hard_loss(student_probs, hard_labels)
        
        if soft_labels is None:
            return hard_loss
        
        # å®é™…ä½¿ç”¨æ¸©åº¦å‚æ•°
        if self.temperature != 1.0:
            # è½¬å›logitsåº”ç”¨æ¸©åº¦å†è½¬å›æ¦‚ç‡
            student_logits = torch.log(student_probs/(1-student_probs))/self.temperature
            teacher_logits = torch.log(soft_labels/(1-soft_labels))/self.temperature
            student_temp = torch.sigmoid(student_logits)
            teacher_temp = torch.sigmoid(teacher_logits)
            soft_loss = self.soft_loss(student_temp, teacher_temp)
        else:
            soft_loss = self.soft_loss(student_probs, soft_labels)
        
        total_loss = (1 - self.alpha) * hard_loss + self.alpha * soft_loss
        return total_loss



def calculate_metrics(labels, predictions, average='micro'):
    """è®¡ç®—å„ç§è¯„ä¼°æŒ‡æ ‡"""
    labels = np.array(labels)
    predictions = np.array(predictions)
    
    micro_f1 = f1_score(labels, predictions, average='micro')
    macro_f1 = f1_score(labels, predictions, average='macro')
    sample_acc = accuracy_score(labels, predictions)
    
    return {
        'micro_f1': micro_f1 * 100,
        'macro_f1': macro_f1 * 100,
        'accuracy': sample_acc * 100
    }


def train_single_config_for_optuna(trial, base_config, results_dir):
    """ä¸ºOptunaä¼˜åŒ–è®­ç»ƒå•ä¸ªé…ç½®ï¼Œè¿”å›å®Œæ•´çš„6ä¸ªæŒ‡æ ‡"""
    
    # ä»trialä¸­è·å–è¶…å‚æ•°
    params = {
        'dropout': trial.suggest_float('dropout', 0.0, 0.4),
        'batch_size': trial.suggest_categorical('batch_size', [8, 16, 32]),
        'lr': trial.suggest_float('lr', 1e-6, 1e-4, log=True),
        'threshold': trial.suggest_float('threshold', 0.2, 0.8),
        'seed': trial.suggest_categorical('seed', [42, 3407, 2023]),
        'num_experts': trial.suggest_int('num_experts', 4, 16),
        'top_k': trial.suggest_int('top_k', 1, 4),
        'expert_dim': trial.suggest_categorical('expert_dim', [256, 512, 768, 1024]),
        'use_separate_moe': trial.suggest_categorical('use_separate_moe', [True, False]),
        'load_balance_weight': trial.suggest_float('load_balance_weight', 0.0001, 0.1, log=True),
        'distillation_alpha': trial.suggest_float('distillation_alpha', 0.05, 0.95), 
        'distillation_temperature': trial.suggest_float('distillation_temperature', 1.0, 15.0),
    }
    
    # è®¾ç½®éšæœºç§å­
    seed = params['seed']
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    
    # åˆ›å»ºé…ç½®å¯¹è±¡
    class Config:
        pass
    
    configs = Config()
    
    # è®¾ç½®åŸºç¡€é…ç½®
    for key, value in base_config.items():
        setattr(configs, key, value)
    
    # è®¾ç½®ä¼˜åŒ–å‚æ•°
    for key, value in params.items():
        setattr(configs, key, value)
    
    # çº¦æŸtop_kä¸èƒ½å¤§äºnum_experts
    if configs.top_k > configs.num_experts:
        configs.top_k = configs.num_experts
    
    # åˆ›å»ºæ£€æŸ¥ç‚¹ç›®å½•
    checkpoint_dir = os.path.join(configs.checkpoint_dir, f"optuna_trial_{trial.number}")
    os.makedirs(checkpoint_dir, exist_ok=True)
    configs.checkpoint_dir = checkpoint_dir
    
    try:
        # åŠ è½½æ•°æ®é›†
        train_dataset = ErrorDetectionDataset(configs.data_path)
        val_dataset = ErrorDetectionDataset(configs.val_data_path)    

        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_dataloader = ErrorDetectionDataLoader(
            dataset=train_dataset,
            batch_size=configs.batch_size,
            max_length=configs.max_length,
            shuffle=True,
            drop_last=True,
            device=configs.device,
            tokenizer_name=configs.model_name
        )

        val_dataloader = ErrorDetectionDataLoader(
            dataset=val_dataset,
            batch_size=configs.batch_size,
            max_length=configs.max_length,
            shuffle=False,
            drop_last=False,
            device=configs.device,
            tokenizer_name=configs.model_name
        )

        # åˆ›å»ºæ¨¡å‹
        model = HierarchicalErrorClassifier(
            pretrained_model_name=configs.model_name,
            num_coarse_labels=configs.num_coarse_labels,
            num_fine_labels=configs.num_fine_labels,
            dropout=configs.dropout,
            num_experts=configs.num_experts,
            expert_dim=configs.expert_dim,
            top_k=configs.top_k,
            use_separate_moe=configs.use_separate_moe,
            load_balance_weight=configs.load_balance_weight
        ).to(configs.device)

        # å®šä¹‰è’¸é¦æŸå¤±å‡½æ•°
        distillation_criterion = DistillationLoss(
            alpha=configs.distillation_alpha,
            temperature=configs.distillation_temperature
        )

        # å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
        criterion = nn.BCELoss()
        optimizer = AdamW(
            filter(lambda p: p.requires_grad, model.parameters()),
            lr=configs.lr,
            weight_decay=getattr(configs, 'weight_decay', 0.01)
        )

        # åˆå§‹åŒ–æœ€ä½³éªŒè¯æ€§èƒ½å’Œ6ä¸ªæŒ‡æ ‡
        best_metrics = {
            'hierarchical_final_micro_f1': 0,
            'hierarchical_final_macro_f1': 0,
            'hierarchical_coarse_micro_f1': 0,
            'hierarchical_fine_micro_f1': 0,
            'hierarchical_coarse_macro_f1': 0,
            'hierarchical_fine_macro_f1': 0,
            'epoch': 0
        }
        
        best_val_f1 = 0
        patience_counter = 0
        
        # è®­ç»ƒå¾ªç¯
        for epoch in range(configs.epochs):
            # è®­ç»ƒé˜¶æ®µ
            model.train()
            train_loss = 0.0
            
            for batch_data in train_dataloader:
                # è§£åŒ…æ•°æ®ï¼ˆç°åœ¨åŒ…å«è½¯æ ‡ç­¾ï¼‰
                input_ids, attention_mask, coarse_labels, fine_labels, sent_ids, coarse_soft_labels, fine_soft_labels = batch_data
                
                optimizer.zero_grad()
                
                # å‰å‘ä¼ æ’­
                coarse_probs, fine_probs, load_balance_loss = model(input_ids, attention_mask)
                
                # è®¡ç®—è’¸é¦æŸå¤±
                if coarse_soft_labels is not None and fine_soft_labels is not None:
                    # ä½¿ç”¨è’¸é¦æŸå¤±
                    coarse_loss = distillation_criterion(coarse_probs, coarse_labels, coarse_soft_labels)
                    fine_loss = distillation_criterion(fine_probs, fine_labels, fine_soft_labels)
                else:
                    # fallbackåˆ°åŸå§‹BCEæŸå¤±
                    criterion = nn.BCELoss()
                    coarse_loss = criterion(coarse_probs, coarse_labels)
                    fine_loss = criterion(fine_probs, fine_labels)
                
                classification_loss = coarse_loss + fine_loss
                
                # æ€»æŸå¤± = åˆ†ç±»æŸå¤± + è´Ÿè½½å‡è¡¡æŸå¤±
                total_loss = classification_loss + load_balance_loss
                
                # åå‘ä¼ æ’­
                total_loss.backward()
                optimizer.step()
                
                train_loss += classification_loss.item()

            # éªŒè¯é˜¶æ®µ
            model.eval()
            all_coarse_preds = []
            all_coarse_labels = []
            all_fine_labels = []
            all_constrained_fine_preds = []

            with torch.no_grad():
                for input_ids, attention_mask, coarse_labels, fine_labels, sent_ids, _, _, in val_dataloader:
                    # å‰å‘ä¼ æ’­
                    coarse_probs, fine_probs, load_balance_loss = model(input_ids, attention_mask)
                    
                    # åº”ç”¨å±‚æ¬¡çº¦æŸ
                    coarse_preds = (coarse_probs > configs.threshold).float()
                    fine_preds = (fine_probs > configs.threshold).float()
                    constrained_fine_preds = model.apply_hierarchical_constraint(coarse_preds, fine_preds)
                    
                    # æ”¶é›†é¢„æµ‹ç»“æœ
                    all_coarse_preds.extend(coarse_preds.cpu().numpy())
                    all_coarse_labels.extend(coarse_labels.cpu().numpy())
                    all_constrained_fine_preds.extend(constrained_fine_preds.cpu().numpy())
                    all_fine_labels.extend(fine_labels.cpu().numpy())
            
            # è®¡ç®—éªŒè¯æŒ‡æ ‡
            val_coarse_metrics_micro = calculate_metrics(all_coarse_labels, all_coarse_preds, average='micro')
            val_coarse_metrics_macro = calculate_metrics(all_coarse_labels, all_coarse_preds, average='macro')
            val_constrained_fine_metrics_micro = calculate_metrics(all_fine_labels, all_constrained_fine_preds, average='micro')
            val_constrained_fine_metrics_macro = calculate_metrics(all_fine_labels, all_constrained_fine_preds, average='macro')
            
            # è®¡ç®—6ä¸ªæŒ‡æ ‡
            hierarchical_final_micro_f1 = (val_constrained_fine_metrics_micro['micro_f1'] + val_coarse_metrics_micro['micro_f1']) / 2
            hierarchical_final_macro_f1 = (val_constrained_fine_metrics_macro['macro_f1'] + val_coarse_metrics_macro['macro_f1']) / 2
            hierarchical_coarse_micro_f1 = val_coarse_metrics_micro['micro_f1']
            hierarchical_fine_micro_f1 = val_constrained_fine_metrics_micro['micro_f1']
            hierarchical_coarse_macro_f1 = val_coarse_metrics_macro['macro_f1']
            hierarchical_fine_macro_f1 = val_constrained_fine_metrics_macro['macro_f1']
            
            current_metrics = {
                'hierarchical_final_micro_f1': hierarchical_final_micro_f1,
                'hierarchical_final_macro_f1': hierarchical_final_macro_f1,
                'hierarchical_coarse_micro_f1': hierarchical_coarse_micro_f1,
                'hierarchical_fine_micro_f1': hierarchical_fine_micro_f1,
                'hierarchical_coarse_macro_f1': hierarchical_coarse_macro_f1,
                'hierarchical_fine_macro_f1': hierarchical_fine_macro_f1,
                'epoch': epoch + 1
            }
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºæœ€ä½³æ€§èƒ½ï¼ˆä½¿ç”¨Final micro f1ä½œä¸ºä¸»è¦æŒ‡æ ‡ï¼‰
            if hierarchical_final_micro_f1 > best_val_f1:
                best_val_f1 = hierarchical_final_micro_f1
                best_metrics = current_metrics.copy()
                patience_counter = 0
            else:
                patience_counter += 1
                
            # æ—©åœæ£€æŸ¥
            if patience_counter >= configs.patience:
                break
            
            # æŠ¥å‘Šä¸­é—´ç»“æœç»™Optunaï¼ˆç”¨äºå‰ªæï¼‰
            trial.report(hierarchical_final_micro_f1, epoch)
            
            # æ£€æŸ¥æ˜¯å¦åº”è¯¥å‰ªæ
            if trial.should_prune():
                raise optuna.exceptions.TrialPruned()
        
        # ä¿å­˜å½“å‰è¯•éªŒç»“æœåˆ°JSON
        trial_result = {
            'trial_number': trial.number,
            'params': params,
            'metrics': best_metrics,
            'status': 'completed',
            'timestamp': datetime.now().isoformat()
        }
        
        # ä¿å­˜æ¯è½®ç»“æœ
        with open(os.path.join(results_dir, f'trial_{trial.number}_result.json'), 'w', encoding='utf-8') as f:
            json.dump(trial_result, f, ensure_ascii=False, indent=2)
        
        # æ¸…ç†æ£€æŸ¥ç‚¹ç›®å½•
        import shutil
        if os.path.exists(checkpoint_dir):
            shutil.rmtree(checkpoint_dir)
        
        return best_val_f1, best_metrics
        
    except Exception as e:
        print(f"Trial {trial.number} å¤±è´¥: {str(e)}")
        
        # ä¿å­˜å¤±è´¥çš„è¯•éªŒç»“æœ
        failed_result = {
            'trial_number': trial.number,
            'params': params,
            'metrics': None,
            'status': 'failed',
            'error': str(e),
            'timestamp': datetime.now().isoformat()
        }
        
        with open(os.path.join(results_dir, f'trial_{trial.number}_result.json'), 'w', encoding='utf-8') as f:
            json.dump(failed_result, f, ensure_ascii=False, indent=2)
        
        # æ¸…ç†æ£€æŸ¥ç‚¹ç›®å½•
        import shutil
        if os.path.exists(checkpoint_dir):
            shutil.rmtree(checkpoint_dir)
        
        raise optuna.exceptions.TrialPruned()


def optuna_hyperparameter_optimization():
    """ä½¿ç”¨Optunaè¿›è¡Œè¶…å‚æ•°ä¼˜åŒ–"""
    
    # å›ºå®šå‚æ•° - è¯·æ ¹æ®ä½ çš„å®é™…è·¯å¾„ä¿®æ”¹
    base_config = {
        'model_name': '/mnt/cfs/huangzhiwei/NLP-WED0910/projects/models/bge-large-zh-v1.5',
        'num_coarse_labels': 4,
        'num_fine_labels': 14,
        'max_length': 128,
        'epochs': 40,  # å‡å°‘epochä»¥åŠ å¿«è°ƒè¯•
        'patience': 6,
        # è¯·ä¿®æ”¹ä¸ºä½ çš„å®é™…æ•°æ®è·¯å¾„
        'data_path': '/mnt/cfs/huangzhiwei/NLP-WED0910/datas/train_merged_origin+qwen256.json',
        'val_data_path': '/mnt/cfs/huangzhiwei/NLP-WED0910/datas/val.json',
        'checkpoint_dir': 'optuna_checkpoints_update0605_bge+qwen256',
        'device': device,
        'weight_decay': 0.01
    }
    
    # åˆ›å»ºç»“æœä¿å­˜ç›®å½•
    results_dir = 'hyperparameter_results_0605_bge+qwen256'
    os.makedirs(results_dir, exist_ok=True)
    
    # åˆ›å»ºæ•°æ®åº“å­˜å‚¨ä¼˜åŒ–å†å²
    storage_name = f"sqlite:///{results_dir}/optuna_study.db"
    
    # åˆ›å»ºstudyå¯¹è±¡ - è´å¶æ–¯ä¼˜åŒ–é…ç½®
    study = optuna.create_study(
        direction='maximize',  # æœ€å¤§åŒ–ç›®æ ‡å‡½æ•°
        sampler=TPESampler(
            seed=42,
            n_startup_trials=10,  # å‰10æ¬¡éšæœºè¯•éªŒç”¨äºåˆå§‹åŒ–
            n_ei_candidates=24,   # æ¯æ¬¡è€ƒè™‘24ä¸ªå€™é€‰ç‚¹
            multivariate=True,    # è€ƒè™‘å‚æ•°é—´ç›¸å…³æ€§
        ),
        pruner=optuna.pruners.MedianPruner(  # ä¸­ä½æ•°å‰ªæå™¨
            n_startup_trials=5,    # å‰5ä¸ªepochä¸å‰ªæ
            n_warmup_steps=10,     # é¢„çƒ­10æ­¥
            interval_steps=1,      # æ¯æ­¥æ£€æŸ¥ä¸€æ¬¡
        ),
        study_name='hierarchical_error_classification',
        storage=storage_name,
        load_if_exists=True  # å¦‚æœå­˜åœ¨åˆ™åŠ è½½
    )
    
    print("å¼€å§‹Optunaè¶…å‚æ•°ä¼˜åŒ–...")
    print(f"æ•°æ®åº“: {storage_name}")
    print("ä¼˜åŒ–ç›®æ ‡: Hierarchical Final Micro F1")
    print("è·Ÿè¸ª6ä¸ªæŒ‡æ ‡:")
    print("  1. hierarchical_final_micro_f1 (ä¸»è¦æŒ‡æ ‡)")
    print("  2. hierarchical_final_macro_f1")
    print("  3. hierarchical_coarse_micro_f1")
    print("  4. hierarchical_fine_micro_f1")
    print("  5. hierarchical_coarse_macro_f1")
    print("  6. hierarchical_fine_macro_f1")
    
    start_time = time.time()
    
    # è®°å½•æ‰€æœ‰è¯•éªŒç»“æœ
    all_trials_results = []
    best_overall_result = {
        'best_score': 0,
        'best_params': {},
        'best_metrics': {},
        'trial_number': -1
    }
    
    # å®šä¹‰å›è°ƒå‡½æ•°æ¥ä¿å­˜ä¸­é—´ç»“æœ
    def save_callback(study, trial):
        nonlocal best_overall_result
        
        if trial.state == optuna.trial.TrialState.COMPLETE:
            # è¯»å–è¯¥è¯•éªŒçš„è¯¦ç»†ç»“æœ
            trial_file = os.path.join(results_dir, f'trial_{trial.number}_result.json')
            if os.path.exists(trial_file):
                with open(trial_file, 'r', encoding='utf-8') as f:
                    trial_data = json.load(f)
                    all_trials_results.append(trial_data)
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯æ–°çš„æœ€ä½³ç»“æœ
                    if trial.value > best_overall_result['best_score']:
                        best_overall_result = {
                            'best_score': trial.value,
                            'best_params': trial.params.copy(),
                            'best_metrics': trial_data['metrics'].copy(),
                            'trial_number': trial.number
                        }
            
            print(f"âœ… Trial {trial.number} å®Œæˆ: å¾—åˆ† = {trial.value:.2f}%")
            print(f"ğŸ“Š å½“å‰æœ€ä½³: {study.best_value:.2f}% (Trial {study.best_trial.number})")
            
            # ä¿å­˜å½“å‰æœ€ä½³ç»“æœ
            with open(os.path.join(results_dir, 'current_best_result.json'), 'w', encoding='utf-8') as f:
                json.dump(best_overall_result, f, ensure_ascii=False, indent=2)
        
        elif trial.state == optuna.trial.TrialState.PRUNED:
            print(f"âœ‚ï¸ Trial {trial.number} è¢«å‰ªæ")
    
    try:
        # å¼€å§‹ä¼˜åŒ–
        def objective(trial):
            score, metrics = train_single_config_for_optuna(trial, base_config, results_dir)
            return score
        
        study.optimize(
            objective,
            n_trials=1000,  # å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´è¯•éªŒæ¬¡æ•°
            timeout=None,  # ä¸è®¾ç½®æ—¶é—´é™åˆ¶
            callbacks=[save_callback]
        )
        
    except KeyboardInterrupt:
        print("ä¼˜åŒ–è¢«ç”¨æˆ·ä¸­æ–­")
    
    total_time = time.time() - start_time
    
    # ä¿å­˜æ‰€æœ‰è¯•éªŒçš„æ±‡æ€»ç»“æœ
    all_trials_summary = {
        'optimization_completed': datetime.now().isoformat(),
        'total_time_hours': total_time / 3600,
        'total_trials': len(study.trials),
        'completed_trials': len([t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]),
        'pruned_trials': len([t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]),
        'best_trial_number': study.best_trial.number if study.best_trial else -1,
        'best_score': study.best_value if study.best_value else 0,
        'all_trials_summary': all_trials_results
    }
    
    with open(os.path.join(results_dir, 'all_trials_summary.json'), 'w', encoding='utf-8') as f:
        json.dump(all_trials_summary, f, ensure_ascii=False, indent=2)
    
    # ä¿å­˜æœ€ä½³å‚æ•°ç»„åˆå’Œ6ä¸ªæŒ‡æ ‡
    if best_overall_result['trial_number'] != -1:
        final_best_result = {
            'best_hyperparameters': best_overall_result['best_params'],
            'best_6_metrics': best_overall_result['best_metrics'],
            'trial_number': best_overall_result['trial_number'],
            'optimization_summary': {
                'total_trials': len(study.trials),
                'total_time_hours': total_time / 3600,
                'optimization_date': datetime.now().isoformat()
            }
        }
        
        with open(os.path.join(results_dir, 'BEST_RESULT.json'), 'w', encoding='utf-8') as f:
            json.dump(final_best_result, f, ensure_ascii=False, indent=2)
    
    # æ‰“å°æœ€ç»ˆç»“æœ
    print(f"\n{'='*80}")
    print("ğŸ‰ Optunaè¶…å‚æ•°ä¼˜åŒ–å®Œæˆ!")
    print(f"â±ï¸  æ€»è€—æ—¶: {total_time/3600:.2f} å°æ—¶")
    print(f"ğŸ§ª æ€»è¯•éªŒæ•°: {len(study.trials)}")
    print(f"âœ… å®Œæˆè¯•éªŒæ•°: {len([t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE])}")
    print(f"âœ‚ï¸ å‰ªæè¯•éªŒæ•°: {len([t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED])}")
    
    if best_overall_result['trial_number'] != -1:
        print(f"\nğŸ† æœ€ä½³ç»“æœ (Trial {best_overall_result['trial_number']}):")
        print(f"ğŸ“ˆ Final Micro F1: {best_overall_result['best_score']:.2f}%")
        
        print(f"\nğŸ”§ æœ€ä½³è¶…å‚æ•°:")
        for param, value in best_overall_result['best_params'].items():
            print(f"   {param}: {value}")
        
        print(f"\nğŸ“Š å®Œæ•´6ä¸ªæŒ‡æ ‡:")
        metrics = best_overall_result['best_metrics']
        print(f"   1. Final Micro F1: {metrics['hierarchical_final_micro_f1']:.2f}%")
        print(f"   2. Final Macro F1: {metrics['hierarchical_final_macro_f1']:.2f}%")
        print(f"   3. Coarse Micro F1: {metrics['hierarchical_coarse_micro_f1']:.2f}%")
        print(f"   4. Fine Micro F1: {metrics['hierarchical_fine_micro_f1']:.2f}%")
        print(f"   5. Coarse Macro F1: {metrics['hierarchical_coarse_macro_f1']:.2f}%")
        print(f"   6. Fine Macro F1: {metrics['hierarchical_fine_macro_f1']:.2f}%")
        print(f"   ğŸ¯ æœ€ä½³epoch: {metrics['epoch']}")
    
    print(f"\nğŸ“ ç»“æœä¿å­˜åœ¨: {results_dir}/")
    print(f"   - BEST_RESULT.json: æœ€ä½³å‚æ•°å’Œ6ä¸ªæŒ‡æ ‡")
    print(f"   - all_trials_summary.json: æ‰€æœ‰è¯•éªŒæ±‡æ€»")
    print(f"   - trial_X_result.json: æ¯ä¸ªè¯•éªŒçš„è¯¦ç»†ç»“æœ")
    print(f"   - current_best_result.json: å®æ—¶æœ€ä½³ç»“æœ")
    
    return best_overall_result, study


if __name__ == '__main__':
    print("ğŸš€ å¯åŠ¨å±‚æ¬¡åŒ–é”™è¯¯åˆ†ç±»è¶…å‚æ•°ä¼˜åŒ–")
    print("ğŸ¯ ä¸»è¦è¯„åˆ¤æŒ‡æ ‡: Final Micro F1")
    print("ğŸ“Š è·Ÿè¸ª6ä¸ªå…³é”®æŒ‡æ ‡")
    print("ğŸ’¾ æ¯è½®ç»“æœä¿å­˜åˆ°JSONæ–‡ä»¶")
    
    # æ£€æŸ¥æ•°æ®è·¯å¾„
    data_paths = [
        '/mnt/cfs/huangzhiwei/NLP-WED0910/datas/train_merged_origin+qwen256.json',
        '/mnt/cfs/huangzhiwei/NLP-WED0910/datas/val.json'
    ]
    
    print("\nğŸ” æ£€æŸ¥æ•°æ®æ–‡ä»¶...")
    for path in data_paths:
        if os.path.exists(path):
            print(f"âœ… {path} - å­˜åœ¨")
        else:
            print(f"âŒ {path} - ä¸å­˜åœ¨")
            print(f"è¯·ä¿®æ”¹ä»£ç ä¸­çš„æ•°æ®è·¯å¾„æˆ–ç¡®ä¿æ–‡ä»¶å­˜åœ¨")
            exit(1)
    
    # æ£€æŸ¥Optunaæ˜¯å¦å®‰è£…
    try:
        import optuna
        print(f"âœ… Optunaç‰ˆæœ¬: {optuna.__version__}")
    except ImportError:
        print("âŒ è¯·å…ˆå®‰è£…Optuna: pip install optuna")
        exit(1)
    
    # å¼€å§‹ä¼˜åŒ–
    best_result, study = optuna_hyperparameter_optimization()
    
    print(f"\nğŸŠ ä¼˜åŒ–å®Œæˆ! æœ€ä½³é…ç½®å·²ä¿å­˜åˆ° hyperparameter_results/BEST_RESULT.json")
