# import torch
# import torch.nn as nn
# import torch.nn.functional as F
# import os
# import random
# import argparse
# import numpy as np
# import json
# from tqdm import tqdm
# from transformers import BertModel, AutoModel
# from torch.optim import AdamW
# from torch.utils.data import Dataset
# from torch.utils.data import DataLoader
# from transformers import AutoTokenizer
# from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score
# import time
# from datetime import datetime

# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# class ErrorDetectionDataset(Dataset):
#     def __init__(
#             self,
#             data_path,
#             coarse_labels={
#                 "å­—ç¬¦çº§é”™è¯¯": 0,
#                 "æˆåˆ†æ®‹ç¼ºåž‹é”™è¯¯": 1, 
#                 "æˆåˆ†èµ˜ä½™åž‹é”™è¯¯": 2,
#                 "æˆåˆ†æ­é…ä¸å½“åž‹é”™è¯¯": 3
#             },
#             fine_labels={
#                 "ç¼ºå­—æ¼å­—": 0,
#                 "é”™åˆ«å­—é”™è¯¯": 1,
#                 "ç¼ºå°‘æ ‡ç‚¹": 2,
#                 "é”™ç”¨æ ‡ç‚¹": 3,
#                 "ä¸»è¯­ä¸æ˜Ž": 4,
#                 "è°“è¯­æ®‹ç¼º": 5,
#                 "å®¾è¯­æ®‹ç¼º": 6,
#                 "å…¶ä»–æˆåˆ†æ®‹ç¼º": 7,
#                 "ä¸»è¯­å¤šä½™": 8,
#                 "è™šè¯å¤šä½™": 9,
#                 "å…¶ä»–æˆåˆ†å¤šä½™": 10,
#                 "è¯­åºä¸å½“": 11,
#                 "åŠ¨å®¾æ­é…ä¸å½“": 12,
#                 "å…¶ä»–æ­é…ä¸å½“": 13
#             }
#     ):
#         self.data_path = data_path
#         self.coarse_labels = coarse_labels
#         self.fine_labels = fine_labels
#         self._get_data()
    
#     def _get_data(self):
#         with open(self.data_path, 'r', encoding='utf-8') as f:
#             data = json.load(f)
        
#         self.data = []
#         for item in data:
#             sent = item['sent']
#             coarse_types = item['CourseGrainedErrorType']
#             fine_types = item['FineGrainedErrorType']
            
#             # æž„å»ºç²—ç²’åº¦æ ‡ç­¾ï¼ˆå¤šæ ‡ç­¾ï¼‰
#             coarse_label = [0] * len(self.coarse_labels)
#             for c_type in coarse_types:
#                 if c_type in self.coarse_labels:
#                     coarse_label[self.coarse_labels[c_type]] = 1
            
#             # æž„å»ºç»†ç²’åº¦æ ‡ç­¾ï¼ˆå¤šæ ‡ç­¾ï¼‰
#             fine_label = [0] * len(self.fine_labels)
#             for f_type in fine_types:
#                 if f_type in self.fine_labels:
#                     fine_label[self.fine_labels[f_type]] = 1
            
#             self.data.append((sent, coarse_label, fine_label, item.get('sent_id', -1)))
    
#     def __len__(self):
#         return len(self.data)
    
#     def get_coarse_labels(self):
#         return self.coarse_labels
    
#     def get_fine_labels(self):
#         return self.fine_labels
    
#     def __getitem__(self, idx):
#         return self.data[idx]


# class ErrorDetectionDataLoader:
#     def __init__(
#         self,
#         dataset,
#         batch_size=16,
#         max_length=128,
#         shuffle=True,
#         drop_last=True,
#         device=None,
#         tokenizer_name='/mnt/cfs/huangzhiwei/NLP-WED0910/projects/models/bge-large-zh-v1.5'
#     ):
#         self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
#         self.dataset = dataset
#         self.batch_size = batch_size
#         self.max_length = max_length
#         self.shuffle = shuffle
#         self.drop_last = drop_last
        
#         if device is None:
#             self.device = torch.device(
#                 'cuda' if torch.cuda.is_available() else 'cpu'
#             )
#         else:
#             self.device = device
        
#         self.loader = DataLoader(
#             dataset=self.dataset,
#             batch_size=self.batch_size,
#             collate_fn=self.collate_fn,
#             shuffle=self.shuffle,
#             drop_last=self.drop_last
#         )
    
#     def collate_fn(self, data):
#         sents = [item[0] for item in data]
#         coarse_labels = [item[1] for item in data]
#         fine_labels = [item[2] for item in data]
#         sent_ids = [item[3] for item in data]
        
#         # ç¼–ç æ–‡æœ¬
#         encoded = self.tokenizer.batch_encode_plus(
#             batch_text_or_text_pairs=sents,
#             truncation=True,
#             padding='max_length',
#             max_length=self.max_length,
#             return_tensors='pt',
#             return_length=True
#         )
        
#         input_ids = encoded['input_ids'].to(self.device)
#         attention_mask = encoded['attention_mask'].to(self.device)
        
#         # å¤„ç†æ ‡ç­¾
#         if coarse_labels[0] == -1:
#             coarse_labels = None
#             fine_labels = None
#         else:
#             coarse_labels = torch.tensor(coarse_labels, dtype=torch.float).to(self.device)
#             fine_labels = torch.tensor(fine_labels, dtype=torch.float).to(self.device)
        
#         return input_ids, attention_mask, coarse_labels, fine_labels, sent_ids
    
#     def __iter__(self):
#         for data in self.loader:
#             yield data
    
#     def __len__(self):
#         return len(self.loader)

# class MoELayer(nn.Module):
#     """Mixture of Experts Layer"""
#     def __init__(self, input_dim, expert_dim, num_experts=8, top_k=2, dropout=0.1, load_balance_weight=0.01):
#         super().__init__()
#         self.num_experts = num_experts
#         self.top_k = top_k
#         self.input_dim = input_dim
#         self.expert_dim = expert_dim
        
#         # Gateç½‘ç»œï¼Œç”¨äºŽé€‰æ‹©ä¸“å®¶
#         self.gate = nn.Linear(input_dim, num_experts)

#         # ä¸“å®¶ç½‘ç»œï¼šæ¯ä¸€ä¸ªä¸“å®¶éƒ½æ˜¯ä¸¤å±‚çš„MLP
#         self.experts = nn.ModuleList([
#             nn.Sequential(
#                 nn.Linear(input_dim, expert_dim),
#                 nn.ReLU(),
#                 nn.Dropout(dropout),
#                 nn.Linear(expert_dim, input_dim),
#                 nn.Dropout(dropout)
#             ) for _ in range(num_experts)
#         ])

#         # è´Ÿè½½å‡è¡¡æŸå¤±çš„æƒé‡
#         self.load_balance_weight = load_balance_weight
    
#     def forward(self, x):
#         batch_size, seq_len = x.size(0), x.size(1) if len(x.shape) > 2 else 1

#         # å¦‚æžœè¾“å…¥æ˜¯3D (batch, seq, hidden)ï¼Œæˆ‘ä»¬åªç”¨[CLS]ä½ç½®
#         if len(x.shape) == 3:
#             x = x[:, 0, :]  # å–[CLS]ä½ç½®

#         # Gateç½‘ç»œè¾“å‡º: é€‰æ‹©ä¸“å®¶çš„æ¦‚çŽ‡
#         gate_logits = self.gate(x) # (batch_size, num_experts)
#         gate_probs = F.softmax(gate_logits, dim=-1)

#         # é€‰æ‹©å‰top_kä¸ªä¸“å®¶
#         top_k_probs, top_k_indices = torch.topk(gate_probs, self.top_k, dim=-1)
#         top_k_probs = top_k_probs / top_k_probs.sum(dim=-1, keepdim=True)  # å½’ä¸€åŒ–

#         # è®¡ç®—ä¸“å®¶è¾“å‡º
#         expert_outputs = []
#         for i in range(self.num_experts):
#             expert_output = self.experts[i](x) # (batch_size, input_dim)
#             expert_outputs.append(expert_output)

#         expert_outputs = torch.stack(expert_outputs, dim=1)  # (batch_size, num_experts, input_dim)

#         # ç»„åˆtop-kä¸“å®¶çš„è¾“å‡º
#         final_output = torch.zeros_like(x) # (batch_size, input_dim)
#         for i in range(self.top_k):
#             expert_idx = top_k_indices[:, i]  # ä¿®å¤è¯­æ³•é”™è¯¯
#             expert_weight = top_k_probs[:, i:i+1]  # (batch_size, 1)

#             # é€‰æ‹©å¯¹åº”ä¸“å®¶çš„è¾“å‡º
#             selected_output = expert_outputs[torch.arange(batch_size), expert_idx]  # (batch_size, input_dim)
#             final_output += expert_weight * selected_output

#         # è®¡ç®—è´Ÿè½½å‡è¡¡æŸå¤±
#         load_balance_loss = self.compute_load_balance_loss(gate_probs)
        
#         return final_output, load_balance_loss

    
#     def compute_load_balance_loss(self, gate_probs):
#         """è®¡ç®—è´Ÿè½½å‡è¡¡æŸå¤±ï¼Œé¼“åŠ±ä¸“å®¶ä½¿ç”¨çš„å‡è¡¡æ€§"""
#         # è®¡ç®—æ¯ä¸ªä¸“å®¶è¢«é€‰æ‹©çš„å¹³å‡æ¦‚çŽ‡
#         expert_usage = gate_probs.mean(dim=0)  # (num_experts,)
        
#         # ç†æƒ³æƒ…å†µä¸‹æ¯ä¸ªä¸“å®¶è¢«ä½¿ç”¨çš„æ¦‚çŽ‡åº”è¯¥æ˜¯ 1/num_experts
#         ideal_usage = 1.0 / self.num_experts
        
#         # è®¡ç®—è´Ÿè½½å‡è¡¡æŸå¤±ï¼ˆä½¿ç”¨L2è·ç¦»ï¼‰
#         load_balance_loss = torch.sum((expert_usage - ideal_usage) ** 2)
        
#         return self.load_balance_weight * load_balance_loss


# class HierarchicalErrorClassifier(nn.Module):
#     def __init__(
#         self, 
#         pretrained_model_name, 
#         num_coarse_labels=4, 
#         num_fine_labels=14, 
#         dropout=0.2,
#         num_experts=8,
#         expert_dim=512,
#         top_k=2,
#         use_separate_moe=True,  # æ˜¯å¦ä¸ºç²—ç²’åº¦å’Œç»†ç²’åº¦ä½¿ç”¨ä¸åŒçš„MoE
#         load_balance_weight=0.01
#     ):
#         super().__init__()
        
#         self.bert = AutoModel.from_pretrained(pretrained_model_name)
#         self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()
#         self.use_separate_moe = use_separate_moe
        
#         hidden_size = self.bert.config.hidden_size
        
#         if use_separate_moe:
#             # ä¸ºç²—ç²’åº¦å’Œç»†ç²’åº¦åˆ†ç±»åˆ†åˆ«ä½¿ç”¨ä¸åŒçš„MoE
#             self.coarse_moe = MoELayer(
#                 input_dim=hidden_size,
#                 expert_dim=expert_dim,
#                 num_experts=num_experts,
#                 top_k=top_k,
#                 dropout=dropout,
#                 load_balance_weight=load_balance_weight
#             )
#             self.fine_moe = MoELayer(
#                 input_dim=hidden_size,
#                 expert_dim=expert_dim,
#                 num_experts=num_experts,
#                 top_k=top_k,
#                 dropout=dropout,
#                 load_balance_weight=load_balance_weight
#             )
#         else:
#             # å…±äº«ä¸€ä¸ªMoEå±‚
#             self.shared_moe = MoELayer(
#                 input_dim=hidden_size,
#                 expert_dim=expert_dim,
#                 num_experts=num_experts,
#                 top_k=top_k,
#                 dropout=dropout,
#                 load_balance_weight=load_balance_weight
#             )
        
#         # ç²—ç²’åº¦åˆ†ç±»å™¨
#         self.coarse_classifier = nn.Linear(self.bert.config.hidden_size, num_coarse_labels)
        
#         # ç»†ç²’åº¦åˆ†ç±»å™¨
#         self.fine_classifier = nn.Linear(self.bert.config.hidden_size, num_fine_labels)
        
#         # å®šä¹‰ç²—ç²’åº¦ç±»åˆ«å’Œå¯¹åº”çš„ç»†ç²’åº¦ç´¢å¼•çš„æ˜ å°„
#         self.coarse_to_fine_indices = {
#             0: [0, 1, 2, 3],    # å­—ç¬¦çº§é”™è¯¯
#             1: [4, 5, 6, 7],    # æˆåˆ†æ®‹ç¼ºåž‹é”™è¯¯
#             2: [8, 9, 10],      # æˆåˆ†å†—ä½™åž‹é”™è¯¯
#             3: [11, 12, 13]     # æˆåˆ†æ­é…ä¸å½“åž‹é”™è¯¯
#         }
    
#     def forward(self, input_ids, attention_mask, token_type_ids=None):
#         # BERTç¼–ç 
#         if token_type_ids is not None:
#             outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)
#         else:
#             outputs = self.bert(input_ids, attention_mask=attention_mask)
        
#         pooled_output = outputs.last_hidden_state[:,0,:]  # å–[CLS]çš„è¾“å‡º
#         pooled_output = self.dropout(pooled_output)

#         total_load_balance_loss = 0
        
#         if self.use_separate_moe:
#             # ä½¿ç”¨ä¸åŒçš„MoEå¤„ç†ç²—ç²’åº¦å’Œç»†ç²’åº¦åˆ†ç±»
#             coarse_features, coarse_lb_loss = self.coarse_moe(pooled_output)
#             fine_features, fine_lb_loss = self.fine_moe(pooled_output)
#             total_load_balance_loss = coarse_lb_loss + fine_lb_loss
#         else:
#             # ä½¿ç”¨å…±äº«MoE
#             shared_features, shared_lb_loss = self.shared_moe(pooled_output)
#             coarse_features = fine_features = shared_features
#             total_load_balance_loss = shared_lb_loss
        
#         # ç²—ç²’åº¦åˆ†ç±»
#         coarse_logits = self.coarse_classifier(pooled_output)
#         coarse_probs = torch.sigmoid(coarse_logits)
        
#         # ç»†ç²’åº¦åˆ†ç±»
#         fine_logits = self.fine_classifier(pooled_output)
#         fine_probs = torch.sigmoid(fine_logits)
        
#         return coarse_probs, fine_probs, total_load_balance_loss
    
#     def apply_hierarchical_constraint(self, coarse_preds, fine_preds):
#         """
#         åº”ç”¨å±‚æ¬¡çº¦æŸï¼šå¦‚æžœç²—ç²’åº¦ç±»åˆ«é¢„æµ‹ä¸ºè´Ÿï¼Œåˆ™è¯¥ç²—ç²’åº¦ä¸‹çš„æ‰€æœ‰ç»†ç²’åº¦ç±»åˆ«å‡è®¾ä¸ºè´Ÿ
#         """
#         constrained_fine_preds = fine_preds.clone()
        
#         # éåŽ†æ¯ä¸ªæ ·æœ¬
#         for i in range(coarse_preds.size(0)):
#             # å¯¹æ¯ä¸ªç²—ç²’åº¦ç±»åˆ«
#             for coarse_idx, fine_indices in self.coarse_to_fine_indices.items():
#                 # å¦‚æžœç²—ç²’åº¦ä¸ºè´Ÿï¼Œåˆ™å¯¹åº”çš„ç»†ç²’åº¦å…¨éƒ¨è®¾ä¸ºè´Ÿ
#                 if coarse_preds[i, coarse_idx] == 0:
#                     constrained_fine_preds[i, fine_indices] = 0
        
#         return constrained_fine_preds


# def calculate_metrics(labels, predictions, average='micro'):
#     """è®¡ç®—å„ç§è¯„ä¼°æŒ‡æ ‡"""
#     labels = np.array(labels)
#     predictions = np.array(predictions)
    
#     micro_f1 = f1_score(labels, predictions, average='micro')
#     macro_f1 = f1_score(labels, predictions, average='macro')
#     sample_acc = accuracy_score(labels, predictions)
    
#     return {
#         'micro_f1': micro_f1 * 100,
#         'macro_f1': macro_f1 * 100,
#         'accuracy': sample_acc * 100
#     }


# def train_model(config):
#     """ä¸»è®­ç»ƒå‡½æ•°"""
#     print("ðŸš€ å¼€å§‹è®­ç»ƒå±‚æ¬¡åŒ–é”™è¯¯åˆ†ç±»æ¨¡åž‹")
#     print(f"ðŸ“± è®¾å¤‡: {config['device']}")
#     print(f"ðŸ·ï¸  ç²—ç²’åº¦ç±»åˆ«æ•°: {config['num_coarse_labels']}")
#     print(f"ðŸ·ï¸  ç»†ç²’åº¦ç±»åˆ«æ•°: {config['num_fine_labels']}")
#     print(f"ðŸ“ æœ€å¤§åºåˆ—é•¿åº¦: {config['max_length']}")
#     print(f"ðŸ”„ æ‰¹æ¬¡å¤§å°: {config['batch_size']}")
#     print(f"ðŸ“š è®­ç»ƒè½®æ•°: {config['epochs']}")
#     print("="*80)
    
#     # è®¾ç½®éšæœºç§å­
#     random.seed(config['seed'])
#     np.random.seed(config['seed'])
#     torch.manual_seed(config['seed'])
#     torch.backends.cudnn.deterministic = True
#     torch.backends.cudnn.benchmark = False
    
#     # åˆ›å»ºç»“æžœä¿å­˜ç›®å½•
#     os.makedirs(config['results_dir'], exist_ok=True)
#     os.makedirs(config['checkpoint_dir'], exist_ok=True)
    
#     # åŠ è½½æ•°æ®é›†
#     print("ðŸ“‚ åŠ è½½æ•°æ®é›†...")
#     train_dataset = ErrorDetectionDataset(config['data_path'])
#     val_dataset = ErrorDetectionDataset(config['val_data_path'])
#     print(f"ðŸ“Š è®­ç»ƒé›†å¤§å°: {len(train_dataset)}")
#     print(f"ðŸ“Š éªŒè¯é›†å¤§å°: {len(val_dataset)}")
    
#     # åˆ›å»ºæ•°æ®åŠ è½½å™¨
#     train_dataloader = ErrorDetectionDataLoader(
#         dataset=train_dataset,
#         batch_size=config['batch_size'],
#         max_length=config['max_length'],
#         shuffle=True,
#         drop_last=True,
#         device=config['device'],
#         tokenizer_name=config['model_name']
#     )

#     val_dataloader = ErrorDetectionDataLoader(
#         dataset=val_dataset,
#         batch_size=config['batch_size'],
#         max_length=config['max_length'],
#         shuffle=False,
#         drop_last=False,
#         device=config['device'],
#         tokenizer_name=config['model_name']
#     )
    
#     # åˆ›å»ºæ¨¡åž‹
#     print("ðŸ—ï¸  æž„å»ºæ¨¡åž‹...")
#     model = HierarchicalErrorClassifier(
#         pretrained_model_name=config['model_name'],
#         num_coarse_labels=config['num_coarse_labels'],
#         num_fine_labels=config['num_fine_labels'],
#         dropout=config['dropout'],
#         num_experts=config['num_experts'],
#         expert_dim=config['expert_dim'],
#         top_k=config['top_k'],
#         use_separate_moe=config['use_separate_moe'],
#         load_balance_weight=config['load_balance_weight']
#     ).to(config['device'])
    
#     print(f"ðŸ”§ æ¨¡åž‹å‚æ•°:")
#     print(f"   - Dropout: {config['dropout']}")
#     print(f"   - ä¸“å®¶æ•°é‡: {config['num_experts']}")
#     print(f"   - Top-K: {config['top_k']}")
#     print(f"   - ä¸“å®¶ç»´åº¦: {config['expert_dim']}")
#     print(f"   - ç‹¬ç«‹MoE: {config['use_separate_moe']}")
#     print(f"   - è´Ÿè½½å‡è¡¡æƒé‡: {config['load_balance_weight']}")
    
#     # å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
#     criterion = nn.BCELoss()
#     optimizer = AdamW(
#         filter(lambda p: p.requires_grad, model.parameters()),
#         lr=config['lr'],
#         weight_decay=config.get('weight_decay', 0.01)
#     )
    
#     # åˆå§‹åŒ–æœ€ä½³éªŒè¯æ€§èƒ½å’Œ6ä¸ªæŒ‡æ ‡
#     best_metrics = {
#         'hierarchical_final_micro_f1': 0,
#         'hierarchical_final_macro_f1': 0,
#         'hierarchical_coarse_micro_f1': 0,
#         'hierarchical_fine_micro_f1': 0,
#         'hierarchical_coarse_macro_f1': 0,
#         'hierarchical_fine_macro_f1': 0,
#         'epoch': 0
#     }
    
#     best_val_f1 = 0
#     patience_counter = 0
    
#     # è®°å½•æ‰€æœ‰epochçš„ç»“æžœ
#     training_history = {
#         'train_loss': [],
#         'val_metrics': [],
#         'best_metrics': best_metrics,
#         'config': config
#     }
    
#     print("\nðŸŽ¯ å¼€å§‹è®­ç»ƒ...")
#     start_time = time.time()
    
#     # è®­ç»ƒå¾ªçŽ¯
#     for epoch in range(config['epochs']):
#         epoch_start_time = time.time()
        
#         # è®­ç»ƒé˜¶æ®µ
#         model.train()
#         train_loss = 0.0
#         train_steps = 0
        
#         train_progress = tqdm(train_dataloader, desc=f"Epoch {epoch+1}/{config['epochs']} [Train]")
#         for input_ids, attention_mask, coarse_labels, fine_labels, sent_ids in train_progress:
#             optimizer.zero_grad()
            
#             # å‰å‘ä¼ æ’­
#             coarse_probs, fine_probs, load_balance_loss = model(input_ids, attention_mask)
            
#             # è®¡ç®—åˆ†ç±»æŸå¤±
#             coarse_loss = criterion(coarse_probs, coarse_labels)
#             fine_loss = criterion(fine_probs, fine_labels)
#             classification_loss = coarse_loss + fine_loss
            
#             # æ€»æŸå¤± = åˆ†ç±»æŸå¤± + è´Ÿè½½å‡è¡¡æŸå¤±
#             total_loss = classification_loss + load_balance_loss
            
#             # åå‘ä¼ æ’­
#             total_loss.backward()
#             optimizer.step()
            
#             train_loss += classification_loss.item()
#             train_steps += 1
            
#             # æ›´æ–°è¿›åº¦æ¡
#             train_progress.set_postfix({
#                 'Loss': f'{classification_loss.item():.4f}',
#                 'LB_Loss': f'{load_balance_loss.item():.4f}'
#             })
        
#         avg_train_loss = train_loss / train_steps
        
#         # éªŒè¯é˜¶æ®µ
#         model.eval()
#         all_coarse_preds = []
#         all_coarse_labels = []
#         all_fine_labels = []
#         all_constrained_fine_preds = []
        
#         val_progress = tqdm(val_dataloader, desc=f"Epoch {epoch+1}/{config['epochs']} [Val]")
#         with torch.no_grad():
#             for input_ids, attention_mask, coarse_labels, fine_labels, sent_ids in val_progress:
#                 # å‰å‘ä¼ æ’­
#                 coarse_probs, fine_probs, load_balance_loss = model(input_ids, attention_mask)
                
#                 # åº”ç”¨å±‚æ¬¡çº¦æŸ
#                 coarse_preds = (coarse_probs > config['threshold']).float()
#                 fine_preds = (fine_probs > config['threshold']).float()
#                 constrained_fine_preds = model.apply_hierarchical_constraint(coarse_preds, fine_preds)
                
#                 # æ”¶é›†é¢„æµ‹ç»“æžœ
#                 all_coarse_preds.extend(coarse_preds.cpu().numpy())
#                 all_coarse_labels.extend(coarse_labels.cpu().numpy())
#                 all_constrained_fine_preds.extend(constrained_fine_preds.cpu().numpy())
#                 all_fine_labels.extend(fine_labels.cpu().numpy())
        
#         # è®¡ç®—éªŒè¯æŒ‡æ ‡
#         val_coarse_metrics_micro = calculate_metrics(all_coarse_labels, all_coarse_preds, average='micro')
#         val_coarse_metrics_macro = calculate_metrics(all_coarse_labels, all_coarse_preds, average='macro')
#         val_constrained_fine_metrics_micro = calculate_metrics(all_fine_labels, all_constrained_fine_preds, average='micro')
#         val_constrained_fine_metrics_macro = calculate_metrics(all_fine_labels, all_constrained_fine_preds, average='macro')
        
#         # è®¡ç®—6ä¸ªæŒ‡æ ‡
#         hierarchical_final_micro_f1 = (val_constrained_fine_metrics_micro['micro_f1'] + val_coarse_metrics_micro['micro_f1']) / 2
#         hierarchical_final_macro_f1 = (val_constrained_fine_metrics_macro['macro_f1'] + val_coarse_metrics_macro['macro_f1']) / 2
#         hierarchical_coarse_micro_f1 = val_coarse_metrics_micro['micro_f1']
#         hierarchical_fine_micro_f1 = val_constrained_fine_metrics_micro['micro_f1']
#         hierarchical_coarse_macro_f1 = val_coarse_metrics_macro['macro_f1']
#         hierarchical_fine_macro_f1 = val_constrained_fine_metrics_macro['macro_f1']
        
#         current_metrics = {
#             'hierarchical_final_micro_f1': hierarchical_final_micro_f1,
#             'hierarchical_final_macro_f1': hierarchical_final_macro_f1,
#             'hierarchical_coarse_micro_f1': hierarchical_coarse_micro_f1,
#             'hierarchical_fine_micro_f1': hierarchical_fine_micro_f1,
#             'hierarchical_coarse_macro_f1': hierarchical_coarse_macro_f1,
#             'hierarchical_fine_macro_f1': hierarchical_fine_macro_f1,
#             'epoch': epoch + 1
#         }
        
#         # è®°å½•è®­ç»ƒåŽ†å²
#         training_history['train_loss'].append(avg_train_loss)
#         training_history['val_metrics'].append(current_metrics.copy())
        
#         epoch_time = time.time() - epoch_start_time
        
#         # æ‰“å°å½“å‰epochç»“æžœ
#         print(f"\nðŸ“Š Epoch {epoch+1}/{config['epochs']} ç»“æžœ (è€—æ—¶: {epoch_time:.1f}s):")
#         print(f"   ðŸ”¥ è®­ç»ƒæŸå¤±: {avg_train_loss:.4f}")
#         print(f"   ðŸ“ˆ Final Micro F1: {hierarchical_final_micro_f1:.2f}%")
#         print(f"   ðŸ“ˆ Final Macro F1: {hierarchical_final_macro_f1:.2f}%")
#         print(f"   ðŸ“Š Coarse Micro F1: {hierarchical_coarse_micro_f1:.2f}%")
#         print(f"   ðŸ“Š Fine Micro F1: {hierarchical_fine_micro_f1:.2f}%")
#         print(f"   ðŸ“Š Coarse Macro F1: {hierarchical_coarse_macro_f1:.2f}%")
#         print(f"   ðŸ“Š Fine Macro F1: {hierarchical_fine_macro_f1:.2f}%")
        
#         # æ£€æŸ¥æ˜¯å¦ä¸ºæœ€ä½³æ€§èƒ½ï¼ˆä½¿ç”¨Final micro f1ä½œä¸ºä¸»è¦æŒ‡æ ‡ï¼‰
#         if hierarchical_final_micro_f1 > best_val_f1:
#             best_val_f1 = hierarchical_final_micro_f1
#             best_metrics = current_metrics.copy()
#             patience_counter = 0
            
#             # ä¿å­˜æœ€ä½³æ¨¡åž‹
#             model_save_path = os.path.join(config['checkpoint_dir'], 'best_model.pt')
#             # torch.save({
#             #     'epoch': epoch + 1,
#             #     'model_state_dict': model.state_dict(),
#             #     'optimizer_state_dict': optimizer.state_dict(),
#             #     'best_metrics': best_metrics,
#             #     'config': config
#             # }, model_save_path)
            
#             print(f"   ðŸŽ‰ æ–°çš„æœ€ä½³æ¨¡åž‹! å·²ä¿å­˜åˆ° {model_save_path}")
            
#         else:
#             patience_counter += 1
#             print(f"   â³ è€å¿ƒç­‰å¾…: {patience_counter}/{config['patience']}")
        
#         # æ›´æ–°è®­ç»ƒåŽ†å²ä¸­çš„æœ€ä½³æŒ‡æ ‡
#         training_history['best_metrics'] = best_metrics.copy()
        
#         # æ—©åœæ£€æŸ¥
#         if patience_counter >= config['patience']:
#             print(f"\nðŸ›‘ æ—©åœè§¦å‘! å·²è¿žç»­ {config['patience']} ä¸ªepochæ²¡æœ‰æ”¹å–„")
#             break
        
#         print("-" * 80)
    
#     total_time = time.time() - start_time
    
#     # ä¿å­˜å®Œæ•´çš„è®­ç»ƒåŽ†å²
#     training_history['total_time_hours'] = total_time / 3600
#     training_history['final_epoch'] = epoch + 1
#     training_history['early_stopped'] = patience_counter >= config['patience']
    
#     history_save_path = os.path.join(config['results_dir'], 'training_history.json')
#     with open(history_save_path, 'w', encoding='utf-8') as f:
#         json.dump(training_history, f, ensure_ascii=False, indent=2)
    
#     # ä¿å­˜æœ€ç»ˆç»“æžœ
#     final_result = {
#         'best_6_metrics': best_metrics,
#         'config': config,
#         'training_summary': {
#             'total_epochs': epoch + 1,
#             'total_time_hours': total_time / 3600,
#             'early_stopped': patience_counter >= config['patience'],
#             'best_epoch': best_metrics['epoch'],
#             'training_date': datetime.now().isoformat()
#         }
#     }
    
#     result_save_path = os.path.join(config['results_dir'], 'final_result.json')
#     with open(result_save_path, 'w', encoding='utf-8') as f:
#         json.dump(final_result, f, ensure_ascii=False, indent=2)
    
#     # æ‰“å°æœ€ç»ˆç»“æžœ
#     print(f"\n{'='*80}")
#     print("ðŸŽ‰ è®­ç»ƒå®Œæˆ!")
#     print(f"â±ï¸  æ€»è€—æ—¶: {total_time/3600:.2f} å°æ—¶")
#     print(f"ðŸ”„ è®­ç»ƒè½®æ•°: {epoch + 1}/{config['epochs']}")
#     print(f"ðŸŽ¯ æœ€ä½³epoch: {best_metrics['epoch']}")
#     if patience_counter >= config['patience']:
#         print("ðŸ›‘ æ—©åœè§¦å‘")
    
#     print(f"\nðŸ† æœ€ä½³6ä¸ªæŒ‡æ ‡:")
#     print(f"   1. Final Micro F1: {best_metrics['hierarchical_final_micro_f1']:.2f}%")
#     print(f"   2. Final Macro F1: {best_metrics['hierarchical_final_macro_f1']:.2f}%")
#     print(f"   3. Coarse Micro F1: {best_metrics['hierarchical_coarse_micro_f1']:.2f}%")
#     print(f"   4. Fine Micro F1: {best_metrics['hierarchical_fine_micro_f1']:.2f}%")
#     print(f"   5. Coarse Macro F1: {best_metrics['hierarchical_coarse_macro_f1']:.2f}%")
#     print(f"   6. Fine Macro F1: {best_metrics['hierarchical_fine_macro_f1']:.2f}%")
    
#     print(f"\nðŸ“ ç»“æžœå·²ä¿å­˜:")
#     print(f"   - æœ€ä½³æ¨¡åž‹: {os.path.join(config['checkpoint_dir'], 'best_model.pt')}")
#     print(f"   - è®­ç»ƒåŽ†å²: {history_save_path}")
#     print(f"   - æœ€ç»ˆç»“æžœ: {result_save_path}")
    
#     return best_metrics, training_history


# def main():
#     """ä¸»å‡½æ•°"""
#     parser = argparse.ArgumentParser(description='å±‚æ¬¡åŒ–é”™è¯¯åˆ†ç±»æ¨¡åž‹è®­ç»ƒ')
    
#     # æ•°æ®è·¯å¾„å‚æ•°
#     parser.add_argument('--data_path', type=str, 
#                        default='/mnt/cfs/huangzhiwei/NLP-WED0910/datas/train.json',
#                        help='è®­ç»ƒæ•°æ®è·¯å¾„')
#     parser.add_argument('--val_data_path', type=str,
#                        default='/mnt/cfs/huangzhiwei/NLP-WED0910/datas/val.json', 
#                        help='éªŒè¯æ•°æ®è·¯å¾„')
#     parser.add_argument('--model_name', type=str,
#                        default='/mnt/cfs/huangzhiwei/NLP-WED0910/projects/models/bge-large-zh-v1.5',
#                        help='é¢„è®­ç»ƒæ¨¡åž‹è·¯å¾„')
    
#     # æ¨¡åž‹å‚æ•°
#     parser.add_argument('--num_coarse_labels', type=int, default=4, help='ç²—ç²’åº¦æ ‡ç­¾æ•°')
#     parser.add_argument('--num_fine_labels', type=int, default=14, help='ç»†ç²’åº¦æ ‡ç­¾æ•°')
#     parser.add_argument('--max_length', type=int, default=128, help='æœ€å¤§åºåˆ—é•¿åº¦')
#     parser.add_argument('--dropout', type=float, default=0.3135999246870766, help='DropoutçŽ‡')
    
#     # MoEå‚æ•°
#     parser.add_argument('--num_experts', type=int, default=15, help='ä¸“å®¶æ•°é‡')
#     parser.add_argument('--expert_dim', type=int, default=512, help='ä¸“å®¶ç»´åº¦')
#     parser.add_argument('--top_k', type=int, default=1, help='Top-Kä¸“å®¶')
#     parser.add_argument('--use_separate_moe', type=bool, default=True, help='æ˜¯å¦ä½¿ç”¨ç‹¬ç«‹MoE')
#     parser.add_argument('--load_balance_weight', type=float, default=0.013540158381723238, help='è´Ÿè½½å‡è¡¡æƒé‡')
    
#     # è®­ç»ƒå‚æ•°
#     parser.add_argument('--batch_size', type=int, default=32, help='æ‰¹æ¬¡å¤§å°')
#     parser.add_argument('--lr', type=float, default=1.86884655838032e-05, help='å­¦ä¹ çŽ‡')
#     parser.add_argument('--weight_decay', type=float, default=0.01, help='æƒé‡è¡°å‡')
#     parser.add_argument('--epochs', type=int, default=40, help='è®­ç»ƒè½®æ•°')
#     parser.add_argument('--patience', type=int, default=5, help='æ—©åœè€å¿ƒå€¼')
#     parser.add_argument('--threshold', type=float, default=0.5, help='äºŒåˆ†ç±»é˜ˆå€¼')
#     parser.add_argument('--seed', type=int, default=3407, help='éšæœºç§å­')
    
#     # ä¿å­˜è·¯å¾„
#     parser.add_argument('--results_dir', type=str, default='training_results', help='ç»“æžœä¿å­˜ç›®å½•')
#     parser.add_argument('--checkpoint_dir', type=str, default='checkpoints', help='æ¨¡åž‹ä¿å­˜ç›®å½•')
    
#     args = parser.parse_args()
    
#     # è½¬æ¢ä¸ºé…ç½®å­—å…¸
#     config = vars(args)
#     config['device'] = device
    
#     # çº¦æŸæ£€æŸ¥
#     if config['top_k'] > config['num_experts']:
#         config['top_k'] = config['num_experts']
#         print(f"âš ï¸  Top-K ({args.top_k}) å¤§äºŽä¸“å®¶æ•°é‡ ({config['num_experts']})ï¼Œå·²è°ƒæ•´ä¸º {config['top_k']}")
    
#     # æ£€æŸ¥æ•°æ®æ–‡ä»¶
#     print("ðŸ” æ£€æŸ¥æ•°æ®æ–‡ä»¶...")
#     for path_key in ['data_path', 'val_data_path']:
#         if not os.path.exists(config[path_key]):
#             print(f"âŒ {config[path_key]} ä¸å­˜åœ¨")
#             return
#         else:
#             print(f"âœ… {config[path_key]} å­˜åœ¨")
    
#     # æ£€æŸ¥æ¨¡åž‹è·¯å¾„
#     if not os.path.exists(config['model_name']):
#         print(f"âŒ æ¨¡åž‹è·¯å¾„ {config['model_name']} ä¸å­˜åœ¨")
#         return
#     else:
#         print(f"âœ… æ¨¡åž‹è·¯å¾„å­˜åœ¨")
    
#     print(f"\nðŸŽ›ï¸  é…ç½®å‚æ•°:")
#     for key, value in config.items():
#         if key not in ['device']:
#             print(f"   {key}: {value}")
    
#     # å¼€å§‹è®­ç»ƒ
#     best_metrics, training_history = train_model(config)
    
#     print(f"\nâœ¨ è®­ç»ƒå®Œæˆ! æœ€ä½³ Final Micro F1: {best_metrics['hierarchical_final_micro_f1']:.2f}%")


# if __name__ == '__main__':
#     main()




import torch
import torch.nn as nn
import torch.nn.functional as F
import os
import random
import argparse
import numpy as np
import json
from tqdm import tqdm
from transformers import BertModel, AutoModel
from torch.optim import AdamW
from torch.utils.data import Dataset
from torch.utils.data import DataLoader
from transformers import AutoTokenizer
from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score
import time
from datetime import datetime

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

class ErrorDetectionDataset(Dataset):
    def __init__(
            self,
            data_path,
            coarse_labels={
                "å­—ç¬¦çº§é”™è¯¯": 0,
                "æˆåˆ†æ®‹ç¼ºåž‹é”™è¯¯": 1, 
                "æˆåˆ†èµ˜ä½™åž‹é”™è¯¯": 2,
                "æˆåˆ†æ­é…ä¸å½“åž‹é”™è¯¯": 3
            },
            fine_labels={
                "ç¼ºå­—æ¼å­—": 0,
                "é”™åˆ«å­—é”™è¯¯": 1,
                "ç¼ºå°‘æ ‡ç‚¹": 2,
                "é”™ç”¨æ ‡ç‚¹": 3,
                "ä¸»è¯­ä¸æ˜Ž": 4,
                "è°“è¯­æ®‹ç¼º": 5,
                "å®¾è¯­æ®‹ç¼º": 6,
                "å…¶ä»–æˆåˆ†æ®‹ç¼º": 7,
                "ä¸»è¯­å¤šä½™": 8,
                "è™šè¯å¤šä½™": 9,
                "å…¶ä»–æˆåˆ†å¤šä½™": 10,
                "è¯­åºä¸å½“": 11,
                "åŠ¨å®¾æ­é…ä¸å½“": 12,
                "å…¶ä»–æ­é…ä¸å½“": 13
            }
    ):
        self.data_path = data_path
        self.coarse_labels = coarse_labels
        self.fine_labels = fine_labels
        self._get_data()
    
    def _get_data(self):
        with open(self.data_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        self.data = []
        for item in data:
            sent = item['sent']
            coarse_types = item['CourseGrainedErrorType']
            fine_types = item['FineGrainedErrorType']
            
            # æž„å»ºç²—ç²’åº¦æ ‡ç­¾ï¼ˆå¤šæ ‡ç­¾ï¼‰
            coarse_label = [0] * len(self.coarse_labels)
            for c_type in coarse_types:
                if c_type in self.coarse_labels:
                    coarse_label[self.coarse_labels[c_type]] = 1
            
            # æž„å»ºç»†ç²’åº¦æ ‡ç­¾ï¼ˆå¤šæ ‡ç­¾ï¼‰
            fine_label = [0] * len(self.fine_labels)
            for f_type in fine_types:
                if f_type in self.fine_labels:
                    fine_label[self.fine_labels[f_type]] = 1
            
            self.data.append((sent, coarse_label, fine_label, item.get('sent_id', -1)))
    
    def __len__(self):
        return len(self.data)
    
    def get_coarse_labels(self):
        return self.coarse_labels
    
    def get_fine_labels(self):
        return self.fine_labels
    
    def __getitem__(self, idx):
        return self.data[idx]


class ErrorDetectionDataLoader:
    def __init__(
        self,
        dataset,
        batch_size=16,
        max_length=128,
        shuffle=True,
        drop_last=True,
        device=None,
        tokenizer_name='/mnt/cfs/huangzhiwei/NLP-WED0910/projects/models/bge-large-zh-v1.5'
    ):
        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
        self.dataset = dataset
        self.batch_size = batch_size
        self.max_length = max_length
        self.shuffle = shuffle
        self.drop_last = drop_last
        
        if device is None:
            self.device = torch.device(
                'cuda' if torch.cuda.is_available() else 'cpu'
            )
        else:
            self.device = device
        
        self.loader = DataLoader(
            dataset=self.dataset,
            batch_size=self.batch_size,
            collate_fn=self.collate_fn,
            shuffle=self.shuffle,
            drop_last=self.drop_last
        )
    
    def collate_fn(self, data):
        sents = [item[0] for item in data]
        coarse_labels = [item[1] for item in data]
        fine_labels = [item[2] for item in data]
        sent_ids = [item[3] for item in data]
        
        # ç¼–ç æ–‡æœ¬
        encoded = self.tokenizer.batch_encode_plus(
            batch_text_or_text_pairs=sents,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt',
            return_length=True
        )
        
        input_ids = encoded['input_ids'].to(self.device)
        attention_mask = encoded['attention_mask'].to(self.device)
        
        # å¤„ç†æ ‡ç­¾
        if coarse_labels[0] == -1:
            coarse_labels = None
            fine_labels = None
        else:
            coarse_labels = torch.tensor(coarse_labels, dtype=torch.float).to(self.device)
            fine_labels = torch.tensor(fine_labels, dtype=torch.float).to(self.device)
        
        return input_ids, attention_mask, coarse_labels, fine_labels, sent_ids
    
    def __iter__(self):
        for data in self.loader:
            yield data
    
    def __len__(self):
        return len(self.loader)

class MoELayer(nn.Module):
    """Mixture of Experts Layer"""
    def __init__(self, input_dim, expert_dim, num_experts=8, top_k=2, dropout=0.1, load_balance_weight=0.01):
        super().__init__()
        self.num_experts = num_experts
        self.top_k = top_k
        self.input_dim = input_dim
        self.expert_dim = expert_dim
        
        # Gateç½‘ç»œï¼Œç”¨äºŽé€‰æ‹©ä¸“å®¶
        self.gate = nn.Linear(input_dim, num_experts)

        # ä¸“å®¶ç½‘ç»œï¼šæ¯ä¸€ä¸ªä¸“å®¶éƒ½æ˜¯ä¸¤å±‚çš„MLP
        self.experts = nn.ModuleList([
            nn.Sequential(
                nn.Linear(input_dim, expert_dim),
                nn.ReLU(),
                nn.Dropout(dropout),
                nn.Linear(expert_dim, input_dim),
                nn.Dropout(dropout)
            ) for _ in range(num_experts)
        ])

        # è´Ÿè½½å‡è¡¡æŸå¤±çš„æƒé‡
        self.load_balance_weight = load_balance_weight
    
    def forward(self, x):
        batch_size, seq_len = x.size(0), x.size(1) if len(x.shape) > 2 else 1

        # å¦‚æžœè¾“å…¥æ˜¯3D (batch, seq, hidden)ï¼Œæˆ‘ä»¬åªç”¨[CLS]ä½ç½®
        if len(x.shape) == 3:
            x = x[:, 0, :]  # å–[CLS]ä½ç½®

        # Gateç½‘ç»œè¾“å‡º: é€‰æ‹©ä¸“å®¶çš„æ¦‚çŽ‡
        gate_logits = self.gate(x) # (batch_size, num_experts)
        gate_probs = F.softmax(gate_logits, dim=-1)

        # é€‰æ‹©å‰top_kä¸ªä¸“å®¶
        top_k_probs, top_k_indices = torch.topk(gate_probs, self.top_k, dim=-1)
        top_k_probs = top_k_probs / top_k_probs.sum(dim=-1, keepdim=True)  # å½’ä¸€åŒ–

        # è®¡ç®—ä¸“å®¶è¾“å‡º
        expert_outputs = []
        for i in range(self.num_experts):
            expert_output = self.experts[i](x) # (batch_size, input_dim)
            expert_outputs.append(expert_output)

        expert_outputs = torch.stack(expert_outputs, dim=1)  # (batch_size, num_experts, input_dim)

        # ç»„åˆtop-kä¸“å®¶çš„è¾“å‡º
        final_output = torch.zeros_like(x) # (batch_size, input_dim)
        for i in range(self.top_k):
            expert_idx = top_k_indices[:, i]  # ä¿®å¤è¯­æ³•é”™è¯¯
            expert_weight = top_k_probs[:, i:i+1]  # (batch_size, 1)

            # é€‰æ‹©å¯¹åº”ä¸“å®¶çš„è¾“å‡º
            selected_output = expert_outputs[torch.arange(batch_size), expert_idx]  # (batch_size, input_dim)
            final_output += expert_weight * selected_output

        # è®¡ç®—è´Ÿè½½å‡è¡¡æŸå¤±
        load_balance_loss = self.compute_load_balance_loss(gate_probs)
        
        return final_output, load_balance_loss

    
    def compute_load_balance_loss(self, gate_probs):
        """è®¡ç®—è´Ÿè½½å‡è¡¡æŸå¤±ï¼Œé¼“åŠ±ä¸“å®¶ä½¿ç”¨çš„å‡è¡¡æ€§"""
        # è®¡ç®—æ¯ä¸ªä¸“å®¶è¢«é€‰æ‹©çš„å¹³å‡æ¦‚çŽ‡
        expert_usage = gate_probs.mean(dim=0)  # (num_experts,)
        
        # ç†æƒ³æƒ…å†µä¸‹æ¯ä¸ªä¸“å®¶è¢«ä½¿ç”¨çš„æ¦‚çŽ‡åº”è¯¥æ˜¯ 1/num_experts
        ideal_usage = 1.0 / self.num_experts
        
        # è®¡ç®—è´Ÿè½½å‡è¡¡æŸå¤±ï¼ˆä½¿ç”¨L2è·ç¦»ï¼‰
        load_balance_loss = torch.sum((expert_usage - ideal_usage) ** 2)
        
        return self.load_balance_weight * load_balance_loss


class HierarchicalErrorClassifier(nn.Module):
    def __init__(
        self, 
        pretrained_model_name, 
        num_coarse_labels=4, 
        num_fine_labels=14, 
        dropout=0.2,
        num_experts=8,
        expert_dim=512,
        top_k=2,
        use_separate_moe=True,  # æ˜¯å¦ä¸ºç²—ç²’åº¦å’Œç»†ç²’åº¦ä½¿ç”¨ä¸åŒçš„MoE
        load_balance_weight=0.01
    ):
        super().__init__()
        
        self.bert = AutoModel.from_pretrained(pretrained_model_name)

        # # è§£å†»æœ€åŽå››å±‚å‚æ•°
        # # å†»ç»“BERTåº•å±‚ï¼Œä¿ç•™é¡¶å±‚å¾®è°ƒ
        # modules = [self.bert.embeddings, *self.bert.encoder.layer[:8]]
        # for module in modules:
        #     for param in module.parameters():
        #         param.requires_grad = False


        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()
        self.use_separate_moe = use_separate_moe
        
        hidden_size = self.bert.config.hidden_size

        
        if use_separate_moe:
            # ä¸ºç²—ç²’åº¦å’Œç»†ç²’åº¦åˆ†ç±»åˆ†åˆ«ä½¿ç”¨ä¸åŒçš„MoE
            self.coarse_moe = MoELayer(
                input_dim=hidden_size,
                expert_dim=expert_dim,
                num_experts=num_experts,
                top_k=top_k,
                dropout=dropout,
                load_balance_weight=load_balance_weight
            )
            self.fine_moe = MoELayer(
                input_dim=hidden_size,
                expert_dim=expert_dim,
                num_experts=num_experts,
                top_k=top_k,
                dropout=dropout,
                load_balance_weight=load_balance_weight
            )
        else:
            # å…±äº«ä¸€ä¸ªMoEå±‚
            self.shared_moe = MoELayer(
                input_dim=hidden_size,
                expert_dim=expert_dim,
                num_experts=num_experts,
                top_k=top_k,
                dropout=dropout,
                load_balance_weight=load_balance_weight
            )
        
        # ç²—ç²’åº¦åˆ†ç±»å™¨
        self.coarse_classifier = nn.Linear(self.bert.config.hidden_size, num_coarse_labels)
        
        # ç»†ç²’åº¦åˆ†ç±»å™¨
        self.fine_classifier = nn.Linear(self.bert.config.hidden_size, num_fine_labels)
        
        # å®šä¹‰ç²—ç²’åº¦ç±»åˆ«å’Œå¯¹åº”çš„ç»†ç²’åº¦ç´¢å¼•çš„æ˜ å°„
        self.coarse_to_fine_indices = {
            0: [0, 1, 2, 3],    # å­—ç¬¦çº§é”™è¯¯
            1: [4, 5, 6, 7],    # æˆåˆ†æ®‹ç¼ºåž‹é”™è¯¯
            2: [8, 9, 10],      # æˆåˆ†å†—ä½™åž‹é”™è¯¯
            3: [11, 12, 13]     # æˆåˆ†æ­é…ä¸å½“åž‹é”™è¯¯
        }
    
    def forward(self, input_ids, attention_mask, token_type_ids=None):
        # BERTç¼–ç 
        if token_type_ids is not None:
            outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)
        else:
            outputs = self.bert(input_ids, attention_mask=attention_mask)
        
        pooled_output = outputs.last_hidden_state[:,0,:]  # å–[CLS]çš„è¾“å‡º
        pooled_output = self.dropout(pooled_output)

        total_load_balance_loss = 0
        
        if self.use_separate_moe:
            # ä½¿ç”¨ä¸åŒçš„MoEå¤„ç†ç²—ç²’åº¦å’Œç»†ç²’åº¦åˆ†ç±»
            coarse_features, coarse_lb_loss = self.coarse_moe(pooled_output)
            fine_features, fine_lb_loss = self.fine_moe(pooled_output)
            total_load_balance_loss = coarse_lb_loss + fine_lb_loss
        else:
            # ä½¿ç”¨å…±äº«MoE
            shared_features, shared_lb_loss = self.shared_moe(pooled_output)
            coarse_features = fine_features = shared_features
            total_load_balance_loss = shared_lb_loss
        
        # ç²—ç²’åº¦åˆ†ç±»
        coarse_logits = self.coarse_classifier(pooled_output)
        coarse_probs = torch.sigmoid(coarse_logits)
        
        # ç»†ç²’åº¦åˆ†ç±»
        fine_logits = self.fine_classifier(pooled_output)
        fine_probs = torch.sigmoid(fine_logits)
        
        return coarse_probs, fine_probs, total_load_balance_loss
    
    def apply_hierarchical_constraint(self, coarse_preds, fine_preds):
        """
        åº”ç”¨å±‚æ¬¡çº¦æŸï¼šå¦‚æžœç²—ç²’åº¦ç±»åˆ«é¢„æµ‹ä¸ºè´Ÿï¼Œåˆ™è¯¥ç²—ç²’åº¦ä¸‹çš„æ‰€æœ‰ç»†ç²’åº¦ç±»åˆ«å‡è®¾ä¸ºè´Ÿ
        """
        constrained_fine_preds = fine_preds.clone()
        
        # éåŽ†æ¯ä¸ªæ ·æœ¬
        for i in range(coarse_preds.size(0)):
            # å¯¹æ¯ä¸ªç²—ç²’åº¦ç±»åˆ«
            for coarse_idx, fine_indices in self.coarse_to_fine_indices.items():
                # å¦‚æžœç²—ç²’åº¦ä¸ºè´Ÿï¼Œåˆ™å¯¹åº”çš„ç»†ç²’åº¦å…¨éƒ¨è®¾ä¸ºè´Ÿ
                if coarse_preds[i, coarse_idx] == 0:
                    constrained_fine_preds[i, fine_indices] = 0
        
        return constrained_fine_preds


def calculate_metrics(labels, predictions, average='micro'):
    """è®¡ç®—å„ç§è¯„ä¼°æŒ‡æ ‡"""
    labels = np.array(labels)
    predictions = np.array(predictions)
    
    micro_f1 = f1_score(labels, predictions, average='micro')
    macro_f1 = f1_score(labels, predictions, average='macro')
    sample_acc = accuracy_score(labels, predictions)
    
    return {
        'micro_f1': micro_f1 * 100,
        'macro_f1': macro_f1 * 100,
        'accuracy': sample_acc * 100
    }


def train_model(config):
    """ä¸»è®­ç»ƒå‡½æ•°"""
    print("ðŸš€ å¼€å§‹è®­ç»ƒå±‚æ¬¡åŒ–é”™è¯¯åˆ†ç±»æ¨¡åž‹")
    print(f"ðŸ“± è®¾å¤‡: {config['device']}")
    print(f"ðŸ·ï¸  ç²—ç²’åº¦ç±»åˆ«æ•°: {config['num_coarse_labels']}")
    print(f"ðŸ·ï¸  ç»†ç²’åº¦ç±»åˆ«æ•°: {config['num_fine_labels']}")
    print(f"ðŸ“ æœ€å¤§åºåˆ—é•¿åº¦: {config['max_length']}")
    print(f"ðŸ”„ æ‰¹æ¬¡å¤§å°: {config['batch_size']}")
    print(f"ðŸ“š è®­ç»ƒè½®æ•°: {config['epochs']}")
    print("="*80)
    
    # è®¾ç½®éšæœºç§å­
    random.seed(config['seed'])
    np.random.seed(config['seed'])
    torch.manual_seed(config['seed'])
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    
    # åˆ›å»ºç»“æžœä¿å­˜ç›®å½•
    os.makedirs(config['results_dir'], exist_ok=True)
    os.makedirs(config['checkpoint_dir'], exist_ok=True)
    
    # åŠ è½½æ•°æ®é›†
    print("ðŸ“‚ åŠ è½½æ•°æ®é›†...")
    train_dataset = ErrorDetectionDataset(config['data_path'])
    val_dataset = ErrorDetectionDataset(config['val_data_path'])
    print(f"ðŸ“Š è®­ç»ƒé›†å¤§å°: {len(train_dataset)}")
    print(f"ðŸ“Š éªŒè¯é›†å¤§å°: {len(val_dataset)}")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_dataloader = ErrorDetectionDataLoader(
        dataset=train_dataset,
        batch_size=config['batch_size'],
        max_length=config['max_length'],
        shuffle=True,
        drop_last=True,
        device=config['device'],
        tokenizer_name=config['model_name']
    )

    val_dataloader = ErrorDetectionDataLoader(
        dataset=val_dataset,
        batch_size=config['batch_size'],
        max_length=config['max_length'],
        shuffle=False,
        drop_last=False,
        device=config['device'],
        tokenizer_name=config['model_name']
    )
    
    # åˆ›å»ºæ¨¡åž‹
    print("ðŸ—ï¸  æž„å»ºæ¨¡åž‹...")
    model = HierarchicalErrorClassifier(
        pretrained_model_name=config['model_name'],
        num_coarse_labels=config['num_coarse_labels'],
        num_fine_labels=config['num_fine_labels'],
        dropout=config['dropout'],
        num_experts=config['num_experts'],
        expert_dim=config['expert_dim'],
        top_k=config['top_k'],
        use_separate_moe=config['use_separate_moe'],
        load_balance_weight=config['load_balance_weight']
    ).to(config['device'])
    
    print(f"ðŸ”§ æ¨¡åž‹å‚æ•°:")
    print(f"   - Dropout: {config['dropout']}")
    print(f"   - ä¸“å®¶æ•°é‡: {config['num_experts']}")
    print(f"   - Top-K: {config['top_k']}")
    print(f"   - ä¸“å®¶ç»´åº¦: {config['expert_dim']}")
    print(f"   - ç‹¬ç«‹MoE: {config['use_separate_moe']}")
    print(f"   - è´Ÿè½½å‡è¡¡æƒé‡: {config['load_balance_weight']}")
    
    # å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.BCELoss()
    optimizer = AdamW(
        filter(lambda p: p.requires_grad, model.parameters()),
        lr=config['lr'],
        weight_decay=config.get('weight_decay', 0.01)
    )
    
    # åˆå§‹åŒ–æœ€ä½³éªŒè¯æ€§èƒ½å’Œ6ä¸ªæŒ‡æ ‡
    best_metrics = {
        'hierarchical_final_micro_f1': 0,
        'hierarchical_final_macro_f1': 0,
        'hierarchical_coarse_micro_f1': 0,
        'hierarchical_fine_micro_f1': 0,
        'hierarchical_coarse_macro_f1': 0,
        'hierarchical_fine_macro_f1': 0,
        'epoch': 0
    }
    
    best_val_f1 = 0
    patience_counter = 0
    
    # è®°å½•æ‰€æœ‰epochçš„ç»“æžœ (å¤„ç†configä¸­ä¸èƒ½åºåˆ—åŒ–çš„å¯¹è±¡)
    serializable_config = config.copy()
    serializable_config['device'] = str(config['device'])  # è½¬æ¢deviceä¸ºå­—ç¬¦ä¸²
    
    training_history = {
        'train_loss': [],
        'val_metrics': [],
        'best_metrics': best_metrics,
        'config': serializable_config
    }
    
    print("\nðŸŽ¯ å¼€å§‹è®­ç»ƒ...")
    start_time = time.time()
    
    # è®­ç»ƒå¾ªçŽ¯
    for epoch in range(config['epochs']):
        epoch_start_time = time.time()
        
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0.0
        train_steps = 0
        
        train_progress = tqdm(train_dataloader, desc=f"Epoch {epoch+1}/{config['epochs']} [Train]")
        for input_ids, attention_mask, coarse_labels, fine_labels, sent_ids in train_progress:
            optimizer.zero_grad()
            
            # å‰å‘ä¼ æ’­
            coarse_probs, fine_probs, load_balance_loss = model(input_ids, attention_mask)
            
            # è®¡ç®—åˆ†ç±»æŸå¤±
            coarse_loss = criterion(coarse_probs, coarse_labels)
            fine_loss = criterion(fine_probs, fine_labels)
            classification_loss = coarse_loss + fine_loss
            
            # æ€»æŸå¤± = åˆ†ç±»æŸå¤± + è´Ÿè½½å‡è¡¡æŸå¤±
            total_loss = classification_loss + load_balance_loss
            
            # åå‘ä¼ æ’­
            total_loss.backward()
            optimizer.step()
            
            train_loss += classification_loss.item()
            train_steps += 1
            
            # æ›´æ–°è¿›åº¦æ¡
            train_progress.set_postfix({
                'Loss': f'{classification_loss.item():.4f}',
                'LB_Loss': f'{load_balance_loss.item():.4f}'
            })
        
        avg_train_loss = train_loss / train_steps
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        all_coarse_preds = []
        all_coarse_labels = []
        all_fine_labels = []
        all_constrained_fine_preds = []
        
        val_progress = tqdm(val_dataloader, desc=f"Epoch {epoch+1}/{config['epochs']} [Val]")
        with torch.no_grad():
            for input_ids, attention_mask, coarse_labels, fine_labels, sent_ids in val_progress:
                # å‰å‘ä¼ æ’­
                coarse_probs, fine_probs, load_balance_loss = model(input_ids, attention_mask)
                
                # åº”ç”¨å±‚æ¬¡çº¦æŸ
                coarse_preds = (coarse_probs > config['threshold']).float()
                fine_preds = (fine_probs > config['threshold']).float()
                constrained_fine_preds = model.apply_hierarchical_constraint(coarse_preds, fine_preds)
                
                # æ”¶é›†é¢„æµ‹ç»“æžœ
                all_coarse_preds.extend(coarse_preds.cpu().numpy())
                all_coarse_labels.extend(coarse_labels.cpu().numpy())
                all_constrained_fine_preds.extend(constrained_fine_preds.cpu().numpy())
                all_fine_labels.extend(fine_labels.cpu().numpy())
        
        # è®¡ç®—éªŒè¯æŒ‡æ ‡
        val_coarse_metrics_micro = calculate_metrics(all_coarse_labels, all_coarse_preds, average='micro')
        val_coarse_metrics_macro = calculate_metrics(all_coarse_labels, all_coarse_preds, average='macro')
        val_constrained_fine_metrics_micro = calculate_metrics(all_fine_labels, all_constrained_fine_preds, average='micro')
        val_constrained_fine_metrics_macro = calculate_metrics(all_fine_labels, all_constrained_fine_preds, average='macro')
        
        # è®¡ç®—6ä¸ªæŒ‡æ ‡
        hierarchical_final_micro_f1 = (val_constrained_fine_metrics_micro['micro_f1'] + val_coarse_metrics_micro['micro_f1']) / 2
        hierarchical_final_macro_f1 = (val_constrained_fine_metrics_macro['macro_f1'] + val_coarse_metrics_macro['macro_f1']) / 2
        hierarchical_coarse_micro_f1 = val_coarse_metrics_micro['micro_f1']
        hierarchical_fine_micro_f1 = val_constrained_fine_metrics_micro['micro_f1']
        hierarchical_coarse_macro_f1 = val_coarse_metrics_macro['macro_f1']
        hierarchical_fine_macro_f1 = val_constrained_fine_metrics_macro['macro_f1']
        
        current_metrics = {
            'hierarchical_final_micro_f1': hierarchical_final_micro_f1,
            'hierarchical_final_macro_f1': hierarchical_final_macro_f1,
            'hierarchical_coarse_micro_f1': hierarchical_coarse_micro_f1,
            'hierarchical_fine_micro_f1': hierarchical_fine_micro_f1,
            'hierarchical_coarse_macro_f1': hierarchical_coarse_macro_f1,
            'hierarchical_fine_macro_f1': hierarchical_fine_macro_f1,
            'epoch': epoch + 1
        }
        
        # è®°å½•è®­ç»ƒåŽ†å²
        training_history['train_loss'].append(avg_train_loss)
        training_history['val_metrics'].append(current_metrics.copy())
        
        epoch_time = time.time() - epoch_start_time
        
        # æ‰“å°å½“å‰epochç»“æžœ
        print(f"\nðŸ“Š Epoch {epoch+1}/{config['epochs']} ç»“æžœ (è€—æ—¶: {epoch_time:.1f}s):")
        print(f"   ðŸ”¥ è®­ç»ƒæŸå¤±: {avg_train_loss:.4f}")
        print(f"   ðŸ“ˆ Final Micro F1: {hierarchical_final_micro_f1:.2f}%")
        print(f"   ðŸ“ˆ Final Macro F1: {hierarchical_final_macro_f1:.2f}%")
        print(f"   ðŸ“Š Coarse Micro F1: {hierarchical_coarse_micro_f1:.2f}%")
        print(f"   ðŸ“Š Fine Micro F1: {hierarchical_fine_micro_f1:.2f}%")
        print(f"   ðŸ“Š Coarse Macro F1: {hierarchical_coarse_macro_f1:.2f}%")
        print(f"   ðŸ“Š Fine Macro F1: {hierarchical_fine_macro_f1:.2f}%")
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºæœ€ä½³æ€§èƒ½ï¼ˆä½¿ç”¨Final micro f1ä½œä¸ºä¸»è¦æŒ‡æ ‡ï¼‰
        if hierarchical_final_micro_f1 > best_val_f1:
            best_val_f1 = hierarchical_final_micro_f1
            best_metrics = current_metrics.copy()
            patience_counter = 0
            
            # ä¿å­˜æœ€ä½³æ¨¡åž‹
            model_save_path = os.path.join(config['checkpoint_dir'], 'best_model.pt')
            # torch.save({
            #     'epoch': epoch + 1,
            #     'model_state_dict': model.state_dict(),
            #     'optimizer_state_dict': optimizer.state_dict(),
            #     'best_metrics': best_metrics,
            #     'config': serializable_config_final
            # }, model_save_path)
            
            print(f"   ðŸŽ‰ æ–°çš„æœ€ä½³æ¨¡åž‹! å·²ä¿å­˜åˆ° {model_save_path}")
            
        else:
            patience_counter += 1
            print(f"   â³ è€å¿ƒç­‰å¾…: {patience_counter}/{config['patience']}")
        
        # æ›´æ–°è®­ç»ƒåŽ†å²ä¸­çš„æœ€ä½³æŒ‡æ ‡
        training_history['best_metrics'] = best_metrics.copy()
        
        # æ—©åœæ£€æŸ¥
        if patience_counter >= config['patience']:
            print(f"\nðŸ›‘ æ—©åœè§¦å‘! å·²è¿žç»­ {config['patience']} ä¸ªepochæ²¡æœ‰æ”¹å–„")
            break
        
        print("-" * 80)
    
    total_time = time.time() - start_time
    
    # ä¿å­˜å®Œæ•´çš„è®­ç»ƒåŽ†å²
    training_history['total_time_hours'] = total_time / 3600
    training_history['final_epoch'] = epoch + 1
    training_history['early_stopped'] = patience_counter >= config['patience']
    
    history_save_path = os.path.join(config['results_dir'], 'training_history.json')
    with open(history_save_path, 'w', encoding='utf-8') as f:
        json.dump(training_history, f, ensure_ascii=False, indent=2)
    
    # ä¿å­˜æœ€ç»ˆç»“æžœ (å¤„ç†configä¸­ä¸èƒ½åºåˆ—åŒ–çš„å¯¹è±¡)
    serializable_config_final = config.copy()
    serializable_config_final['device'] = str(config['device'])  # è½¬æ¢deviceä¸ºå­—ç¬¦ä¸²
    
    final_result = {
        'best_6_metrics': best_metrics,
        'config': serializable_config_final,
        'training_summary': {
            'total_epochs': epoch + 1,
            'total_time_hours': total_time / 3600,
            'early_stopped': patience_counter >= config['patience'],
            'best_epoch': best_metrics['epoch'],
            'training_date': datetime.now().isoformat()
        }
    }
    
    result_save_path = os.path.join(config['results_dir'], 'final_result.json')
    with open(result_save_path, 'w', encoding='utf-8') as f:
        json.dump(final_result, f, ensure_ascii=False, indent=2)
    
    # æ‰“å°æœ€ç»ˆç»“æžœ
    print(f"\n{'='*80}")
    print("ðŸŽ‰ è®­ç»ƒå®Œæˆ!")
    print(f"â±ï¸  æ€»è€—æ—¶: {total_time/3600:.2f} å°æ—¶")
    print(f"ðŸ”„ è®­ç»ƒè½®æ•°: {epoch + 1}/{config['epochs']}")
    print(f"ðŸŽ¯ æœ€ä½³epoch: {best_metrics['epoch']}")
    if patience_counter >= config['patience']:
        print("ðŸ›‘ æ—©åœè§¦å‘")
    
    print(f"\nðŸ† æœ€ä½³6ä¸ªæŒ‡æ ‡:")
    print(f"   1. Final Micro F1: {best_metrics['hierarchical_final_micro_f1']:.2f}%")
    print(f"   2. Final Macro F1: {best_metrics['hierarchical_final_macro_f1']:.2f}%")
    print(f"   3. Coarse Micro F1: {best_metrics['hierarchical_coarse_micro_f1']:.2f}%")
    print(f"   4. Fine Micro F1: {best_metrics['hierarchical_fine_micro_f1']:.2f}%")
    print(f"   5. Coarse Macro F1: {best_metrics['hierarchical_coarse_macro_f1']:.2f}%")
    print(f"   6. Fine Macro F1: {best_metrics['hierarchical_fine_macro_f1']:.2f}%")
    
    print(f"\nðŸ“ ç»“æžœå·²ä¿å­˜:")
    print(f"   - æœ€ä½³æ¨¡åž‹: {os.path.join(config['checkpoint_dir'], 'best_model.pt')}")
    print(f"   - è®­ç»ƒåŽ†å²: {history_save_path}")
    print(f"   - æœ€ç»ˆç»“æžœ: {result_save_path}")
    
    return best_metrics, training_history


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='å±‚æ¬¡åŒ–é”™è¯¯åˆ†ç±»æ¨¡åž‹è®­ç»ƒ')
    
    # æ•°æ®è·¯å¾„å‚æ•°
    parser.add_argument('--data_path', type=str, 
                       default='/mnt/cfs/huangzhiwei/NLP-WED0910/datas/train_new.json',
                       help='è®­ç»ƒæ•°æ®è·¯å¾„')
    parser.add_argument('--val_data_path', type=str,
                       default='/mnt/cfs/huangzhiwei/NLP-WED0910/datas/val.json', 
                       help='éªŒè¯æ•°æ®è·¯å¾„')
    parser.add_argument('--model_name', type=str,
                       default='/mnt/cfs/huangzhiwei/NLP-WED0910/projects/models/bge-large-zh-v1.5',
                       help='é¢„è®­ç»ƒæ¨¡åž‹è·¯å¾„')
    
    # æ¨¡åž‹å‚æ•°
    parser.add_argument('--num_coarse_labels', type=int, default=4, help='ç²—ç²’åº¦æ ‡ç­¾æ•°')
    parser.add_argument('--num_fine_labels', type=int, default=14, help='ç»†ç²’åº¦æ ‡ç­¾æ•°')
    parser.add_argument('--max_length', type=int, default=128, help='æœ€å¤§åºåˆ—é•¿åº¦')
    parser.add_argument('--dropout', type=float, default=0.3135999246870766, help='DropoutçŽ‡')
    
    # MoEå‚æ•°
    parser.add_argument('--num_experts', type=int, default=15, help='ä¸“å®¶æ•°é‡')
    parser.add_argument('--expert_dim', type=int, default=512, help='ä¸“å®¶ç»´åº¦')
    parser.add_argument('--top_k', type=int, default=1, help='Top-Kä¸“å®¶')
    parser.add_argument('--use_separate_moe', type=bool, default=True, help='æ˜¯å¦ä½¿ç”¨ç‹¬ç«‹MoE')
    parser.add_argument('--load_balance_weight', type=float, default=0.013540158381723238, help='è´Ÿè½½å‡è¡¡æƒé‡')
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument('--batch_size', type=int, default=32, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--lr', type=float, default=1.86884655838032e-05, help='å­¦ä¹ çŽ‡')
    parser.add_argument('--weight_decay', type=float, default=0.01, help='æƒé‡è¡°å‡')
    parser.add_argument('--epochs', type=int, default=40, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--patience', type=int, default=5, help='æ—©åœè€å¿ƒå€¼')
    parser.add_argument('--threshold', type=float, default=0.31676353792873924, help='äºŒåˆ†ç±»é˜ˆå€¼')
    parser.add_argument('--seed', type=int, default=3407, help='éšæœºç§å­')
    
    # ä¿å­˜è·¯å¾„
    parser.add_argument('--results_dir', type=str, default='training_results', help='ç»“æžœä¿å­˜ç›®å½•')
    parser.add_argument('--checkpoint_dir', type=str, default='checkpoints', help='æ¨¡åž‹ä¿å­˜ç›®å½•')
    
    args = parser.parse_args()
    
    # è½¬æ¢ä¸ºé…ç½®å­—å…¸
    config = vars(args)
    config['device'] = device
    
    # çº¦æŸæ£€æŸ¥
    if config['top_k'] > config['num_experts']:
        config['top_k'] = config['num_experts']
        print(f"âš ï¸  Top-K ({args.top_k}) å¤§äºŽä¸“å®¶æ•°é‡ ({config['num_experts']})ï¼Œå·²è°ƒæ•´ä¸º {config['top_k']}")
    
    # æ£€æŸ¥æ•°æ®æ–‡ä»¶
    print("ðŸ” æ£€æŸ¥æ•°æ®æ–‡ä»¶...")
    for path_key in ['data_path', 'val_data_path']:
        if not os.path.exists(config[path_key]):
            print(f"âŒ {config[path_key]} ä¸å­˜åœ¨")
            return
        else:
            print(f"âœ… {config[path_key]} å­˜åœ¨")
    
    # æ£€æŸ¥æ¨¡åž‹è·¯å¾„
    if not os.path.exists(config['model_name']):
        print(f"âŒ æ¨¡åž‹è·¯å¾„ {config['model_name']} ä¸å­˜åœ¨")
        return
    else:
        print(f"âœ… æ¨¡åž‹è·¯å¾„å­˜åœ¨")
    
    print(f"\nðŸŽ›ï¸  é…ç½®å‚æ•°:")
    for key, value in config.items():
        if key not in ['device']:
            print(f"   {key}: {value}")
    
    # å¼€å§‹è®­ç»ƒ
    best_metrics, training_history = train_model(config)
    
    print(f"\nâœ¨ è®­ç»ƒå®Œæˆ! æœ€ä½³ Final Micro F1: {best_metrics['hierarchical_final_micro_f1']:.2f}%")


if __name__ == '__main__':
    main()