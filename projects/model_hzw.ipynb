{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import wandb\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import BertModel\n",
    "from transformers import AdamW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据预处理部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "\n",
    "class ErrorDetectionDataset(Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            data_path,\n",
    "            coarse_labels={\n",
    "                \"字符级错误\": 0,\n",
    "                \"成分残缺型错误\": 1, \n",
    "                \"成分冗余型错误\": 2,\n",
    "                \"成分搭配不当型错误\": 3\n",
    "            },\n",
    "            fine_labels={\n",
    "                \"缺字漏字\": 0,\n",
    "                \"错别字错误\": 1,\n",
    "                \"缺少标点\": 2,\n",
    "                \"错用标点\": 3,\n",
    "                \"主语不明\": 4,\n",
    "                \"谓语残缺\": 5,\n",
    "                \"宾语残缺\": 6,\n",
    "                \"其他成分残缺\": 7,\n",
    "                \"主语多余\": 8,\n",
    "                \"谓词多余\": 9,\n",
    "                \"其他成分多余\": 10,\n",
    "                \"语序不当\": 11,\n",
    "                \"动宾搭配不当\": 12,\n",
    "                \"其他搭配不当\": 13\n",
    "            }\n",
    "    ):\n",
    "        self.data_path = data_path\n",
    "        self.coarse_labels = coarse_labels\n",
    "        self.fine_labels = fine_labels\n",
    "        self._get_data()\n",
    "    \n",
    "    def _get_data(self):\n",
    "        with open(self.data_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        self.data = []\n",
    "        for item in data:\n",
    "            sent = item['sent']\n",
    "            coarse_types = item['CourseGrainedErrorType']\n",
    "            fine_types = item['FineGrainedErrorType']\n",
    "            \n",
    "            # 构建粗粒度标签（多标签）\n",
    "            coarse_label = [0] * len(self.coarse_labels)\n",
    "            for c_type in coarse_types:\n",
    "                if c_type in self.coarse_labels:\n",
    "                    coarse_label[self.coarse_labels[c_type]] = 1\n",
    "            \n",
    "            # 构建细粒度标签（多标签）\n",
    "            fine_label = [0] * len(self.fine_labels)\n",
    "            for f_type in fine_types:\n",
    "                if f_type in self.fine_labels:\n",
    "                    fine_label[self.fine_labels[f_type]] = 1\n",
    "            \n",
    "            self.data.append((sent, coarse_label, fine_label, item.get('sent_id', -1)))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def get_coarse_labels(self):\n",
    "        return self.coarse_labels\n",
    "    \n",
    "    def get_fine_labels(self):\n",
    "        return self.fine_labels\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据加载部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "class ErrorDetectionDataLoader:\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset,\n",
    "        batch_size=16,\n",
    "        max_length=128,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        device=None,\n",
    "        tokenizer_name='bge-large-zh-v1.5'\n",
    "    ):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.max_length = max_length\n",
    "        self.shuffle = shuffle\n",
    "        self.drop_last = drop_last\n",
    "        \n",
    "        if device is None:\n",
    "            self.device = torch.device(\n",
    "                'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "            )\n",
    "        else:\n",
    "            self.device = device\n",
    "        \n",
    "        self.loader = DataLoader(\n",
    "            dataset=self.dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            collate_fn=self.collate_fn,\n",
    "            shuffle=self.shuffle,\n",
    "            drop_last=self.drop_last\n",
    "        )\n",
    "    \n",
    "    def collate_fn(self, data):\n",
    "        sents = [item[0] for item in data]\n",
    "        coarse_labels = [item[1] for item in data]\n",
    "        fine_labels = [item[2] for item in data]\n",
    "        sent_ids = [item[3] for item in data]\n",
    "        \n",
    "        # 编码文本\n",
    "        encoded = self.tokenizer.batch_encode_plus(\n",
    "            batch_text_or_text_pairs=sents,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt',\n",
    "            return_length=True\n",
    "        )\n",
    "        \n",
    "        input_ids = encoded['input_ids'].to(self.device)\n",
    "        attention_mask = encoded['attention_mask'].to(self.device)\n",
    "        token_type_ids = encoded.get('token_type_ids', None)\n",
    "        \n",
    "        if token_type_ids is not None:\n",
    "            token_type_ids = token_type_ids.to(self.device)\n",
    "        \n",
    "        # 处理标签\n",
    "        if coarse_labels[0] == -1:\n",
    "            coarse_labels = None\n",
    "            fine_labels = None\n",
    "        else:\n",
    "            coarse_labels = torch.tensor(coarse_labels, dtype=torch.float).to(self.device)\n",
    "            fine_labels = torch.tensor(fine_labels, dtype=torch.float).to(self.device)\n",
    "        \n",
    "        return input_ids, attention_mask, token_type_ids, coarse_labels, fine_labels, sent_ids\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for data in self.loader:\n",
    "            yield data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 模型部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel\n",
    "\n",
    "class HierarchicalErrorClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        pretrained_model_name, \n",
    "        num_coarse_labels=4, \n",
    "        num_fine_labels=14, \n",
    "        freeze_pooler=False, \n",
    "        dropout=0.2\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.freeze_pooler = freeze_pooler\n",
    "        self.bert = AutoModel.from_pretrained(pretrained_model_name)\n",
    "        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n",
    "        \n",
    "        if freeze_pooler:\n",
    "            for param in self.bert.pooler.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # 粗粒度分类器\n",
    "        self.coarse_classifier = nn.Linear(self.bert.config.hidden_size, num_coarse_labels)\n",
    "        \n",
    "        # 细粒度分类器\n",
    "        self.fine_classifier = nn.Linear(self.bert.config.hidden_size, num_fine_labels)\n",
    "        \n",
    "        # 定义粗粒度类别和对应的细粒度索引的映射\n",
    "        self.coarse_to_fine_indices = {\n",
    "            0: [0, 1, 2, 3],    # 字符级错误\n",
    "            1: [4, 5, 6, 7],    # 成分残缺型错误\n",
    "            2: [8, 9, 10],      # 成分冗余型错误\n",
    "            3: [11, 12, 13]     # 成分搭配不当型错误\n",
    "        }\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids=None):\n",
    "        # BERT编码\n",
    "        if token_type_ids is not None:\n",
    "            outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        else:\n",
    "            outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        pooled_output = outputs.pooler_output\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        \n",
    "        # 粗粒度分类\n",
    "        coarse_logits = self.coarse_classifier(pooled_output)\n",
    "        coarse_probs = torch.sigmoid(coarse_logits)\n",
    "        \n",
    "        # 细粒度分类\n",
    "        fine_logits = self.fine_classifier(pooled_output)\n",
    "        fine_probs = torch.sigmoid(fine_logits)\n",
    "        \n",
    "        return coarse_probs, fine_probs\n",
    "    \n",
    "    def apply_hierarchical_constraint(self, coarse_preds, fine_preds):\n",
    "        \"\"\"\n",
    "        应用层次约束：如果粗粒度类别预测为负，则该粗粒度下的所有细粒度类别均设为负\n",
    "        \n",
    "        Args:\n",
    "            coarse_preds: 粗粒度预测结果，shape [batch_size, num_coarse_labels]\n",
    "            fine_preds: 细粒度预测结果，shape [batch_size, num_fine_labels]\n",
    "            \n",
    "        Returns:\n",
    "            应用约束后的细粒度预测结果\n",
    "        \"\"\"\n",
    "        constrained_fine_preds = fine_preds.clone()\n",
    "        \n",
    "        # 遍历每个样本\n",
    "        for i in range(coarse_preds.size(0)):\n",
    "            # 对每个粗粒度类别\n",
    "            for coarse_idx, fine_indices in self.coarse_to_fine_indices.items():\n",
    "                # 如果粗粒度为负，则对应的细粒度全部设为负\n",
    "                if coarse_preds[i, coarse_idx] == 0:\n",
    "                    constrained_fine_preds[i, fine_indices] = 0\n",
    "        \n",
    "        return constrained_fine_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练、验证和测试部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in Jupyter environment, using default configs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:gmd413xf) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">bge-large-zh-v1.5_b16_e10_len128_lr1e-05</strong> at: <a href='https://wandb.ai/akccc/hierarchical_error_detection/runs/gmd413xf' target=\"_blank\">https://wandb.ai/akccc/hierarchical_error_detection/runs/gmd413xf</a><br/> View project at: <a href='https://wandb.ai/akccc/hierarchical_error_detection' target=\"_blank\">https://wandb.ai/akccc/hierarchical_error_detection</a><br/>Synced 4 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250313_110627-gmd413xf/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:gmd413xf). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/cfs/huangzhiwei/NLP-WED0910/projects/wandb/run-20250313_110842-29ov910b</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/akccc/hierarchical_error_detection/runs/29ov910b' target=\"_blank\">bge-large-zh-v1.5_b16_e20_len128_lr1e-05</a></strong> to <a href='https://wandb.ai/akccc/hierarchical_error_detection' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/akccc/hierarchical_error_detection' target=\"_blank\">https://wandb.ai/akccc/hierarchical_error_detection</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/akccc/hierarchical_error_detection/runs/29ov910b' target=\"_blank\">https://wandb.ai/akccc/hierarchical_error_detection/runs/29ov910b</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1/20: 100%|██| 6/6 [00:01<00:00,  3.51batch/s, coarse_loss=0.564, fine_loss=0.614, loss=1.178]\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/20:\n",
      "  Train Loss: 1.2734\n",
      "  Train Coarse-grained Metrics:\n",
      "    Micro F1: 42.86\n",
      "    Macro F1: 22.20\n",
      "    Accuracy: 17.71\n",
      "  Train Fine-grained Metrics (Unconstrained):\n",
      "    Micro F1: 17.51\n",
      "    Macro F1: 11.99\n",
      "    Accuracy: 0.00\n",
      "  Train Fine-grained Metrics (Constrained):\n",
      "    Micro F1: 15.28\n",
      "    Macro F1: 8.92\n",
      "    Accuracy: 0.00\n",
      "  Val Loss: 1.0928\n",
      "  Val Coarse-grained Metrics:\n",
      "    Micro F1: 60.00\n",
      "    Macro F1: 20.00\n",
      "    Accuracy: 37.04\n",
      "  Val Fine-grained Metrics (Unconstrained):\n",
      "    Micro F1: 22.50\n",
      "    Macro F1: 7.19\n",
      "    Accuracy: 0.00\n",
      "  Val Fine-grained Metrics (Constrained):\n",
      "    Micro F1: 26.87\n",
      "    Macro F1: 7.32\n",
      "    Accuracy: 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██| 6/6 [00:00<00:00,  8.04batch/s, coarse_loss=0.452, fine_loss=0.495, loss=0.948]\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/20:\n",
      "  Train Loss: 1.0306\n",
      "  Train Coarse-grained Metrics:\n",
      "    Micro F1: 55.77\n",
      "    Macro F1: 19.59\n",
      "    Accuracy: 32.29\n",
      "  Train Fine-grained Metrics (Unconstrained):\n",
      "    Micro F1: 11.21\n",
      "    Macro F1: 2.48\n",
      "    Accuracy: 3.12\n",
      "  Train Fine-grained Metrics (Constrained):\n",
      "    Micro F1: 11.76\n",
      "    Macro F1: 2.56\n",
      "    Accuracy: 4.17\n",
      "  Val Loss: 0.9171\n",
      "  Val Coarse-grained Metrics:\n",
      "    Micro F1: 62.30\n",
      "    Macro F1: 20.65\n",
      "    Accuracy: 40.74\n",
      "  Val Fine-grained Metrics (Unconstrained):\n",
      "    Micro F1: 0.00\n",
      "    Macro F1: 0.00\n",
      "    Accuracy: 0.00\n",
      "  Val Fine-grained Metrics (Constrained):\n",
      "    Micro F1: 0.00\n",
      "    Macro F1: 0.00\n",
      "    Accuracy: 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██| 6/6 [00:00<00:00,  8.01batch/s, coarse_loss=0.454, fine_loss=0.418, loss=0.872]\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/20:\n",
      "  Train Loss: 0.9016\n",
      "  Train Coarse-grained Metrics:\n",
      "    Micro F1: 55.50\n",
      "    Macro F1: 19.59\n",
      "    Accuracy: 32.29\n",
      "  Train Fine-grained Metrics (Unconstrained):\n",
      "    Micro F1: 2.35\n",
      "    Macro F1: 0.87\n",
      "    Accuracy: 1.04\n",
      "  Train Fine-grained Metrics (Constrained):\n",
      "    Micro F1: 2.35\n",
      "    Macro F1: 0.87\n",
      "    Accuracy: 1.04\n",
      "  Val Loss: 0.8472\n",
      "  Val Coarse-grained Metrics:\n",
      "    Micro F1: 62.30\n",
      "    Macro F1: 20.65\n",
      "    Accuracy: 40.74\n",
      "  Val Fine-grained Metrics (Unconstrained):\n",
      "    Micro F1: 0.00\n",
      "    Macro F1: 0.00\n",
      "    Accuracy: 0.00\n",
      "  Val Fine-grained Metrics (Constrained):\n",
      "    Micro F1: 0.00\n",
      "    Macro F1: 0.00\n",
      "    Accuracy: 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██| 6/6 [00:00<00:00,  8.07batch/s, coarse_loss=0.407, fine_loss=0.390, loss=0.797]\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/20:\n",
      "  Train Loss: 0.8308\n",
      "  Train Coarse-grained Metrics:\n",
      "    Micro F1: 56.19\n",
      "    Macro F1: 19.41\n",
      "    Accuracy: 32.29\n",
      "  Train Fine-grained Metrics (Unconstrained):\n",
      "    Micro F1: 0.00\n",
      "    Macro F1: 0.00\n",
      "    Accuracy: 2.08\n",
      "  Train Fine-grained Metrics (Constrained):\n",
      "    Micro F1: 0.00\n",
      "    Macro F1: 0.00\n",
      "    Accuracy: 2.08\n",
      "  Val Loss: 0.8027\n",
      "  Val Coarse-grained Metrics:\n",
      "    Micro F1: 54.55\n",
      "    Macro F1: 18.75\n",
      "    Accuracy: 29.63\n",
      "  Val Fine-grained Metrics (Unconstrained):\n",
      "    Micro F1: 0.00\n",
      "    Macro F1: 0.00\n",
      "    Accuracy: 0.00\n",
      "  Val Fine-grained Metrics (Constrained):\n",
      "    Micro F1: 0.00\n",
      "    Macro F1: 0.00\n",
      "    Accuracy: 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|██| 6/6 [00:00<00:00,  8.00batch/s, coarse_loss=0.412, fine_loss=0.363, loss=0.775]\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/20:\n",
      "  Train Loss: 0.7905\n",
      "  Train Coarse-grained Metrics:\n",
      "    Micro F1: 59.34\n",
      "    Macro F1: 23.11\n",
      "    Accuracy: 34.38\n",
      "  Train Fine-grained Metrics (Unconstrained):\n",
      "    Micro F1: 0.00\n",
      "    Macro F1: 0.00\n",
      "    Accuracy: 2.08\n",
      "  Train Fine-grained Metrics (Constrained):\n",
      "    Micro F1: 0.00\n",
      "    Macro F1: 0.00\n",
      "    Accuracy: 2.08\n",
      "  Val Loss: 0.7730\n",
      "  Val Coarse-grained Metrics:\n",
      "    Micro F1: 57.69\n",
      "    Macro F1: 20.27\n",
      "    Accuracy: 29.63\n",
      "  Val Fine-grained Metrics (Unconstrained):\n",
      "    Micro F1: 0.00\n",
      "    Macro F1: 0.00\n",
      "    Accuracy: 0.00\n",
      "  Val Fine-grained Metrics (Constrained):\n",
      "    Micro F1: 0.00\n",
      "    Macro F1: 0.00\n",
      "    Accuracy: 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|██| 6/6 [00:00<00:00,  8.07batch/s, coarse_loss=0.392, fine_loss=0.374, loss=0.765]\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/tmp/ipykernel_163965/1630725655.py:392: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(os.path.join(checkpoint_dir, 'best_model.pt')))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/20:\n",
      "  Train Loss: 0.7452\n",
      "  Train Coarse-grained Metrics:\n",
      "    Micro F1: 60.67\n",
      "    Macro F1: 23.55\n",
      "    Accuracy: 40.62\n",
      "  Train Fine-grained Metrics (Unconstrained):\n",
      "    Micro F1: 1.20\n",
      "    Macro F1: 0.46\n",
      "    Accuracy: 2.08\n",
      "  Train Fine-grained Metrics (Constrained):\n",
      "    Micro F1: 1.20\n",
      "    Macro F1: 0.46\n",
      "    Accuracy: 2.08\n",
      "  Val Loss: 0.7624\n",
      "  Val Coarse-grained Metrics:\n",
      "    Micro F1: 60.38\n",
      "    Macro F1: 21.05\n",
      "    Accuracy: 33.33\n",
      "  Val Fine-grained Metrics (Unconstrained):\n",
      "    Micro F1: 0.00\n",
      "    Macro F1: 0.00\n",
      "    Accuracy: 0.00\n",
      "  Val Fine-grained Metrics (Constrained):\n",
      "    Micro F1: 0.00\n",
      "    Macro F1: 0.00\n",
      "    Accuracy: 0.00\n",
      "Early stopping triggered.\n",
      "\n",
      "===== Testing best model =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Final Test Results =====\n",
      "Final micro f1: 26.87\n",
      "Final macro f1: 7.32\n",
      "\n",
      "Coarse-grained micro f1: 60.00\n",
      "Fine-grained micro f1: 26.87\n",
      "\n",
      "Coarse-grained macro f1: 20.00\n",
      "Fine-grained macro f1: 7.32\n",
      "\n",
      "Accuracy: 0.00\n",
      "\n",
      "\n",
      "+---------------+---------------+---------------+---------------+---------------+---------------+\n",
      "| Final         | Final         | Course-       | Fine-grained  | Course-       | Fine-grained  |\n",
      "| micro f1      | macro f1      | grained micro f1 | micro f1      | grained macro f1 | macro f1      |\n",
      "+---------------+---------------+---------------+---------------+---------------+---------------+\n",
      "| 26.87         | 7.32          | 60.00         | 26.87         | 20.00         | 7.32          |\n",
      "+---------------+---------------+---------------+---------------+---------------+---------------+\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch_coarse_loss</td><td>█▇▇▆▅▅▅▃▃▄▅▃▄▄▃▃▃▃▃▃▃▃▃▂▃▃▂▂▁▂▂▁▂▂▁▂</td></tr><tr><td>batch_fine_loss</td><td>██▇▇▆▆▆▅▅▄▄▄▃▃▃▃▂▃▂▂▁▃▁▂▂▂▂▁▂▂▁▁▂▂▁▂</td></tr><tr><td>batch_loss</td><td>██▇▆▆▆▅▄▄▄▅▃▃▄▃▃▃▃▃▂▂▃▂▂▂▂▂▂▂▂▂▁▂▂▁▂</td></tr><tr><td>epoch</td><td>▁▂▄▅▇█</td></tr><tr><td>final_macro_f1</td><td>▁</td></tr><tr><td>final_micro_f1</td><td>▁</td></tr><tr><td>test_coarse_accuracy</td><td>▁</td></tr><tr><td>test_coarse_macro_f1</td><td>▁</td></tr><tr><td>test_coarse_micro_f1</td><td>▁</td></tr><tr><td>test_constrained_fine_accuracy</td><td>▁</td></tr><tr><td>test_constrained_fine_macro_f1</td><td>▁</td></tr><tr><td>test_constrained_fine_micro_f1</td><td>▁</td></tr><tr><td>test_fine_accuracy</td><td>▁</td></tr><tr><td>test_fine_macro_f1</td><td>▁</td></tr><tr><td>test_fine_micro_f1</td><td>▁</td></tr><tr><td>train_coarse_accuracy</td><td>▁▅▅▅▆█</td></tr><tr><td>train_coarse_macro_f1</td><td>▆▁▁▁▇█</td></tr><tr><td>train_coarse_micro_f1</td><td>▁▆▆▆▇█</td></tr><tr><td>train_constrained_fine_accuracy</td><td>▁█▃▅▅▅</td></tr><tr><td>train_constrained_fine_macro_f1</td><td>█▃▂▁▁▁</td></tr><tr><td>train_constrained_fine_micro_f1</td><td>█▆▂▁▁▂</td></tr><tr><td>train_fine_accuracy</td><td>▁█▃▆▆▆</td></tr><tr><td>train_fine_macro_f1</td><td>█▂▂▁▁▁</td></tr><tr><td>train_fine_micro_f1</td><td>█▅▂▁▁▁</td></tr><tr><td>train_loss</td><td>█▅▃▂▂▁</td></tr><tr><td>val_coarse_accuracy</td><td>▆██▁▁▃</td></tr><tr><td>val_coarse_macro_f1</td><td>▅▇▇▁▆█</td></tr><tr><td>val_coarse_micro_f1</td><td>▆██▁▄▆</td></tr><tr><td>val_constrained_fine_accuracy</td><td>▁▁▁▁▁▁</td></tr><tr><td>val_constrained_fine_macro_f1</td><td>█▁▁▁▁▁</td></tr><tr><td>val_constrained_fine_micro_f1</td><td>█▁▁▁▁▁</td></tr><tr><td>val_fine_accuracy</td><td>▁▁▁▁▁▁</td></tr><tr><td>val_fine_macro_f1</td><td>█▁▁▁▁▁</td></tr><tr><td>val_fine_micro_f1</td><td>█▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▃▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch_coarse_loss</td><td>0.39192</td></tr><tr><td>batch_fine_loss</td><td>0.3735</td></tr><tr><td>batch_loss</td><td>0.76542</td></tr><tr><td>epoch</td><td>6</td></tr><tr><td>final_macro_f1</td><td>7.32468</td></tr><tr><td>final_micro_f1</td><td>26.86567</td></tr><tr><td>test_coarse_accuracy</td><td>37.03704</td></tr><tr><td>test_coarse_macro_f1</td><td>20</td></tr><tr><td>test_coarse_micro_f1</td><td>60.0</td></tr><tr><td>test_constrained_fine_accuracy</td><td>0</td></tr><tr><td>test_constrained_fine_macro_f1</td><td>7.32468</td></tr><tr><td>test_constrained_fine_micro_f1</td><td>26.86567</td></tr><tr><td>test_fine_accuracy</td><td>0</td></tr><tr><td>test_fine_macro_f1</td><td>7.19281</td></tr><tr><td>test_fine_micro_f1</td><td>22.5</td></tr><tr><td>train_coarse_accuracy</td><td>40.625</td></tr><tr><td>train_coarse_macro_f1</td><td>23.55392</td></tr><tr><td>train_coarse_micro_f1</td><td>60.67416</td></tr><tr><td>train_constrained_fine_accuracy</td><td>2.08333</td></tr><tr><td>train_constrained_fine_macro_f1</td><td>0.46083</td></tr><tr><td>train_constrained_fine_micro_f1</td><td>1.1976</td></tr><tr><td>train_fine_accuracy</td><td>2.08333</td></tr><tr><td>train_fine_macro_f1</td><td>0.46083</td></tr><tr><td>train_fine_micro_f1</td><td>1.1976</td></tr><tr><td>train_loss</td><td>0.74518</td></tr><tr><td>val_coarse_accuracy</td><td>33.33333</td></tr><tr><td>val_coarse_macro_f1</td><td>21.05263</td></tr><tr><td>val_coarse_micro_f1</td><td>60.37736</td></tr><tr><td>val_constrained_fine_accuracy</td><td>0</td></tr><tr><td>val_constrained_fine_macro_f1</td><td>0</td></tr><tr><td>val_constrained_fine_micro_f1</td><td>0</td></tr><tr><td>val_fine_accuracy</td><td>0</td></tr><tr><td>val_fine_macro_f1</td><td>0</td></tr><tr><td>val_fine_micro_f1</td><td>0</td></tr><tr><td>val_loss</td><td>0.76235</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">bge-large-zh-v1.5_b16_e20_len128_lr1e-05</strong> at: <a href='https://wandb.ai/akccc/hierarchical_error_detection/runs/29ov910b' target=\"_blank\">https://wandb.ai/akccc/hierarchical_error_detection/runs/29ov910b</a><br/> View project at: <a href='https://wandb.ai/akccc/hierarchical_error_detection' target=\"_blank\">https://wandb.ai/akccc/hierarchical_error_detection</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 6 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250313_110842-29ov910b/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import wandb\n",
    "import random\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from transformers import AdamW\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "# 导入自定义模块\n",
    "# from error_detection_dataset import ErrorDetectionDataset\n",
    "# from error_detection_dataloader import ErrorDetectionDataLoader\n",
    "# from hierarchical_classifier_model import HierarchicalErrorClassifier\n",
    "\n",
    "def argparser():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--model_name', type=str, default='./models/bge-large-zh-v1.5')\n",
    "    parser.add_argument('--num_coarse_labels', type=int, default=4)\n",
    "    parser.add_argument('--num_fine_labels', type=int, default=14)\n",
    "    parser.add_argument('--dropout', type=float, default=0.2)\n",
    "    parser.add_argument('--freeze_pooler', action='store_true', help='Flag to freeze the pooler layer')\n",
    "    parser.add_argument('--batch_size', type=int, default=16)\n",
    "    parser.add_argument('--max_length', type=int, default=128)\n",
    "    parser.add_argument('--lr', type=float, default=1e-5)\n",
    "    parser.add_argument('--epochs', type=int, default=40)\n",
    "    parser.add_argument('--device', type=str, required=False)\n",
    "    parser.add_argument('--project', type=str, default='hierarchical_error_detection')\n",
    "    parser.add_argument('--entity', type=str, default='akccc')\n",
    "    parser.add_argument('--name', type=str, required=False)\n",
    "    parser.add_argument('--seed', type=int, default=42)\n",
    "    parser.add_argument('--data_path', type=str, default='../datas/train.json')\n",
    "    parser.add_argument('--val_data_path', type=str, default='../datas/val.json')\n",
    "    parser.add_argument('--test_data_path', type=str, default='../dats/val.json')\n",
    "    parser.add_argument('--checkpoint_dir', type=str, default='checkpoints')\n",
    "    parser.add_argument('--threshold', type=float, default=0.5)\n",
    "    parser.add_argument('--patience', type=int, default=5)\n",
    "    \n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "\n",
    "# 如果在Jupyter Notebook中运行，可以使用这个自定义参数函数替代argparser\n",
    "def get_default_configs():\n",
    "    \"\"\"在Jupyter环境中使用的默认配置，避免argparse解析错误\"\"\"\n",
    "    class Args:\n",
    "        def __init__(self):\n",
    "            self.model_name = './models/bge-large-zh-v1.5'\n",
    "            self.num_coarse_labels = 4\n",
    "            self.num_fine_labels = 14\n",
    "            self.dropout = 0.2\n",
    "            self.freeze_pooler = False\n",
    "            self.batch_size = 16\n",
    "            self.max_length = 128\n",
    "            self.lr = 1e-5\n",
    "            self.epochs = 20\n",
    "            self.device = None\n",
    "            self.project = 'hierarchical_error_detection'\n",
    "            self.entity = 'akccc'\n",
    "            self.name = None\n",
    "            self.seed = 42\n",
    "            self.data_path = '../datas/train.json'\n",
    "            self.val_data_path = '../datas/val.json'\n",
    "            self.test_data_path = '../datas/val.json'\n",
    "            self.checkpoint_dir = 'checkpoints'\n",
    "            self.threshold = 0.5\n",
    "            self.patience = 5\n",
    "            self.exp_name = 'default_run'\n",
    "    return Args()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_metrics(labels, predictions, average='micro'):\n",
    "    \"\"\"\n",
    "    计算各种评估指标\n",
    "    \n",
    "    Args:\n",
    "        labels: 真实标签\n",
    "        predictions: 预测标签\n",
    "        average: 平均方法，'micro'或'macro'\n",
    "        \n",
    "    Returns:\n",
    "        包含各种指标的字典\n",
    "    \"\"\"\n",
    "    # 将数组转换为numpy格式以确保兼容性\n",
    "    labels = np.array(labels)\n",
    "    predictions = np.array(predictions)\n",
    "    \n",
    "    # 计算微平均和宏平均的F1分数\n",
    "    micro_f1 = f1_score(labels, predictions, average='micro')\n",
    "    macro_f1 = f1_score(labels, predictions, average='macro')\n",
    "    \n",
    "    # 计算样本级别的准确率（每个样本的所有标签都要正确）\n",
    "    sample_acc = accuracy_score(labels, predictions)\n",
    "    \n",
    "    return {\n",
    "        'micro_f1': micro_f1 * 100,  # 转换为百分比\n",
    "        'macro_f1': macro_f1 * 100,\n",
    "        'accuracy': sample_acc * 100\n",
    "    }\n",
    "\n",
    "def train(configs):\n",
    "    # 初始化wandb\n",
    "    wandb.init(\n",
    "        project=configs.project,\n",
    "        entity=configs.entity,\n",
    "        name=configs.exp_name,\n",
    "    )\n",
    "\n",
    "    # 配置wandb\n",
    "    wandb_config = wandb.config\n",
    "    wandb_config.model_name = configs.model_name\n",
    "    wandb_config.num_coarse_labels = configs.num_coarse_labels\n",
    "    wandb_config.num_fine_labels = configs.num_fine_labels\n",
    "    wandb_config.dropout = configs.dropout\n",
    "    wandb_config.freeze_pooler = configs.freeze_pooler\n",
    "    wandb_config.batch_size = configs.batch_size\n",
    "    wandb_config.max_length = configs.max_length\n",
    "    wandb_config.lr = configs.lr\n",
    "    wandb_config.epochs = configs.epochs\n",
    "    wandb_config.device = configs.device\n",
    "    wandb_config.seed = configs.seed\n",
    "    wandb_config.threshold = configs.threshold\n",
    "\n",
    "    # 设置随机种子\n",
    "    random.seed(configs.seed)\n",
    "    np.random.seed(configs.seed)\n",
    "    torch.manual_seed(configs.seed)\n",
    "    \n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    # 创建检查点目录\n",
    "    checkpoint_dir = os.path.join(configs.checkpoint_dir, configs.exp_name)\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    # 加载数据集\n",
    "    train_dataset = ErrorDetectionDataset(configs.data_path)\n",
    "    val_dataset = ErrorDetectionDataset(configs.val_data_path)\n",
    "    test_dataset = ErrorDetectionDataset(configs.test_data_path)\n",
    "\n",
    "    # 创建数据加载器\n",
    "    train_dataloader = ErrorDetectionDataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=configs.batch_size,\n",
    "        max_length=configs.max_length,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        device=configs.device,\n",
    "        tokenizer_name=configs.model_name\n",
    "    )\n",
    "\n",
    "    val_dataloader = ErrorDetectionDataLoader(\n",
    "        dataset=val_dataset,\n",
    "        batch_size=configs.batch_size,\n",
    "        max_length=configs.max_length,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        device=configs.device,\n",
    "        tokenizer_name=configs.model_name\n",
    "    )\n",
    "\n",
    "    test_dataloader = ErrorDetectionDataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size=configs.batch_size,\n",
    "        max_length=configs.max_length,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        device=configs.device,\n",
    "        tokenizer_name=configs.model_name\n",
    "    )\n",
    "\n",
    "    # 创建模型\n",
    "    model = HierarchicalErrorClassifier(\n",
    "        pretrained_model_name=configs.model_name,\n",
    "        num_coarse_labels=configs.num_coarse_labels,\n",
    "        num_fine_labels=configs.num_fine_labels,\n",
    "        dropout=configs.dropout,\n",
    "        freeze_pooler=configs.freeze_pooler\n",
    "    ).to(configs.device)\n",
    "\n",
    "    # 定义损失函数\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    # 定义优化器\n",
    "    optimizer = AdamW(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()),\n",
    "        lr=configs.lr\n",
    "    )\n",
    "\n",
    "    # 初始化最佳验证损失和早停计数器\n",
    "    best_val_f1 = 0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    # 监控模型\n",
    "    wandb.watch(model, log='all')\n",
    "    \n",
    "    # 训练循环\n",
    "    for epoch in range(configs.epochs):\n",
    "        # 训练阶段\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        all_coarse_preds = []\n",
    "        all_coarse_labels = []\n",
    "        all_fine_preds = []\n",
    "        all_fine_labels = []\n",
    "        all_constrained_fine_preds = []\n",
    "        \n",
    "        with tqdm(\n",
    "            train_dataloader,\n",
    "            total=len(train_dataloader),\n",
    "            desc=f'Epoch {epoch + 1}/{configs.epochs}',\n",
    "            unit='batch',\n",
    "            ncols=100\n",
    "        ) as pbar:\n",
    "            for input_ids, attention_mask, token_type_ids, coarse_labels, fine_labels, _ in pbar:\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # 前向传播\n",
    "                coarse_probs, fine_probs = model(input_ids, attention_mask, token_type_ids)\n",
    "                \n",
    "                # 计算损失\n",
    "                coarse_loss = criterion(coarse_probs, coarse_labels)\n",
    "                fine_loss = criterion(fine_probs, fine_labels)\n",
    "                loss = coarse_loss + fine_loss\n",
    "                \n",
    "                # 反向传播\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "                \n",
    "                # 收集预测结果\n",
    "                coarse_preds = (coarse_probs > configs.threshold).float().cpu().numpy()\n",
    "                fine_preds = (fine_probs > configs.threshold).float().cpu().numpy()\n",
    "                constrained_fine_preds = model.apply_hierarchical_constraint(\n",
    "                    (coarse_probs > configs.threshold).float(), \n",
    "                    (fine_probs > configs.threshold).float()\n",
    "                ).cpu().numpy()\n",
    "                \n",
    "                all_coarse_preds.extend(coarse_preds)\n",
    "                all_coarse_labels.extend(coarse_labels.cpu().numpy())\n",
    "                all_fine_preds.extend(fine_preds)\n",
    "                all_fine_labels.extend(fine_labels.cpu().numpy())\n",
    "                all_constrained_fine_preds.extend(constrained_fine_preds)\n",
    "                \n",
    "                # 更新进度条\n",
    "                pbar.set_postfix(\n",
    "                    loss=f'{loss.item():.3f}',\n",
    "                    coarse_loss=f'{coarse_loss.item():.3f}',\n",
    "                    fine_loss=f'{fine_loss.item():.3f}'\n",
    "                )\n",
    "                \n",
    "                # 记录到wandb\n",
    "                wandb.log({\n",
    "                    'batch_loss': loss.item(),\n",
    "                    'batch_coarse_loss': coarse_loss.item(),\n",
    "                    'batch_fine_loss': fine_loss.item()\n",
    "                })\n",
    "        \n",
    "        # 计算训练指标\n",
    "        train_loss = train_loss / len(train_dataloader)\n",
    "        \n",
    "        # 计算各种评估指标\n",
    "        train_coarse_metrics_micro = calculate_metrics(all_coarse_labels, all_coarse_preds, average='micro')\n",
    "        train_coarse_metrics_macro = calculate_metrics(all_coarse_labels, all_coarse_preds, average='macro')\n",
    "        train_fine_metrics_micro = calculate_metrics(all_fine_labels, all_fine_preds, average='micro')\n",
    "        train_fine_metrics_macro = calculate_metrics(all_fine_labels, all_fine_preds, average='macro')\n",
    "        train_constrained_fine_metrics_micro = calculate_metrics(all_fine_labels, all_constrained_fine_preds, average='micro')\n",
    "        train_constrained_fine_metrics_macro = calculate_metrics(all_fine_labels, all_constrained_fine_preds, average='macro')\n",
    "        \n",
    "        # 保存模型检查点\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f'epoch_{epoch + 1}.pt')\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        wandb.save(checkpoint_path)\n",
    "        \n",
    "        # 验证阶段\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        all_coarse_preds = []\n",
    "        all_coarse_labels = []\n",
    "        all_fine_preds = []\n",
    "        all_fine_labels = []\n",
    "        all_constrained_fine_preds = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for input_ids, attention_mask, token_type_ids, coarse_labels, fine_labels, _ in val_dataloader:\n",
    "                # 前向传播\n",
    "                coarse_probs, fine_probs = model(input_ids, attention_mask, token_type_ids)\n",
    "                \n",
    "                # 应用层次约束\n",
    "                coarse_preds = (coarse_probs > configs.threshold).float()\n",
    "                fine_preds = (fine_probs > configs.threshold).float()\n",
    "                constrained_fine_preds = model.apply_hierarchical_constraint(coarse_preds, fine_preds)\n",
    "                \n",
    "                # 计算损失\n",
    "                coarse_loss = criterion(coarse_probs, coarse_labels)\n",
    "                fine_loss = criterion(fine_probs, fine_labels)\n",
    "                loss = coarse_loss + fine_loss\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                # 收集预测结果\n",
    "                all_coarse_preds.extend(coarse_preds.cpu().numpy())\n",
    "                all_coarse_labels.extend(coarse_labels.cpu().numpy())\n",
    "                all_fine_preds.extend(fine_preds.cpu().numpy())\n",
    "                all_constrained_fine_preds.extend(constrained_fine_preds.cpu().numpy())\n",
    "                all_fine_labels.extend(fine_labels.cpu().numpy())\n",
    "        \n",
    "        # 计算验证指标\n",
    "        val_loss = val_loss / len(val_dataloader)\n",
    "        \n",
    "        # 计算各种评估指标\n",
    "        val_coarse_metrics_micro = calculate_metrics(all_coarse_labels, all_coarse_preds, average='micro')\n",
    "        val_coarse_metrics_macro = calculate_metrics(all_coarse_labels, all_coarse_preds, average='macro')\n",
    "        val_fine_metrics_micro = calculate_metrics(all_fine_labels, all_fine_preds, average='micro')\n",
    "        val_fine_metrics_macro = calculate_metrics(all_fine_labels, all_fine_preds, average='macro')\n",
    "        val_constrained_fine_metrics_micro = calculate_metrics(all_fine_labels, all_constrained_fine_preds, average='micro')\n",
    "        val_constrained_fine_metrics_macro = calculate_metrics(all_fine_labels, all_constrained_fine_preds, average='macro')\n",
    "        \n",
    "        # 输出训练和验证指标\n",
    "        print(f'\\nEpoch {epoch+1}/{configs.epochs}:')\n",
    "        print(f'  Train Loss: {train_loss:.4f}')\n",
    "        print(f'  Train Coarse-grained Metrics:')\n",
    "        print(f'    Micro F1: {train_coarse_metrics_micro[\"micro_f1\"]:.2f}')\n",
    "        print(f'    Macro F1: {train_coarse_metrics_macro[\"macro_f1\"]:.2f}')\n",
    "        print(f'    Accuracy: {train_coarse_metrics_micro[\"accuracy\"]:.2f}')\n",
    "        print(f'  Train Fine-grained Metrics (Unconstrained):')\n",
    "        print(f'    Micro F1: {train_fine_metrics_micro[\"micro_f1\"]:.2f}')\n",
    "        print(f'    Macro F1: {train_fine_metrics_macro[\"macro_f1\"]:.2f}')\n",
    "        print(f'    Accuracy: {train_fine_metrics_micro[\"accuracy\"]:.2f}')\n",
    "        print(f'  Train Fine-grained Metrics (Constrained):')\n",
    "        print(f'    Micro F1: {train_constrained_fine_metrics_micro[\"micro_f1\"]:.2f}')\n",
    "        print(f'    Macro F1: {train_constrained_fine_metrics_macro[\"macro_f1\"]:.2f}')\n",
    "        print(f'    Accuracy: {train_constrained_fine_metrics_micro[\"accuracy\"]:.2f}')\n",
    "        \n",
    "        print(f'  Val Loss: {val_loss:.4f}')\n",
    "        print(f'  Val Coarse-grained Metrics:')\n",
    "        print(f'    Micro F1: {val_coarse_metrics_micro[\"micro_f1\"]:.2f}')\n",
    "        print(f'    Macro F1: {val_coarse_metrics_macro[\"macro_f1\"]:.2f}')\n",
    "        print(f'    Accuracy: {val_coarse_metrics_micro[\"accuracy\"]:.2f}')\n",
    "        print(f'  Val Fine-grained Metrics (Unconstrained):')\n",
    "        print(f'    Micro F1: {val_fine_metrics_micro[\"micro_f1\"]:.2f}')\n",
    "        print(f'    Macro F1: {val_fine_metrics_macro[\"macro_f1\"]:.2f}')\n",
    "        print(f'    Accuracy: {val_fine_metrics_micro[\"accuracy\"]:.2f}')\n",
    "        print(f'  Val Fine-grained Metrics (Constrained):')\n",
    "        print(f'    Micro F1: {val_constrained_fine_metrics_micro[\"micro_f1\"]:.2f}')\n",
    "        print(f'    Macro F1: {val_constrained_fine_metrics_macro[\"macro_f1\"]:.2f}')\n",
    "        print(f'    Accuracy: {val_constrained_fine_metrics_micro[\"accuracy\"]:.2f}')\n",
    "        \n",
    "        # 记录到wandb\n",
    "        wandb.log({\n",
    "            'train_loss': train_loss,\n",
    "            'train_coarse_micro_f1': train_coarse_metrics_micro[\"micro_f1\"],\n",
    "            'train_coarse_macro_f1': train_coarse_metrics_macro[\"macro_f1\"],\n",
    "            'train_coarse_accuracy': train_coarse_metrics_micro[\"accuracy\"],\n",
    "            'train_fine_micro_f1': train_fine_metrics_micro[\"micro_f1\"],\n",
    "            'train_fine_macro_f1': train_fine_metrics_macro[\"macro_f1\"],\n",
    "            'train_fine_accuracy': train_fine_metrics_micro[\"accuracy\"],\n",
    "            'train_constrained_fine_micro_f1': train_constrained_fine_metrics_micro[\"micro_f1\"],\n",
    "            'train_constrained_fine_macro_f1': train_constrained_fine_metrics_macro[\"macro_f1\"],\n",
    "            'train_constrained_fine_accuracy': train_constrained_fine_metrics_micro[\"accuracy\"],\n",
    "            'val_loss': val_loss,\n",
    "            'val_coarse_micro_f1': val_coarse_metrics_micro[\"micro_f1\"],\n",
    "            'val_coarse_macro_f1': val_coarse_metrics_macro[\"macro_f1\"],\n",
    "            'val_coarse_accuracy': val_coarse_metrics_micro[\"accuracy\"],\n",
    "            'val_fine_micro_f1': val_fine_metrics_micro[\"micro_f1\"],\n",
    "            'val_fine_macro_f1': val_fine_metrics_macro[\"macro_f1\"],\n",
    "            'val_fine_accuracy': val_fine_metrics_micro[\"accuracy\"],\n",
    "            'val_constrained_fine_micro_f1': val_constrained_fine_metrics_micro[\"micro_f1\"],\n",
    "            'val_constrained_fine_macro_f1': val_constrained_fine_metrics_macro[\"macro_f1\"],\n",
    "            'val_constrained_fine_accuracy': val_constrained_fine_metrics_micro[\"accuracy\"],\n",
    "            'epoch': epoch + 1\n",
    "        })\n",
    "        \n",
    "        # 检查是否保存最佳模型并应用早停\n",
    "        if val_constrained_fine_metrics_micro[\"micro_f1\"] > best_val_f1:\n",
    "            best_val_f1 = val_constrained_fine_metrics_micro[\"micro_f1\"]\n",
    "            torch.save(model.state_dict(), os.path.join(checkpoint_dir, 'best_model.pt'))\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= configs.patience:\n",
    "                print('Early stopping triggered.')\n",
    "                break\n",
    "    \n",
    "    # 加载最佳模型进行测试\n",
    "    print(\"\\n===== Testing best model =====\")\n",
    "    model.load_state_dict(torch.load(os.path.join(checkpoint_dir, 'best_model.pt')))\n",
    "    model.eval()\n",
    "    \n",
    "    all_coarse_preds = []\n",
    "    all_coarse_labels = []\n",
    "    all_fine_preds = []\n",
    "    all_fine_labels = []\n",
    "    all_constrained_fine_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask, token_type_ids, coarse_labels, fine_labels, _ in test_dataloader:\n",
    "            # 前向传播\n",
    "            coarse_probs, fine_probs = model(input_ids, attention_mask, token_type_ids)\n",
    "            \n",
    "            # 应用层次约束\n",
    "            coarse_preds = (coarse_probs > configs.threshold).float()\n",
    "            fine_preds = (fine_probs > configs.threshold).float()\n",
    "            constrained_fine_preds = model.apply_hierarchical_constraint(coarse_preds, fine_preds)\n",
    "            \n",
    "            # 收集预测结果\n",
    "            all_coarse_preds.extend(coarse_preds.cpu().numpy())\n",
    "            all_coarse_labels.extend(coarse_labels.cpu().numpy())\n",
    "            all_fine_preds.extend(fine_preds.cpu().numpy())\n",
    "            all_constrained_fine_preds.extend(constrained_fine_preds.cpu().numpy())\n",
    "            all_fine_labels.extend(fine_labels.cpu().numpy())\n",
    "    \n",
    "    # 计算测试指标\n",
    "    test_coarse_metrics_micro = calculate_metrics(all_coarse_labels, all_coarse_preds, average='micro')\n",
    "    test_coarse_metrics_macro = calculate_metrics(all_coarse_labels, all_coarse_preds, average='macro')\n",
    "    test_fine_metrics_micro = calculate_metrics(all_fine_labels, all_fine_preds, average='micro')\n",
    "    test_fine_metrics_macro = calculate_metrics(all_fine_labels, all_fine_preds, average='macro')\n",
    "    test_constrained_fine_metrics_micro = calculate_metrics(all_fine_labels, all_constrained_fine_preds, average='micro')\n",
    "    test_constrained_fine_metrics_macro = calculate_metrics(all_fine_labels, all_constrained_fine_preds, average='macro')\n",
    "    \n",
    "    # 输出测试结果\n",
    "    print(\"\\n===== Final Test Results =====\")\n",
    "    print(f\"Final micro f1: {test_constrained_fine_metrics_micro['micro_f1']:.2f}\")\n",
    "    print(f\"Final macro f1: {test_constrained_fine_metrics_macro['macro_f1']:.2f}\")\n",
    "    \n",
    "    print(\"\\nCoarse-grained micro f1: {:.2f}\".format(test_coarse_metrics_micro['micro_f1']))\n",
    "    print(\"Fine-grained micro f1: {:.2f}\".format(test_constrained_fine_metrics_micro['micro_f1']))\n",
    "    \n",
    "    print(\"\\nCoarse-grained macro f1: {:.2f}\".format(test_coarse_metrics_macro['macro_f1']))\n",
    "    print(\"Fine-grained macro f1: {:.2f}\".format(test_constrained_fine_metrics_macro['macro_f1']))\n",
    "    \n",
    "    print(\"\\nAccuracy: {:.2f}\".format(test_constrained_fine_metrics_micro['accuracy']))\n",
    "    \n",
    "    # 以表格形式输出所有指标（与给定的评估表格格式一致）\n",
    "    print(\"\\n\")\n",
    "    print(\"+\" + \"-\"*15 + \"+\" + \"-\"*15 + \"+\" + \"-\"*15 + \"+\" + \"-\"*15 + \"+\" + \"-\"*15 + \"+\" + \"-\"*15 + \"+\")\n",
    "    print(\"| {:<13} | {:<13} | {:<13} | {:<13} | {:<13} | {:<13} |\".format(\n",
    "        \"Final\", \"Final\", \"Course-\", \"Fine-grained\", \"Course-\", \"Fine-grained\"))\n",
    "    print(\"| {:<13} | {:<13} | {:<13} | {:<13} | {:<13} | {:<13} |\".format(\n",
    "        \"micro f1\", \"macro f1\", \"grained micro f1\", \"micro f1\", \"grained macro f1\", \"macro f1\"))\n",
    "    print(\"+\" + \"-\"*15 + \"+\" + \"-\"*15 + \"+\" + \"-\"*15 + \"+\" + \"-\"*15 + \"+\" + \"-\"*15 + \"+\" + \"-\"*15 + \"+\")\n",
    "    print(\"| {:<13.2f} | {:<13.2f} | {:<13.2f} | {:<13.2f} | {:<13.2f} | {:<13.2f} |\".format(\n",
    "        test_constrained_fine_metrics_micro['micro_f1'],\n",
    "        test_constrained_fine_metrics_macro['macro_f1'],\n",
    "        test_coarse_metrics_micro['micro_f1'],\n",
    "        test_constrained_fine_metrics_micro['micro_f1'],\n",
    "        test_coarse_metrics_macro['macro_f1'],\n",
    "        test_constrained_fine_metrics_macro['macro_f1']\n",
    "    ))\n",
    "    print(\"+\" + \"-\"*15 + \"+\" + \"-\"*15 + \"+\" + \"-\"*15 + \"+\" + \"-\"*15 + \"+\" + \"-\"*15 + \"+\" + \"-\"*15 + \"+\")\n",
    "    \n",
    "    # 记录最终结果到wandb\n",
    "    wandb.log({\n",
    "        'test_coarse_micro_f1': test_coarse_metrics_micro[\"micro_f1\"],\n",
    "        'test_coarse_macro_f1': test_coarse_metrics_macro[\"macro_f1\"],\n",
    "        'test_coarse_accuracy': test_coarse_metrics_micro[\"accuracy\"],\n",
    "        'test_fine_micro_f1': test_fine_metrics_micro[\"micro_f1\"],\n",
    "        'test_fine_macro_f1': test_fine_metrics_macro[\"macro_f1\"],\n",
    "        'test_fine_accuracy': test_fine_metrics_micro[\"accuracy\"],\n",
    "        'test_constrained_fine_micro_f1': test_constrained_fine_metrics_micro[\"micro_f1\"],\n",
    "        'test_constrained_fine_macro_f1': test_constrained_fine_metrics_macro[\"macro_f1\"],\n",
    "        'test_constrained_fine_accuracy': test_constrained_fine_metrics_micro[\"accuracy\"],\n",
    "        'final_micro_f1': test_constrained_fine_metrics_micro[\"micro_f1\"],\n",
    "        'final_macro_f1': test_constrained_fine_metrics_macro[\"macro_f1\"]\n",
    "    })\n",
    "    \n",
    "    # 完成wandb记录\n",
    "    wandb.finish()\n",
    "\n",
    "def predict(model, text, tokenizer, device, threshold=0.5):\n",
    "    \"\"\"\n",
    "    对单个文本进行预测\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # 编码文本\n",
    "    encoded = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=128,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    input_ids = encoded['input_ids'].to(device)\n",
    "    attention_mask = encoded['attention_mask'].to(device)\n",
    "    token_type_ids = encoded.get('token_type_ids', None)\n",
    "    \n",
    "    if token_type_ids is not None:\n",
    "        token_type_ids = token_type_ids.to(device)\n",
    "    \n",
    "    # 获取预测结果\n",
    "    with torch.no_grad():\n",
    "        coarse_probs, fine_probs = model(input_ids, attention_mask, token_type_ids)\n",
    "        \n",
    "        # 应用阈值\n",
    "        coarse_preds = (coarse_probs > threshold).float()\n",
    "        fine_preds = (fine_probs > threshold).float()\n",
    "        \n",
    "        # 应用层次约束\n",
    "        constrained_fine_preds = model.apply_hierarchical_constraint(coarse_preds, fine_preds)\n",
    "    \n",
    "    # 映射结果到标签\n",
    "    coarse_indices = torch.nonzero(coarse_preds[0]).cpu().numpy().flatten()\n",
    "    fine_indices = torch.nonzero(constrained_fine_preds[0]).cpu().numpy().flatten()\n",
    "    \n",
    "    # 将索引转换为标签名称（需要模型中有这些映射）\n",
    "    coarse_label_map = {v: k for k, v in model.coarse_labels.items()}\n",
    "    fine_label_map = {v: k for k, v in model.fine_labels.items()}\n",
    "    \n",
    "    predicted_coarse = [coarse_label_map[idx] for idx in coarse_indices]\n",
    "    predicted_fine = [fine_label_map[idx] for idx in fine_indices]\n",
    "    \n",
    "    return predicted_coarse, predicted_fine\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 在以下主函数中添加判断Jupyter环境的逻辑\n",
    "if __name__ == '__main__':\n",
    "    # 判断是否在Jupyter环境中运行\n",
    "    try:\n",
    "        # 检查是否在Jupyter中运行\n",
    "        get_ipython = globals().get('get_ipython', None)\n",
    "        if get_ipython and 'IPKernelApp' in get_ipython().config:\n",
    "            # 在Jupyter环境中运行，使用默认配置\n",
    "            print(\"Running in Jupyter environment, using default configs\")\n",
    "            configs = get_default_configs()\n",
    "        else:\n",
    "            # 在命令行环境中运行，使用argparse\n",
    "            configs = argparser()\n",
    "    except:\n",
    "        # 任何异常都使用argparse处理\n",
    "        configs = argparser()\n",
    "    \n",
    "    # 设置实验名称\n",
    "    if configs.name is None:\n",
    "        configs.exp_name = \\\n",
    "            f'{os.path.basename(configs.model_name)}' + \\\n",
    "            f'{\"_fp\" if configs.freeze_pooler else \"\"}' + \\\n",
    "            f'_b{configs.batch_size}_e{configs.epochs}' + \\\n",
    "            f'_len{configs.max_length}_lr{configs.lr}'\n",
    "    else:\n",
    "        configs.exp_name = configs.name\n",
    "    \n",
    "    # 设置设备\n",
    "    if configs.device is None:\n",
    "        configs.device = torch.device(\n",
    "            'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        )\n",
    "    \n",
    "    # 调用训练函数\n",
    "    train(configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pykt-hzw3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
