{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import wandb\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import BertModel, AutoModel\n",
    "from transformers import AdamW\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据预处理部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "\n",
    "class ErrorDetectionDataset(Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            data_path,\n",
    "            coarse_labels={\n",
    "                \"字符级错误\": 0,\n",
    "                \"成分残缺型错误\": 1, \n",
    "                \"成分赘余型错误\": 2,\n",
    "                \"成分搭配不当型错误\": 3\n",
    "            },\n",
    "            fine_labels={\n",
    "                \"缺字漏字\": 0,\n",
    "                \"错别字错误\": 1,\n",
    "                \"缺少标点\": 2,\n",
    "                \"错用标点\": 3,\n",
    "                \"主语不明\": 4,\n",
    "                \"谓语残缺\": 5,\n",
    "                \"宾语残缺\": 6,\n",
    "                \"其他成分残缺\": 7,\n",
    "                \"主语多余\": 8,\n",
    "                \"虚词多余\": 9,\n",
    "                \"其他成分多余\": 10,\n",
    "                \"语序不当\": 11,\n",
    "                \"动宾搭配不当\": 12,\n",
    "                \"其他搭配不当\": 13\n",
    "            }\n",
    "    ):\n",
    "        self.data_path = data_path\n",
    "        self.coarse_labels = coarse_labels\n",
    "        self.fine_labels = fine_labels\n",
    "        self._get_data()\n",
    "    \n",
    "    def _get_data(self):\n",
    "        with open(self.data_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        self.data = []\n",
    "        for item in data:\n",
    "            sent = item['sent']\n",
    "            coarse_types = item['CourseGrainedErrorType']\n",
    "            fine_types = item['FineGrainedErrorType']\n",
    "            \n",
    "            # 构建粗粒度标签（多标签）\n",
    "            coarse_label = [0] * len(self.coarse_labels)\n",
    "            for c_type in coarse_types:\n",
    "                if c_type in self.coarse_labels:\n",
    "                    coarse_label[self.coarse_labels[c_type]] = 1\n",
    "            \n",
    "            # 构建细粒度标签（多标签）\n",
    "            fine_label = [0] * len(self.fine_labels)\n",
    "            for f_type in fine_types:\n",
    "                if f_type in self.fine_labels:\n",
    "                    fine_label[self.fine_labels[f_type]] = 1\n",
    "            \n",
    "            self.data.append((sent, coarse_label, fine_label, item.get('sent_id', -1)))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def get_coarse_labels(self):\n",
    "        return self.coarse_labels\n",
    "    \n",
    "    def get_fine_labels(self):\n",
    "        return self.fine_labels\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据加载部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "class ErrorDetectionDataLoader:\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset,\n",
    "        batch_size=16,\n",
    "        max_length=128,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        device=None,\n",
    "        tokenizer_name='./models/bge-large-zh-v1.5'\n",
    "    ):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.max_length = max_length\n",
    "        self.shuffle = shuffle\n",
    "        self.drop_last = drop_last\n",
    "        \n",
    "        if device is None:\n",
    "            self.device = torch.device(\n",
    "                'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "            )\n",
    "        else:\n",
    "            self.device = device\n",
    "        \n",
    "        self.loader = DataLoader(\n",
    "            dataset=self.dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            collate_fn=self.collate_fn,\n",
    "            shuffle=self.shuffle,\n",
    "            drop_last=self.drop_last\n",
    "        )\n",
    "    \n",
    "    def collate_fn(self, data):\n",
    "        sents = [item[0] for item in data]\n",
    "        coarse_labels = [item[1] for item in data]\n",
    "        fine_labels = [item[2] for item in data]\n",
    "        sent_ids = [item[3] for item in data]\n",
    "        \n",
    "        # 编码文本\n",
    "        encoded = self.tokenizer.batch_encode_plus(\n",
    "            batch_text_or_text_pairs=sents,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt',\n",
    "            return_length=True\n",
    "        )\n",
    "        \n",
    "        input_ids = encoded['input_ids'].to(self.device)\n",
    "        attention_mask = encoded['attention_mask'].to(self.device)\n",
    "        token_type_ids = encoded.get('token_type_ids', None)\n",
    "        \n",
    "        if token_type_ids is not None:\n",
    "            token_type_ids = token_type_ids.to(self.device)\n",
    "        \n",
    "        # 处理标签\n",
    "        if coarse_labels[0] == -1:\n",
    "            coarse_labels = None\n",
    "            fine_labels = None\n",
    "        else:\n",
    "            coarse_labels = torch.tensor(coarse_labels, dtype=torch.float).to(self.device)\n",
    "            fine_labels = torch.tensor(fine_labels, dtype=torch.float).to(self.device)\n",
    "        \n",
    "        return input_ids, attention_mask, token_type_ids, coarse_labels, fine_labels, sent_ids\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for data in self.loader:\n",
    "            yield data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据处理和划分的 校验部分  \n",
    "训练集 104个sentences   验证集  27个sentences  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "train data size: 104\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 4]) torch.Size([1, 14])\n",
      "val data size: 27\n"
     ]
    }
   ],
   "source": [
    "# 校验上述数据加载器出来的数据是否正确，输出数据出来看一看\n",
    "train_data_path = '../datas/train.json'\n",
    "val_data_path = '../datas/val.json'\n",
    "train_dataset = ErrorDetectionDataset(train_data_path)\n",
    "train_dataloader = ErrorDetectionDataLoader(train_dataset, batch_size=1)\n",
    "val_dataset = ErrorDetectionDataset(val_data_path)\n",
    "val_dataloader = ErrorDetectionDataLoader(val_dataset, batch_size=1)\n",
    "\n",
    "cnt_train=0\n",
    "for batch in train_dataloader:\n",
    "    cnt_train += 1\n",
    "    input_ids, attention_mask, token_type_ids, coarse_labels, fine_labels, sent_ids = batch\n",
    "    print(input_ids.shape, attention_mask.shape, token_type_ids.shape, coarse_labels.shape, fine_labels.shape)\n",
    "    # break\n",
    "print(\"train data size:\", cnt_train)\n",
    "\n",
    "cnt_val=0\n",
    "for batch in val_dataloader:\n",
    "    cnt_val += 1\n",
    "    input_ids, attention_mask, token_type_ids, coarse_labels, fine_labels, sent_ids = batch\n",
    "    print(input_ids.shape, attention_mask.shape, token_type_ids.shape, coarse_labels.shape, fine_labels.shape)\n",
    "    # break\n",
    "print(\"val data size:\", cnt_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "作为一名英语课代表，对徐老师的认识可能比同学更清楚。 [0, 1, 0, 0] [0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0] 5059\n",
      "因而，保存良好的家风，摒弃有害而无益的家风，是有助于人成长的一大益事。 [0, 0, 0, 1] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0] 254\n",
      "早上阳光明媚，如我的心灵，丝毫不被外界的阴云所引响，只觉得蔚蓝的天空上漂浮着的云，星星点点素雅的小花，让人倍感舒爽到了中午，远处传来一丝丝的锣鼓声，一众人从小到大到我眼前，看清上面有一个方形的木盒，很大，盖住了天上的太阳。 [1, 1, 0, 0] [0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0] 5612\n",
      "周末时，老师邀请我与其他的同学去为母校的同学们界绍一位著名的冬奥运动员。 [1, 0, 0, 0] [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 4614\n",
      "每次当我回到家时，从来也没有再见到那个向我飞扑过来的狗，也从没忘记那个向我撒娇。认人心软的狗，我家的那地地毯也被妈妈洗了，也看不见那让我永远忘不掉的梅花。 [1, 0, 1, 0] [0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0] 5735\n",
      "老师刚说，话音未落就见我们消失在果树之中，果树上都是橙子，我们8人分成4组，两人两把剪刀一个栏子，我们两人一起行动，另一人是小宇，我的好兄弟， [1, 0, 0, 0] [0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 8885\n",
      "我不仅从一个小学生变成了初中生，我也懂得了如何去诊惜生活中的每分每秒。 [1, 0, 0, 0] [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 5342\n",
      "记忆的盒子瞬打开，把我拉回了相片中的时间。 [1, 0, 0, 0] [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 3774\n",
      "先从后面偷偷摸摸进去，先是杀了两个厨房里的仆人，后是把”老的小的“通通“砍掉”，刀都能砍卷了。 [0, 1, 0, 0] [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0] 2474\n",
      "回溯历史，君不见太史公司马迁谨等父亲命训，恪守《命于训》之家风教诲，终写成被誉为“史家之学唱，天蕴之《离骚》”，于青史留下重要的一笔。 [1, 1, 0, 0] [0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0] 2812\n",
      "还记得那天，董晨欣说她买了本《山海经》里面各路神仙怪物都有，最主要的是都非常丑。 [1, 0, 0, 0] [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 3602\n",
      "狗狗事件不值一提了，但我奶奶您辛苦了，我以念念不忘的这件事来以此激励自己坚持走下去不让您老人家失望。 [1, 0, 1, 0] [0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0] 145\n",
      "绵风微掠，缕缕清香经了夏风吹漾和水波的摇拂，悠悠荡至心间，仿佛隐约远山淡淡轮廓中渺茫的笛声。 [0, 0, 1, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0] 511\n",
      "仙人掌是生活在南北回归线之间的沙漠里。 [0, 0, 1, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0] 25\n",
      "坐在车上，望见奶奶给我的袋子里有一瓶牛奶。 [0, 1, 0, 1] [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0] 6060\n",
      "也就是说，光鲜亮丽的背后，总会有一段艰辛的奋斗与自强不息的奋斗。 [0, 0, 1, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0] 1298\n",
      "人类文明走过数万年的沉淀，将智慧的结晶浓缩在书中。 [0, 0, 0, 1] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0] 8356\n",
      "它们，为了生命能在这寒冬中存活，挺立起了身躯，多像那一个个为了生活坚苦奋斗的人们！ [0, 0, 1, 0] [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0] 6001\n",
      "我挑战了自己、战胜了自己、突破了自己！ [1, 0, 0, 0] [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 6153\n",
      "这次的期中考试，我终于让徐老师欣慰，尽管不是班里最高分，但也达到了很大的进步。 [0, 0, 0, 1] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0] 5062\n",
      "2007年10月24日的那天不仅是中国机械工程的第一所任务，承载了探月卫星成功发射，也是我出生的日子，因为这天的重要性，所以我的名字也与它息息相关——张宇航。 [1, 1, 0, 0] [0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0] 2544\n",
      "我还要感谢我的同学，回忆着我的初中时光，我的脑海总映放着各位同学与我一起学习、进步、相互帮助的画面，大家总争着第一，早早的踏入校园，用行动书写着“一年之计在于晨”。 [1, 0, 0, 1] [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0] 9341\n",
      "十年前，一位四十岁的女人带着儿子踏进我们的小区，女人脸上挂着一副眼镜，隐藏住她那小小的眼睛，泛黄的头发里混入了几根白发，背上背着大书包，手上提着行李箱，她儿子眼睛大大的，脸小小的，肉嘟嘟地，长的一点也不像她。 [1, 0, 0, 1] [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0] 1630\n",
      "做到以上两点，相信大家的文笔更至于精神都能有极大的进步。 [0, 0, 0, 1] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1] 9556\n",
      "那黑酸酸的树影，在这样的夜色下隐隐约约，看不太真切。 [1, 0, 0, 0] [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 7397\n",
      "而且日军在占领南京后，对南京的居民进行了血腥大屠杀，仅仅一周，30万手无寸铁的居民和放下武器士兵就被屠杀了。 [0, 1, 0, 0] [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0] 1874\n",
      "“她。是一个坚强，坚持不懈，勇敢拼搏的人。希望大家也能习一下她那些优秀的精神品质，争做一名新时代的好少年！无论是学习还是生活上，都要像她一样，”赢在家门口“，取得成功！ [1, 0, 0, 0] [1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 480\n",
      "由此可见，真正的读者外界影响，专心致志的人，但是，你捧着书，像他们一样，就成为读者了吗？ [1, 1, 0, 0] [0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0] 7579\n",
      "我是本次云旅游的导游--新新，今天我们所要参观的是中华古城--南京，共同沉浸于金陵之古韵。 [0, 1, 0, 0] [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0] 106\n",
      "说话讲究技巧不仅为了别人也为了你，你说话若是不好听，别人还会愿意与你交往吗？ [1, 0, 0, 0] [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 9454\n",
      "在我初二暑假时，我把厚厚的暑假作业写完后我心里就只想着如何去奖励自己了。 [1, 0, 1, 0] [0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0] 9385\n",
      "不久，到了饭店，我果然是天生的吃货，纵使再不情愿，也还是被满桌的菜吸引了眼球。 [1, 0, 0, 0] [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 2865\n",
      "正如我文章开头的景象，也许有的人靠在躺椅上，依旧是无法静下心来，可在书读着发觉它乏味，索性就放弃不读，换一本读，甚至犹于环境的舒适，他们还会昏睡过去，而这样的人，充其量只是“正在放松的人”，而决非读者。 [1, 1, 0, 1] [0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0] 7577\n",
      "其实这就是实践的可贵，知识终归是抽象的概念，不化为行动徒劳无用，同时实践也要检验真理的唯一标准，通过尝试与探索能够更加深入地了解知识的本质，“纸上得来终觉浅，绝知此事要躬行”，实践出真知。 [0, 0, 0, 1] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0] 8175\n",
      "中华民族的历史长河滚滚不息，流过了五千年的漫长当月，见证了华夏文明的璀璨瑰丽，也记录了无数王朝的更迭兴衰。 [1, 0, 0, 1] [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0] 1059\n",
      "相信大家都看过冬奥会了吧，你们一定印象深刻的中国冬奥选手，他们在本届冬奥会上大显伸手，取得了由史以来最好的成绩。 [1, 1, 0, 0] [0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0] 5425\n",
      "周五，他的妈妈都要带她去专业训练队训练，从学校到那里要4个小时，这4个小时，她就一直在车上看书，学习，没有一次报怨累。 [1, 0, 0, 0] [0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 4294\n",
      "比外她还夺得了冬奥会上的女子自由式滑雪U型场地的金牌和女子自由式滑雪坡面障碍技巧的银牌。 [1, 0, 1, 0] [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0] 4287\n",
      "在一次阴差阳错之下我成功地获得了加入中国共青团的的欲选名额，被要求参加中国共青团团课，争取早日进入共青团。 [1, 0, 1, 0] [0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0] 6809\n",
      "小学毕业那天，我有幸亲眼见证了兵马俑这一令人震撼的历史瑰宝。 [0, 0, 0, 1] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0] 1060\n",
      "可能仍有人质疑，吴用的武义并不高强，甚至令人感觉有些柔弱，不能像鲁智深一样去保护自己， [1, 0, 0, 0] [0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 4081\n",
      "走后还惦念金氏父女应己逃走，心中大喜。 [0, 1, 0, 0] [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0] 3016\n",
      "刚进初中之时，曾占据班级前十之位，已是超越大多数人，但就有些懈怠，可在如此紧张的初中生活中，犹如一片波涛汹涌的大海之中来回在一片片砖块上跳跃，一不小心便会名落孙山。 [1, 1, 0, 0] [0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0] 9030\n",
      "我们抄小道到老门东，正好，就在我热得快趴到地上时，“救星”出现了，一家小卖铺。 [1, 0, 1, 0] [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0] 4304\n",
      "所以，毅力对于一个向往成功的人是十分重要的，杨根思说过“就是有九十九个困难，只要有一个坚强的意志就不困难。” [1, 1, 0, 0] [0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0] 9895\n",
      "黎明时分，作为一片花瓣，我来到了南京，见征了南京历史的悠久和人文景观，作为一片花辨，我也在南京的梧桐树下与南法这个城市一起睡着了。 [1, 0, 0, 1] [0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0] 646\n",
      "这是老班在日常行为上对我们的管辖，虽然脾气暴躁但老班的学习精神值得学习。 [1, 0, 0, 1] [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0] 5085\n",
      "著名的音乐家贝多芬，在音乐上有着极高的天赋，却在最得意时，他失聪了。 [0, 0, 1, 0] [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0] 9018\n",
      "正是因为他有着乐观向前，活在当下的心态，没有沉侵在以往安享的生治中，而是靠着自己闯出一片天地。 [1, 0, 0, 0] [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 2318\n",
      "我无意间的抬头，我看到了电线杆前站了一个身形佝偻的清洁工。 [1, 0, 1, 0] [0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0] 1704\n",
      "“彼方尚有荣光在”选好青春赛道努力拼搏，走好自己的路。别选好走的路。 [1, 0, 0, 0] [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 2162\n",
      "那天晚上，雨下得那般大，雨“哗哗啦啦”滴在外面的台子上，早晨起来，就淅淅沥沥的，“春天终于要来啦！” [0, 0, 1, 0] [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0] 5296\n",
      "这样的人同学面前倒是大方了，可内心终究还是迟早得用明朗的笑声去面对。 [0, 1, 1, 0] [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0] 5245\n",
      "但我身上没钱，我咽了咽口水，只好顶着饥饿与亲爱的美食“拜拜”。 [0, 0, 0, 1] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0] 7891\n",
      "因难就更多了，牛顿被苹果为什么会从树下掉下来感到困惑，最后研究出了万有引律；李时珍为百姓疾病困浇，走遍天下，最后完成了《本草纲目》；司马迁克服了在狱中的种种痛苦，完成了千古名著《史记》。 [1, 0, 0, 1] [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1] 9014\n",
      "我其实只想对这种人说一句话：“当有一天，早上没有人喊醒你，放学没有人接你，有里只有冷冰冰的家具和你，Pleasetellme!这是自由？还是孤独。” [1, 0, 0, 0] [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 5150\n",
      "逐渐这也成为了我的习惯，这与之前“黑暗”的我是完全不一样的！ [0, 0, 0, 1] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0] 9393\n",
      "一个人如果只读书，不实践，那么只能书呆子。 [0, 1, 0, 0] [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0] 9052\n",
      "过了一会同学和刚才在旁边跟我们说话的老大爷跑了进来，老大爷手里提着医疗箱，他问我：“你的情况我以经知道了，接下来我会帮你治疗，可能会有点疼”。 [1, 0, 0, 0] [1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 5225\n",
      "走过一段墓道，面前同样的白石蓝瓦的单檐歇山式建陵门。 [0, 1, 1, 0] [0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0] 2898\n",
      "从这一件事我可以看出，武松的勇猛忠义、侠肝义胆、仗义诚信，如果我是他的朋友，他一定会在现实生活中帮助我面对困难，也会对朋友用心、不欺骗我，他是一个有良心敢作敢当的斗士，在生活中我会向他学习。 [1, 0, 0, 0] [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 7015\n",
      "在出生之前，爸爸妈妈给我取了小名“开心”，据说，这个创意风靡妈妈的班公室，我妈以前同事的孩子，许多是“高兴”之类。 [1, 1, 0, 0] [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0] 2845\n",
      "《傅雷家书》收录了1954年到1966年间傅雷夫妇写给两个儿子的家信一百多封， [1, 0, 0, 1] [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0] 6887\n",
      "我有些难受的看着那位大爷——我一直乐意去他那打饭。 [1, 0, 0, 0] [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 4816\n",
      "真正的读者并不是看'书'的人，而是专心致志地投入进去并从书中得到启发。 [1, 1, 0, 0] [0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0] 9048\n",
      "一阵风吹过，树枝微微作响，仿佛在感冬关心。 [1, 0, 0, 0] [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 6275\n",
      "愿我的家乡广西越来越繁荣，也愿祖国越来越富强，完成伟大复兴！ [0, 0, 0, 1] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0] 2558\n",
      "”生命只有一次，污的畅快淋漓，未米米日方长保持好心情”她飞奔回到家，不顾那厚湿的大衣，在白张上下写开头。 [1, 1, 0, 1] [0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0] 9881\n",
      "它确实可以简化成“走”，但其实这两个词根本上的区别是：前进是主动的状态，它包含着对生活的的热情和积极；而走则是麻木与被动的。仿佛根本无所谓自己踏上的路。 [1, 0, 1, 0] [0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0] 6791\n",
      "阳光是暖的，它是有亲情味儿的。这是一条与远在天堂的爷爷相接的纽带。 [1, 0, 1, 0] [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0] 4024\n",
      "然我想说，并不是所有书籍都有能力完成承载读者，如同一艘船一样驶向对生活、对人类、乃至对社会的思考的海面的。 [1, 1, 0, 0] [1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0] 8698\n",
      "他是我们中华民族的重要建设者，也为中华人民共和国在国际舞台上站稳脚步做出了巨大贡献。 [0, 0, 1, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0] 4394\n",
      "在亭下有一处泉水，用这一处的泉水酿酒，酒香且清，是可谓上品。 [0, 0, 1, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0] 9250\n",
      "惊天动地的大事，我在一旁看着妈妈，妈妈的脸上露出不解的神情。 [0, 1, 0, 0] [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0] 4307\n",
      "忽然“嗒”一声微弱的脆响雨滴落入了小水坑中，凌霄花又抬起了头，皱巴巴的小脸，却偏苦涩涩地笑着，雨势渐停，方才显得粉白的苦笑，又有了红润的喜色，轻抖身上的雨珠，挺直了腰板。 [1, 1, 1, 0] [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0] 4927\n",
      "一天下来，做完作业后总会回想起那些发挥失常，粗心大意的瞬间。信心也随之流失。 [1, 0, 0, 1] [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0] 8714\n",
      "就算风吹日晒、大雨滂沱，依然向上拼搏，顽强开放。在同一个花丛中彼此搀扶，无畏风雨。 [1, 1, 0, 0] [0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0] 3626\n",
      "你见过大风大雨，得过许多奖励，即使一模的失败使你错过了特招的机会，但你仍要不屈不挠，勇往直前，奋力一搏，记住，中考才是这段马拉松的终点。 [0, 0, 0, 1] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1] 7444\n",
      "具体时间早已记不清了罢，却记得那天晴空万里，骄阳似火。 [0, 0, 1, 1] [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1] 4689\n",
      "大家好，我是今天的阳光小导游新新，由我来带领大家寻访北京的天安门广场。 [0, 0, 0, 1] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0] 9808\n",
      "如果你在我们学校看见一位头发披着，戴着眼镜，基本每次见她都穿着牛仔裤，说话温柔的那就是我们唐老师， [0, 1, 1, 1] [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0] 2434\n",
      "“千磨万击还坚励，任尔东西南北风。”就让我们一起自强不息，乘着自强的大船驶向成功的彼岸！ [1, 0, 0, 0] [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 9403\n",
      "她于是一脸骄傲起来。越发肯定自己的想法。 [1, 0, 0, 1] [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0] 1424\n",
      "我反驳道：“青春不是用来欣赏的吗？正值年轻，不应该去那些年迈时看不到的风景，做那些年迈时做不了的事情吗？ [1, 1, 0, 0] [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0] 1908\n",
      "她脸不大，带上眼镜后更为娇小，与众不同的是，她带上眼睛似乎更有神了。 [1, 1, 0, 0] [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0] 1077\n",
      "比知识，也正如那句古话“腹有诗书气自华。” [0, 0, 1, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0] 4792\n",
      "他加了，这时候他的目的已经改变。不是冠军，不是三连冠，是自己的梦想，人类的极限。 [1, 1, 0, 0] [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0] 5454\n",
      "而我的爸爸在一家厦门的公司上班，名字叫英华达，后来，不知怎的的，所以经常不在家。 [0, 1, 1, 0] [0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0] 2847\n",
      "阅读还能充实你的知识，当你登上了泰山，有人会说啊，真美，而你会说，会当凌绝顶，一览众山小，这感是不是立马就不一样了？ [1, 0, 0, 0] [1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 7280\n",
      "我与他相识不久，只记得他是单眼皮，一双水汪汪的大眼睛，和一张能说会道的嘴巴。 [0, 1, 0, 0] [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0] 4499\n",
      "当鲁迅先生赞美东北人民的奋斗与抗争、当青年人，农民们读见先生的文章，他们的思想就在一刻间共通了，行为的不同恰反应出精神的相同，信念的一致恰指引了行动的分別。 [1, 0, 0, 1] [0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1] 2392\n",
      "具体来说，英雄是技艺超群，无私奉献，不畏艰险而拼搏的人。 [1, 0, 1, 0] [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0] 8798\n",
      "走近些看，原来是几位环卫工爷爷，为了避免路面结冰酿成危险，正在忙着铲雪，我特意路过他们的身旁，看到一双双慈祥的眼镜。一张张苍老，布满皱纹，饱经风霜的脸。 [1, 0, 0, 0] [0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 5987\n",
      "由于电子商品无论如何多少都有一定的蓝光，电子阅读次数多了，它会伤害你的眼睛，使其视力下降；而若是阅读纸质书，那你不用担心这一问题。 [0, 0, 1, 0] [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0] 7323\n",
      "也许就是这样的您，可能也只有您能把我身边的那个傲娇的，从不听话敢去校长室的小公主归于您自己，让她对您百般温顺，小学的六年时光，只有您让我全身心得放下防备，放心的把自己的后背交给您。 [1, 0, 1, 1] [0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1] 5594\n",
      "应多读一些有内涵，有价值的名著，提升自己的文学素养，去感受大家们笔下的人物形象与答与其中的名言，如鲁迅笔下的阿Q，代表了当时封建主义的种种缺点，表达对封建社会的批判，用一个人仅巧妙代表了当时的整个社会风气。 [1, 0, 1, 1] [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0] 6621\n",
      "我也不敢说什么，心里也挺烦，就直接下楼了，连声招呼都没打。 [0, 0, 0, 1] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0] 4264\n",
      "香槟色的天空像半透明的绸布铺在了大气中，只有在顶部还嵌进几抹浅蓝，浅蓝下是几朵似有似无的白云，它们聚得很快，散得也很快，像极了我们在一起的短短的三年， [1, 0, 1, 0] [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0] 5969\n",
      "为了进一步为你们解答，如下是我向大家发起的倡仪： [1, 1, 0, 0] [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0] 7330\n",
      "一天我气冲冲回到房间，一抬头望见了那被雨打击一天一夜的多肉，对不起我的多肉，对不起我的家人。 [0, 1, 0, 0] [0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0] 5306\n",
      "虽说他只是这本书中配角，但人气与热度却远远超过了两位男主，而我想这其中的原因应该就是他坎坷而又悲惨的一生了吧。 [0, 0, 1, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0] 5018\n",
      "——平时不怎么交流，也不知道有什么话该说，在一起小时候还能玩一玩，长大了也不知该玩啥了， [1, 0, 0, 1] [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0] 4261\n",
      "若一开始便弃了，唯一的抓手弃掉，后面的一切都是空中楼阁。 [0, 1, 1, 0] [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0] 570\n",
      "此外，对于机会不要盲目。 [0, 1, 0, 0] [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0] 1496\n"
     ]
    }
   ],
   "source": [
    "# 检查一下dataset中处理出来的数据是否正确\n",
    "for data in train_dataset:\n",
    "    sent, coarse_label, fine_label, sent_id = data\n",
    "    print(sent, coarse_label, fine_label, sent_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 处理对最后14个错误的描述的embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fine_description_embedding(model_path):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModel.from_pretrained(model_path)\n",
    "\n",
    "    # 设置为评估模式\n",
    "    model.eval()\n",
    "\n",
    "    # 存储所有句子的向量表示\n",
    "    fine_description = []\n",
    "\n",
    "    # 获取每一个细粒度错误的详细描述\n",
    "    sentences = [\n",
    "        \"句子中缺少字（需要添加）\",\n",
    "        \"句子中出现错别字（需要修改或删除）\",\n",
    "        \"应断句的地方没有使用标点把句子断开\",\n",
    "        \"标点使用错误，如本来应使用句号却分号或括号使用错误\",\n",
    "        \"句子缺少主语，或主语不清晰，修改是要增加主语或使主语显现\",\n",
    "        \"句子缺少谓语，修改是要增加谓语\",\n",
    "        \"句子缺少宾语，修改是要增加宾语\",\n",
    "        \"修改是要增加除主语、谓语、宾语之外的其他情况\",\n",
    "        \"一般是句子较长，前一个主语说出后，紧接着有一个较长、较复杂的形容词修饰过了一会儿，我忘记了测试，老师说等消息，（去掉主语：修改是要删除主语）\",\n",
    "        \"副词\\\"的\\\"、\\\"所\\\"的多余，需改掉或删除副词\",\n",
    "        \"除主语、谓词之外的成分多余，修改特别容易混淆区，都是一些平时见不到的洲狮，孟加拉虎等猛兽，据说这里有300多只老虎生活在这里呢是不是很惊叹呢！（多个\\\"这里\\\"，表达重复）\",\n",
    "        \"句子中词语或子句的顺序不合理，修改是调换某几个词汇或子句的顺序\",\n",
    "        \"谓语与宾语搭配不当，修改是要用其他词替换句子的谓语或宾语\",\n",
    "        \"除动宾，语序不当不当之外的其他搭配不当情况，修改是要用其他词替换句子中的某个成分\"\n",
    "    ]\n",
    "    encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "        \n",
    "    fine_description = model_output.last_hidden_state[:, 0, :]\n",
    "    return fine_description\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 模型部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel\n",
    "\n",
    "class HierarchicalErrorClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        pretrained_model_name, \n",
    "        num_coarse_labels=4, \n",
    "        num_fine_labels=14, \n",
    "        freeze_pooler=False, \n",
    "        dropout=0.2,\n",
    "        fine_description=None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.freeze_pooler = freeze_pooler\n",
    "        self.bert = AutoModel.from_pretrained(pretrained_model_name)\n",
    "        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n",
    "        self.fine_description = fine_description.to(device)\n",
    "        \n",
    "        if freeze_pooler:\n",
    "            for param in self.bert.pooler.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # 粗粒度分类器\n",
    "        self.coarse_classifier = nn.Linear(self.bert.config.hidden_size, num_coarse_labels)\n",
    "        \n",
    "        # 细粒度分类器\n",
    "        self.fine_classifier = nn.Linear(self.bert.config.hidden_size, num_fine_labels)\n",
    "        \n",
    "        # 定义粗粒度类别和对应的细粒度索引的映射\n",
    "        self.coarse_to_fine_indices = {\n",
    "            0: [0, 1, 2, 3],    # 字符级错误\n",
    "            1: [4, 5, 6, 7],    # 成分残缺型错误\n",
    "            2: [8, 9, 10],      # 成分冗余型错误\n",
    "            3: [11, 12, 13]     # 成分搭配不当型错误\n",
    "        }\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids=None):\n",
    "        # BERT编码\n",
    "        if token_type_ids is not None:\n",
    "            outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        else:\n",
    "            outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        pooled_output = outputs.pooler_output\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        \n",
    "        # 粗粒度分类\n",
    "        coarse_logits = self.coarse_classifier(pooled_output)\n",
    "        coarse_probs = torch.sigmoid(coarse_logits)\n",
    "        \n",
    "        # # 细粒度分类\n",
    "        # fine_logits = self.fine_classifier(pooled_output)\n",
    "        # fine_probs = torch.sigmoid(fine_logits)\n",
    "        \n",
    "        # 细粒度分类，包含14个补充信息\n",
    "        fine_logits = torch.matmul(pooled_output, self.fine_description.T)\n",
    "        fine_probs = torch.sigmoid(fine_logits)\n",
    "        \n",
    "        return coarse_probs, fine_probs\n",
    "    \n",
    "    def apply_hierarchical_constraint(self, coarse_preds, fine_preds):\n",
    "        \"\"\"\n",
    "        应用层次约束：如果粗粒度类别预测为负，则该粗粒度下的所有细粒度类别均设为负\n",
    "        \n",
    "        Args:\n",
    "            coarse_preds: 粗粒度预测结果，shape [batch_size, num_coarse_labels]\n",
    "            fine_preds: 细粒度预测结果，shape [batch_size, num_fine_labels]\n",
    "            \n",
    "        Returns:\n",
    "            应用约束后的细粒度预测结果\n",
    "        \"\"\"\n",
    "        constrained_fine_preds = fine_preds.clone()\n",
    "        \n",
    "        # 遍历每个样本\n",
    "        for i in range(coarse_preds.size(0)):  # 遍历每个样本（第一个维度是批次大小）\n",
    "            # 对每个粗粒度类别\n",
    "            for coarse_idx, fine_indices in self.coarse_to_fine_indices.items():\n",
    "                # 如果粗粒度为负，则对应的细粒度全部设为负\n",
    "                if coarse_preds[i, coarse_idx] == 0:\n",
    "                    constrained_fine_preds[i, fine_indices] = 0\n",
    "        \n",
    "        return constrained_fine_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练、验证和测试部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "包的导入、参数定义、计算最终指标的函数定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb\n",
    "import random\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from transformers import AdamW\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "\n",
    "# 在代码开始处禁用 wandb 记录\n",
    "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
    "\n",
    "# 导入自定义模块\n",
    "# from error_detection_dataset import ErrorDetectionDataset\n",
    "# from error_detection_dataloader import ErrorDetectionDataLoader\n",
    "# from hierarchical_classifier_model import HierarchicalErrorClassifier\n",
    "\n",
    "# 如果分项目的时候就可以使用这个参数解析函数\n",
    "def argparser():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--model_name', type=str, default='./models/bge-large-zh-v1.5')\n",
    "    parser.add_argument('--num_coarse_labels', type=int, default=4)\n",
    "    parser.add_argument('--num_fine_labels', type=int, default=14)\n",
    "    parser.add_argument('--dropout', type=float, default=0.3)\n",
    "    parser.add_argument('--freeze_pooler', action='store_true', help='Flag to freeze the pooler layer')\n",
    "    parser.add_argument('--batch_size', type=int, default=16)\n",
    "    parser.add_argument('--max_length', type=int, default=128)\n",
    "    parser.add_argument('--lr', type=float, default=1e-5)\n",
    "    parser.add_argument('--epochs', type=int, default=40)\n",
    "    parser.add_argument('--device', type=str, required=False)\n",
    "    parser.add_argument('--project', type=str, default='hierarchical_error_detection')\n",
    "    parser.add_argument('--entity', type=str, default='akccc')\n",
    "    parser.add_argument('--name', type=str, required=False)\n",
    "    parser.add_argument('--seed', type=int, default=42)\n",
    "    parser.add_argument('--data_path', type=str, default='../datas/train.json')\n",
    "    parser.add_argument('--val_data_path', type=str, default='../datas/val.json')\n",
    "    parser.add_argument('--test_data_path', type=str, default='../dats/val.json')\n",
    "    parser.add_argument('--checkpoint_dir', type=str, default='checkpoints')\n",
    "    parser.add_argument('--threshold', type=float, default=0.75)\n",
    "    parser.add_argument('--patience', type=int, default=5)\n",
    "    \n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "\n",
    "# 如果在Jupyter Notebook中运行，可以使用这个自定义参数函数替代argparser\n",
    "def get_default_configs():\n",
    "    \"\"\"在Jupyter环境中使用的默认配置，避免argparse解析错误\"\"\"\n",
    "    class Args:\n",
    "        def __init__(self):\n",
    "            self.model_name = './models/bge-large-zh-v1.5'\n",
    "            self.num_coarse_labels = 4\n",
    "            self.num_fine_labels = 14\n",
    "            self.dropout = 0.3\n",
    "            self.freeze_pooler = False\n",
    "            self.batch_size = 8\n",
    "            self.max_length = 128\n",
    "            self.lr = 1e-5\n",
    "            self.epochs = 40\n",
    "            self.device = device\n",
    "            self.project = 'hierarchical_error_detection'\n",
    "            self.entity = 'akccc'\n",
    "            self.name = None\n",
    "            self.seed = 3407\n",
    "            self.data_path = '../datas/train.json'\n",
    "            self.val_data_path = '../datas/val.json'\n",
    "            self.test_data_path = '../datas/val.json'\n",
    "            self.checkpoint_dir = 'checkpoints'\n",
    "            self.threshold = 0.5\n",
    "            self.patience = 5\n",
    "            self.exp_name = 'default_run'\n",
    "    return Args()\n",
    "\n",
    "\n",
    "def calculate_metrics(labels, predictions, average='micro'):\n",
    "    \"\"\"\n",
    "    计算各种评估指标\n",
    "    \n",
    "    Args:\n",
    "        labels: 真实标签\n",
    "        predictions: 预测标签\n",
    "        average: 平均方法，'micro'或'macro'\n",
    "        \n",
    "    Returns:\n",
    "        包含各种指标的字典\n",
    "    \"\"\"\n",
    "    # 将数组转换为numpy格式以确保兼容性\n",
    "    labels = np.array(labels)\n",
    "    predictions = np.array(predictions)\n",
    "    \n",
    "    # 计算微平均和宏平均的F1分数\n",
    "    micro_f1 = f1_score(labels, predictions, average='micro')\n",
    "    macro_f1 = f1_score(labels, predictions, average='macro')\n",
    "    \n",
    "    # 计算样本级别的准确率（每个样本的所有标签都要正确）\n",
    "    sample_acc = accuracy_score(labels, predictions)\n",
    "    \n",
    "    return {\n",
    "        'micro_f1': micro_f1 * 100,  # 转换为百分比\n",
    "        'macro_f1': macro_f1 * 100,\n",
    "        'accuracy': sample_acc * 100\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in Jupyter environment, using default configs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1/40: 100%|█| 13/13 [00:00<00:00, 13.67batch/s, coarse_loss=0.641, fine_loss=1.064, loss=1.705\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/40:\n",
      "  Train Loss: 2.1052\n",
      "  Train Coarse-grained Metrics:\n",
      "    Micro F1: 39.44\n",
      "    Macro F1: 30.83\n",
      "    Accuracy: 11.54\n",
      "  Train Fine-grained Metrics (Unconstrained):\n",
      "    Micro F1: 16.76\n",
      "    Macro F1: 12.88\n",
      "    Accuracy: 0.00\n",
      "  Train Fine-grained Metrics (Constrained):\n",
      "    Micro F1: 11.81\n",
      "    Macro F1: 7.32\n",
      "    Accuracy: 0.96\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "  Val Loss: 1.3504\n",
      "  Val Coarse-grained Metrics:\n",
      "    Micro F1: 56.72\n",
      "    Macro F1: 20.65\n",
      "    Accuracy: 29.63\n",
      "  Val Fine-grained Metrics (Unconstrained):\n",
      "    Micro F1: 9.88\n",
      "    Macro F1: 5.40\n",
      "    Accuracy: 0.00\n",
      "  Val Fine-grained Metrics (Constrained):\n",
      "    Micro F1: 9.68\n",
      "    Macro F1: 3.02\n",
      "    Accuracy: 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/40: 100%|█| 13/13 [00:00<00:00, 13.79batch/s, coarse_loss=0.650, fine_loss=0.939, loss=1.589\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/40:\n",
      "  Train Loss: 1.5290\n",
      "  Train Coarse-grained Metrics:\n",
      "    Micro F1: 45.21\n",
      "    Macro F1: 20.77\n",
      "    Accuracy: 15.38\n",
      "  Train Fine-grained Metrics (Unconstrained):\n",
      "    Micro F1: 23.14\n",
      "    Macro F1: 12.53\n",
      "    Accuracy: 0.00\n",
      "  Train Fine-grained Metrics (Constrained):\n",
      "    Micro F1: 22.47\n",
      "    Macro F1: 7.44\n",
      "    Accuracy: 0.96\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "  Val Loss: 1.4677\n",
      "  Val Coarse-grained Metrics:\n",
      "    Micro F1: 57.58\n",
      "    Macro F1: 20.65\n",
      "    Accuracy: 29.63\n",
      "  Val Fine-grained Metrics (Unconstrained):\n",
      "    Micro F1: 0.00\n",
      "    Macro F1: 0.00\n",
      "    Accuracy: 0.00\n",
      "  Val Fine-grained Metrics (Constrained):\n",
      "    Micro F1: 0.00\n",
      "    Macro F1: 0.00\n",
      "    Accuracy: 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/40: 100%|█| 13/13 [00:00<00:00, 13.76batch/s, coarse_loss=0.639, fine_loss=0.626, loss=1.265\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/40:\n",
      "  Train Loss: 1.4089\n",
      "  Train Coarse-grained Metrics:\n",
      "    Micro F1: 48.67\n",
      "    Macro F1: 21.76\n",
      "    Accuracy: 20.19\n",
      "  Train Fine-grained Metrics (Unconstrained):\n",
      "    Micro F1: 20.57\n",
      "    Macro F1: 14.59\n",
      "    Accuracy: 0.00\n",
      "  Train Fine-grained Metrics (Constrained):\n",
      "    Micro F1: 18.44\n",
      "    Macro F1: 6.02\n",
      "    Accuracy: 0.00\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "  Val Loss: 1.5886\n",
      "  Val Coarse-grained Metrics:\n",
      "    Micro F1: 57.58\n",
      "    Macro F1: 20.65\n",
      "    Accuracy: 29.63\n",
      "  Val Fine-grained Metrics (Unconstrained):\n",
      "    Micro F1: 0.00\n",
      "    Macro F1: 0.00\n",
      "    Accuracy: 0.00\n",
      "  Val Fine-grained Metrics (Constrained):\n",
      "    Micro F1: 0.00\n",
      "    Macro F1: 0.00\n",
      "    Accuracy: 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/40: 100%|█| 13/13 [00:00<00:00, 13.78batch/s, coarse_loss=0.656, fine_loss=0.857, loss=1.513\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/40:\n",
      "  Train Loss: 1.4906\n",
      "  Train Coarse-grained Metrics:\n",
      "    Micro F1: 48.06\n",
      "    Macro F1: 19.02\n",
      "    Accuracy: 20.19\n",
      "  Train Fine-grained Metrics (Unconstrained):\n",
      "    Micro F1: 16.51\n",
      "    Macro F1: 11.86\n",
      "    Accuracy: 0.00\n",
      "  Train Fine-grained Metrics (Constrained):\n",
      "    Micro F1: 15.72\n",
      "    Macro F1: 6.15\n",
      "    Accuracy: 1.92\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "  Val Loss: 1.2465\n",
      "  Val Coarse-grained Metrics:\n",
      "    Micro F1: 57.58\n",
      "    Macro F1: 20.65\n",
      "    Accuracy: 29.63\n",
      "  Val Fine-grained Metrics (Unconstrained):\n",
      "    Micro F1: 20.34\n",
      "    Macro F1: 4.76\n",
      "    Accuracy: 3.70\n",
      "  Val Fine-grained Metrics (Constrained):\n",
      "    Micro F1: 20.34\n",
      "    Macro F1: 4.76\n",
      "    Accuracy: 3.70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/40: 100%|█| 13/13 [00:00<00:00, 13.85batch/s, coarse_loss=0.666, fine_loss=0.509, loss=1.175\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/40:\n",
      "  Train Loss: 1.3501\n",
      "  Train Coarse-grained Metrics:\n",
      "    Micro F1: 48.28\n",
      "    Macro F1: 18.98\n",
      "    Accuracy: 21.15\n",
      "  Train Fine-grained Metrics (Unconstrained):\n",
      "    Micro F1: 23.76\n",
      "    Macro F1: 15.61\n",
      "    Accuracy: 1.92\n",
      "  Train Fine-grained Metrics (Constrained):\n",
      "    Micro F1: 22.37\n",
      "    Macro F1: 8.12\n",
      "    Accuracy: 1.92\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "  Val Loss: 1.0523\n",
      "  Val Coarse-grained Metrics:\n",
      "    Micro F1: 57.58\n",
      "    Macro F1: 20.65\n",
      "    Accuracy: 29.63\n",
      "  Val Fine-grained Metrics (Unconstrained):\n",
      "    Micro F1: 7.69\n",
      "    Macro F1: 2.86\n",
      "    Accuracy: 0.00\n",
      "  Val Fine-grained Metrics (Constrained):\n",
      "    Micro F1: 7.69\n",
      "    Macro F1: 2.86\n",
      "    Accuracy: 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/40: 100%|█| 13/13 [00:00<00:00, 13.65batch/s, coarse_loss=0.618, fine_loss=0.586, loss=1.204\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/40:\n",
      "  Train Loss: 1.1867\n",
      "  Train Coarse-grained Metrics:\n",
      "    Micro F1: 48.67\n",
      "    Macro F1: 19.05\n",
      "    Accuracy: 21.15\n",
      "  Train Fine-grained Metrics (Unconstrained):\n",
      "    Micro F1: 29.48\n",
      "    Macro F1: 23.64\n",
      "    Accuracy: 1.92\n",
      "  Train Fine-grained Metrics (Constrained):\n",
      "    Micro F1: 25.30\n",
      "    Macro F1: 10.41\n",
      "    Accuracy: 0.00\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "  Val Loss: 1.2001\n",
      "  Val Coarse-grained Metrics:\n",
      "    Micro F1: 57.58\n",
      "    Macro F1: 20.65\n",
      "    Accuracy: 29.63\n",
      "  Val Fine-grained Metrics (Unconstrained):\n",
      "    Micro F1: 7.69\n",
      "    Macro F1: 2.60\n",
      "    Accuracy: 0.00\n",
      "  Val Fine-grained Metrics (Constrained):\n",
      "    Micro F1: 7.69\n",
      "    Macro F1: 2.60\n",
      "    Accuracy: 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/40: 100%|█| 13/13 [00:00<00:00, 13.72batch/s, coarse_loss=0.629, fine_loss=0.617, loss=1.247\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/40:\n",
      "  Train Loss: 1.2442\n",
      "  Train Coarse-grained Metrics:\n",
      "    Micro F1: 48.67\n",
      "    Macro F1: 19.05\n",
      "    Accuracy: 21.15\n",
      "  Train Fine-grained Metrics (Unconstrained):\n",
      "    Micro F1: 22.22\n",
      "    Macro F1: 15.88\n",
      "    Accuracy: 1.92\n",
      "  Train Fine-grained Metrics (Constrained):\n",
      "    Micro F1: 19.64\n",
      "    Macro F1: 9.00\n",
      "    Accuracy: 1.92\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "  Val Loss: 1.4964\n",
      "  Val Coarse-grained Metrics:\n",
      "    Micro F1: 57.58\n",
      "    Macro F1: 20.65\n",
      "    Accuracy: 29.63\n",
      "  Val Fine-grained Metrics (Unconstrained):\n",
      "    Micro F1: 0.00\n",
      "    Macro F1: 0.00\n",
      "    Accuracy: 0.00\n",
      "  Val Fine-grained Metrics (Constrained):\n",
      "    Micro F1: 0.00\n",
      "    Macro F1: 0.00\n",
      "    Accuracy: 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/40: 100%|█| 13/13 [00:00<00:00, 13.71batch/s, coarse_loss=0.654, fine_loss=0.490, loss=1.144\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8/40:\n",
      "  Train Loss: 1.2849\n",
      "  Train Coarse-grained Metrics:\n",
      "    Micro F1: 48.85\n",
      "    Macro F1: 19.16\n",
      "    Accuracy: 21.15\n",
      "  Train Fine-grained Metrics (Unconstrained):\n",
      "    Micro F1: 23.56\n",
      "    Macro F1: 15.46\n",
      "    Accuracy: 3.85\n",
      "  Train Fine-grained Metrics (Constrained):\n",
      "    Micro F1: 22.86\n",
      "    Macro F1: 8.17\n",
      "    Accuracy: 2.88\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "  Val Loss: 1.2551\n",
      "  Val Coarse-grained Metrics:\n",
      "    Micro F1: 57.58\n",
      "    Macro F1: 20.65\n",
      "    Accuracy: 29.63\n",
      "  Val Fine-grained Metrics (Unconstrained):\n",
      "    Micro F1: 11.32\n",
      "    Macro F1: 3.57\n",
      "    Accuracy: 3.70\n",
      "  Val Fine-grained Metrics (Constrained):\n",
      "    Micro F1: 11.32\n",
      "    Macro F1: 3.57\n",
      "    Accuracy: 3.70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/40: 100%|█| 13/13 [00:00<00:00, 13.73batch/s, coarse_loss=0.637, fine_loss=0.334, loss=0.971\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/tmp/ipykernel_529631/4111148653.py:353: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(os.path.join(checkpoint_dir, 'best_model.pt')))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9/40:\n",
      "  Train Loss: 1.1070\n",
      "  Train Coarse-grained Metrics:\n",
      "    Micro F1: 48.85\n",
      "    Macro F1: 19.16\n",
      "    Accuracy: 21.15\n",
      "  Train Fine-grained Metrics (Unconstrained):\n",
      "    Micro F1: 38.20\n",
      "    Macro F1: 26.89\n",
      "    Accuracy: 7.69\n",
      "  Train Fine-grained Metrics (Constrained):\n",
      "    Micro F1: 32.75\n",
      "    Macro F1: 12.41\n",
      "    Accuracy: 4.81\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "  Val Loss: 1.4199\n",
      "  Val Coarse-grained Metrics:\n",
      "    Micro F1: 57.58\n",
      "    Macro F1: 20.65\n",
      "    Accuracy: 29.63\n",
      "  Val Fine-grained Metrics (Unconstrained):\n",
      "    Micro F1: 0.00\n",
      "    Macro F1: 0.00\n",
      "    Accuracy: 0.00\n",
      "  Val Fine-grained Metrics (Constrained):\n",
      "    Micro F1: 0.00\n",
      "    Macro F1: 0.00\n",
      "    Accuracy: 0.00\n",
      "Early stopping triggered.\n",
      "\n",
      "===== Testing best model =====\n",
      "\n",
      "===== Final Test Results =====\n",
      "Final micro f1: 20.34\n",
      "Final macro f1: 4.76\n",
      "\n",
      "Coarse-grained micro f1: 57.58\n",
      "Fine-grained micro f1: 20.34\n",
      "\n",
      "Coarse-grained macro f1: 20.65\n",
      "Fine-grained macro f1: 4.76\n",
      "\n",
      "Accuracy: 3.70\n",
      "\n",
      "\n",
      "+---------------+---------------+---------------+---------------+---------------+---------------+\n",
      "| Final         | Final         | Course-       | Fine-grained  | Course-       | Fine-grained  |\n",
      "| micro f1      | macro f1      | grained micro f1 | micro f1      | grained macro f1 | macro f1      |\n",
      "+---------------+---------------+---------------+---------------+---------------+---------------+\n",
      "| 20.34         | 4.76          | 57.58         | 20.34         | 20.65         | 4.76          |\n",
      "+---------------+---------------+---------------+---------------+---------------+---------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/mnt/cfs/anaconda3/envs/pykt-hzw3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    }
   ],
   "source": [
    "def train(configs):\n",
    "    # 初始化wandb\n",
    "    wandb.init(\n",
    "        project=configs.project,\n",
    "        entity=configs.entity,\n",
    "        name=configs.exp_name,\n",
    "    )\n",
    "\n",
    "    # 配置wandb\n",
    "    wandb_config = wandb.config\n",
    "    wandb_config.model_name = configs.model_name\n",
    "    wandb_config.num_coarse_labels = configs.num_coarse_labels\n",
    "    wandb_config.num_fine_labels = configs.num_fine_labels\n",
    "    wandb_config.dropout = configs.dropout\n",
    "    wandb_config.freeze_pooler = configs.freeze_pooler\n",
    "    wandb_config.batch_size = configs.batch_size\n",
    "    wandb_config.max_length = configs.max_length\n",
    "    wandb_config.lr = configs.lr\n",
    "    wandb_config.epochs = configs.epochs\n",
    "    wandb_config.device = configs.device\n",
    "    wandb_config.seed = configs.seed\n",
    "    wandb_config.threshold = configs.threshold\n",
    "\n",
    "    # 设置随机种子\n",
    "    random.seed(configs.seed)\n",
    "    np.random.seed(configs.seed)\n",
    "    torch.manual_seed(configs.seed)\n",
    "    \n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    # 创建检查点目录\n",
    "    checkpoint_dir = os.path.join(configs.checkpoint_dir, configs.exp_name)\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    # 加载数据集\n",
    "    train_dataset = ErrorDetectionDataset(configs.data_path)\n",
    "    val_dataset = ErrorDetectionDataset(configs.val_data_path)\n",
    "    test_dataset = ErrorDetectionDataset(configs.test_data_path)\n",
    "    \n",
    "\n",
    "    # 创建数据加载器\n",
    "    train_dataloader = ErrorDetectionDataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=configs.batch_size,\n",
    "        max_length=configs.max_length,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        device=configs.device,\n",
    "        tokenizer_name=configs.model_name\n",
    "    )\n",
    "\n",
    "    val_dataloader = ErrorDetectionDataLoader(\n",
    "        dataset=val_dataset,\n",
    "        batch_size=configs.batch_size,\n",
    "        max_length=configs.max_length,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        device=configs.device,\n",
    "        tokenizer_name=configs.model_name\n",
    "    )\n",
    "\n",
    "    test_dataloader = ErrorDetectionDataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size=configs.batch_size,\n",
    "        max_length=configs.max_length,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        device=configs.device,\n",
    "        tokenizer_name=configs.model_name\n",
    "    )\n",
    "    \n",
    "    fine_description = get_fine_description_embedding(model_path=configs.model_name)\n",
    "\n",
    "    # 创建模型\n",
    "    model = HierarchicalErrorClassifier(\n",
    "        pretrained_model_name=configs.model_name,\n",
    "        num_coarse_labels=configs.num_coarse_labels,\n",
    "        num_fine_labels=configs.num_fine_labels,\n",
    "        dropout=configs.dropout,\n",
    "        freeze_pooler=configs.freeze_pooler,\n",
    "        fine_description=fine_description\n",
    "    ).to(configs.device)\n",
    "\n",
    "    # 定义损失函数\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    # 定义优化器\n",
    "    optimizer = AdamW(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()),\n",
    "        lr=configs.lr\n",
    "    )\n",
    "\n",
    "    # 初始化最佳验证损失和早停计数器\n",
    "    best_val_f1 = 0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    # 监控模型\n",
    "    wandb.watch(model, log='all')\n",
    "    \n",
    "    # 获取标签映射，用于后续预测结果记录\n",
    "    coarse_label_map = {v: k for k, v in val_dataset.get_coarse_labels().items()}\n",
    "    fine_label_map = {v: k for k, v in val_dataset.get_fine_labels().items()}\n",
    "    \n",
    "    # print(\"coarse_label_map:\", coarse_label_map)\n",
    "    # print(\"fine_label_map:\", fine_label_map)    \n",
    "    # # 终止运行，用来debug\n",
    "    # return\n",
    "    \n",
    "    \n",
    "    # 训练循环\n",
    "    for epoch in range(configs.epochs):\n",
    "        # 训练阶段\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        all_coarse_preds = []\n",
    "        all_coarse_labels = []\n",
    "        all_fine_preds = []\n",
    "        all_fine_labels = []\n",
    "        all_constrained_fine_preds = []\n",
    "        \n",
    "        with tqdm(\n",
    "            train_dataloader,\n",
    "            total=len(train_dataloader),\n",
    "            desc=f'Epoch {epoch + 1}/{configs.epochs}',\n",
    "            unit='batch',\n",
    "            ncols=100\n",
    "        ) as pbar:\n",
    "            for input_ids, attention_mask, token_type_ids, coarse_labels, fine_labels, sent_ids in pbar:\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # 前向传播\n",
    "                coarse_probs, fine_probs = model(input_ids, attention_mask, token_type_ids)\n",
    "                \n",
    "                # print(\"coarse_probs:\", coarse_probs)\n",
    "                \n",
    "                # 计算损失\n",
    "                coarse_loss = criterion(coarse_probs, coarse_labels)\n",
    "                fine_loss = criterion(fine_probs, fine_labels)\n",
    "                loss = coarse_loss + fine_loss\n",
    "                \n",
    "                # 反向传播\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "                \n",
    "                # 收集预测结果\n",
    "                coarse_preds = (coarse_probs > configs.threshold).float().cpu().numpy()\n",
    "                fine_preds = (fine_probs > configs.threshold).float().cpu().numpy()\n",
    "                constrained_fine_preds = model.apply_hierarchical_constraint(\n",
    "                    (coarse_probs > configs.threshold).float(), \n",
    "                    (fine_probs > configs.threshold).float()\n",
    "                ).cpu().numpy()\n",
    "                \n",
    "                all_coarse_preds.extend(coarse_preds)\n",
    "                all_coarse_labels.extend(coarse_labels.cpu().numpy())\n",
    "                all_fine_preds.extend(fine_preds)\n",
    "                all_fine_labels.extend(fine_labels.cpu().numpy())\n",
    "                all_constrained_fine_preds.extend(constrained_fine_preds)\n",
    "                \n",
    "                # 更新进度条\n",
    "                pbar.set_postfix(\n",
    "                    loss=f'{loss.item():.3f}',\n",
    "                    coarse_loss=f'{coarse_loss.item():.3f}',\n",
    "                    fine_loss=f'{fine_loss.item():.3f}'\n",
    "                )\n",
    "                \n",
    "                # 记录到wandb\n",
    "                wandb.log({\n",
    "                    'batch_loss': loss.item(),\n",
    "                    'batch_coarse_loss': coarse_loss.item(),\n",
    "                    'batch_fine_loss': fine_loss.item()\n",
    "                })\n",
    "        \n",
    "        # 计算训练指标\n",
    "        train_loss = train_loss / len(train_dataloader)\n",
    "        \n",
    "        # 计算各种评估指标\n",
    "        train_coarse_metrics_micro = calculate_metrics(all_coarse_labels, all_coarse_preds, average='micro')\n",
    "        train_coarse_metrics_macro = calculate_metrics(all_coarse_labels, all_coarse_preds, average='macro')\n",
    "        train_fine_metrics_micro = calculate_metrics(all_fine_labels, all_fine_preds, average='micro')\n",
    "        train_fine_metrics_macro = calculate_metrics(all_fine_labels, all_fine_preds, average='macro')\n",
    "        train_constrained_fine_metrics_micro = calculate_metrics(all_fine_labels, all_constrained_fine_preds, average='micro')\n",
    "        train_constrained_fine_metrics_macro = calculate_metrics(all_fine_labels, all_constrained_fine_preds, average='macro')\n",
    "        \n",
    "        # 保存模型检查点\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f'epoch_{epoch + 1}.pt')\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        wandb.save(checkpoint_path)\n",
    "        \n",
    "        # 验证阶段\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        all_coarse_preds = []\n",
    "        all_coarse_labels = []\n",
    "        all_fine_preds = []\n",
    "        all_fine_labels = []\n",
    "        all_constrained_fine_preds = []\n",
    "        \n",
    "        # 记录验证集和测试集每个句子的真实标签和预测结果\n",
    "        val_sentence_predictions = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for input_ids, attention_mask, token_type_ids, coarse_labels, fine_labels, sent_ids in val_dataloader:\n",
    "                # 前向传播\n",
    "                coarse_probs, fine_probs = model(input_ids, attention_mask, token_type_ids)\n",
    "                \n",
    "                # 应用层次约束\n",
    "                coarse_preds = (coarse_probs > configs.threshold).float()\n",
    "                fine_preds = (fine_probs > configs.threshold).float()\n",
    "                constrained_fine_preds = model.apply_hierarchical_constraint(coarse_preds, fine_preds)\n",
    "                \n",
    "                # 计算损失\n",
    "                coarse_loss = criterion(coarse_probs, coarse_labels)\n",
    "                fine_loss = criterion(fine_probs, fine_labels)\n",
    "                loss = coarse_loss + fine_loss\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                # 收集预测结果\n",
    "                all_coarse_preds.extend(coarse_preds.cpu().numpy())\n",
    "                all_coarse_labels.extend(coarse_labels.cpu().numpy())\n",
    "                all_fine_preds.extend(fine_preds.cpu().numpy())\n",
    "                all_constrained_fine_preds.extend(constrained_fine_preds.cpu().numpy())\n",
    "                all_fine_labels.extend(fine_labels.cpu().numpy())\n",
    "                \n",
    "                # 记录当前 batch 中每个样本的预测结果和真实标签\n",
    "                coarse_preds_np = coarse_preds.cpu().numpy()\n",
    "                fine_preds_np = constrained_fine_preds.cpu().numpy()\n",
    "                \n",
    "                # 获取当前批次的原始句子\n",
    "                # 由于sent_ids可能是数字或者直接是句子标识符，视情况处理\n",
    "                # 这里我们使用索引从数据集中获取句子\n",
    "                batch_sentences = []\n",
    "                for sid in sent_ids:\n",
    "                    # 如果sent_ids是数字索引\n",
    "                    if isinstance(sid, int) or (isinstance(sid, torch.Tensor) and sid.numel() == 1):\n",
    "                        idx = sid if isinstance(sid, int) else sid.item()\n",
    "                        if idx >= 0 and idx < len(val_dataset):\n",
    "                            batch_sentences.append(val_dataset.data[idx][0])\n",
    "                        else:\n",
    "                            # 如果索引无效，使用一个默认值\n",
    "                            batch_sentences.append(f\"[Unknown sentence with ID {sid}]\")\n",
    "                    else:\n",
    "                        # 如果sent_ids本身就是句子标识符（字符串）\n",
    "                        batch_sentences.append(str(sid))\n",
    "                \n",
    "                for i in range(len(batch_sentences)):\n",
    "                    coarse_indices = np.where(coarse_preds_np[i] == 1)[0]\n",
    "                    fine_indices = np.where(fine_preds_np[i] == 1)[0]\n",
    "                    predicted_coarse = [coarse_label_map[idx] for idx in coarse_indices]\n",
    "                    predicted_fine = [fine_label_map[idx] for idx in fine_indices]\n",
    "                    \n",
    "                    # 真实标签\n",
    "                    true_coarse_indices = np.where(coarse_labels[i].cpu().numpy() == 1)[0]\n",
    "                    true_fine_indices = np.where(fine_labels[i].cpu().numpy() == 1)[0]\n",
    "                    true_coarse = [coarse_label_map[idx] for idx in true_coarse_indices]\n",
    "                    true_fine = [fine_label_map[idx] for idx in true_fine_indices]\n",
    "\n",
    "                    val_sentence_predictions.append({\n",
    "                        'sentence': batch_sentences[i],\n",
    "                        'predicted_coarse': predicted_coarse,\n",
    "                        'predicted_fine': predicted_fine,\n",
    "                        'true_coarse': true_coarse,\n",
    "                        'true_fine': true_fine\n",
    "                    })\n",
    "                    \n",
    "        # 保存验证集预测结果和真实标签到文件\n",
    "        val_pred_file = os.path.join(checkpoint_dir, f\"val_predictions_epoch_{epoch+1}.json\")\n",
    "        with open(val_pred_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(val_sentence_predictions, f, ensure_ascii=False, indent=4)\n",
    "        \n",
    "        # 计算验证指标\n",
    "        val_loss = val_loss / len(val_dataloader)\n",
    "        \n",
    "        # 计算各种评估指标\n",
    "        val_coarse_metrics_micro = calculate_metrics(all_coarse_labels, all_coarse_preds, average='micro')\n",
    "        val_coarse_metrics_macro = calculate_metrics(all_coarse_labels, all_coarse_preds, average='macro')\n",
    "        val_fine_metrics_micro = calculate_metrics(all_fine_labels, all_fine_preds, average='micro')\n",
    "        val_fine_metrics_macro = calculate_metrics(all_fine_labels, all_fine_preds, average='macro')\n",
    "        val_constrained_fine_metrics_micro = calculate_metrics(all_fine_labels, all_constrained_fine_preds, average='micro')\n",
    "        val_constrained_fine_metrics_macro = calculate_metrics(all_fine_labels, all_constrained_fine_preds, average='macro')\n",
    "        \n",
    "        # 输出训练和验证指标\n",
    "        print(f'\\nEpoch {epoch+1}/{configs.epochs}:')\n",
    "        print(f'  Train Loss: {train_loss:.4f}')\n",
    "        print(f'  Train Coarse-grained Metrics:')\n",
    "        print(f'    Micro F1: {train_coarse_metrics_micro[\"micro_f1\"]:.2f}')\n",
    "        print(f'    Macro F1: {train_coarse_metrics_macro[\"macro_f1\"]:.2f}')\n",
    "        print(f'    Accuracy: {train_coarse_metrics_micro[\"accuracy\"]:.2f}')\n",
    "        print(f'  Train Fine-grained Metrics (Unconstrained):')\n",
    "        print(f'    Micro F1: {train_fine_metrics_micro[\"micro_f1\"]:.2f}')\n",
    "        print(f'    Macro F1: {train_fine_metrics_macro[\"macro_f1\"]:.2f}')\n",
    "        print(f'    Accuracy: {train_fine_metrics_micro[\"accuracy\"]:.2f}')\n",
    "        print(f'  Train Fine-grained Metrics (Constrained):')\n",
    "        print(f'    Micro F1: {train_constrained_fine_metrics_micro[\"micro_f1\"]:.2f}')\n",
    "        print(f'    Macro F1: {train_constrained_fine_metrics_macro[\"macro_f1\"]:.2f}')\n",
    "        print(f'    Accuracy: {train_constrained_fine_metrics_micro[\"accuracy\"]:.2f}')\n",
    "        print(\"+\"*50)\n",
    "        print(f'  Val Loss: {val_loss:.4f}')\n",
    "        print(f'  Val Coarse-grained Metrics:')\n",
    "        print(f'    Micro F1: {val_coarse_metrics_micro[\"micro_f1\"]:.2f}')\n",
    "        print(f'    Macro F1: {val_coarse_metrics_macro[\"macro_f1\"]:.2f}')\n",
    "        print(f'    Accuracy: {val_coarse_metrics_micro[\"accuracy\"]:.2f}')\n",
    "        print(f'  Val Fine-grained Metrics (Unconstrained):')\n",
    "        print(f'    Micro F1: {val_fine_metrics_micro[\"micro_f1\"]:.2f}')\n",
    "        print(f'    Macro F1: {val_fine_metrics_macro[\"macro_f1\"]:.2f}')\n",
    "        print(f'    Accuracy: {val_fine_metrics_micro[\"accuracy\"]:.2f}')\n",
    "        print(f'  Val Fine-grained Metrics (Constrained):')\n",
    "        print(f'    Micro F1: {val_constrained_fine_metrics_micro[\"micro_f1\"]:.2f}')\n",
    "        print(f'    Macro F1: {val_constrained_fine_metrics_macro[\"macro_f1\"]:.2f}')\n",
    "        print(f'    Accuracy: {val_constrained_fine_metrics_micro[\"accuracy\"]:.2f}')\n",
    "        \n",
    "        # 记录到wandb\n",
    "        wandb.log({\n",
    "            'train_loss': train_loss,\n",
    "            'train_coarse_micro_f1': train_coarse_metrics_micro[\"micro_f1\"],\n",
    "            'train_coarse_macro_f1': train_coarse_metrics_macro[\"macro_f1\"],\n",
    "            'train_coarse_accuracy': train_coarse_metrics_micro[\"accuracy\"],\n",
    "            'train_fine_micro_f1': train_fine_metrics_micro[\"micro_f1\"],\n",
    "            'train_fine_macro_f1': train_fine_metrics_macro[\"macro_f1\"],\n",
    "            'train_fine_accuracy': train_fine_metrics_micro[\"accuracy\"],\n",
    "            'train_constrained_fine_micro_f1': train_constrained_fine_metrics_micro[\"micro_f1\"],\n",
    "            'train_constrained_fine_macro_f1': train_constrained_fine_metrics_macro[\"macro_f1\"],\n",
    "            'train_constrained_fine_accuracy': train_constrained_fine_metrics_micro[\"accuracy\"],\n",
    "            'val_loss': val_loss,\n",
    "            'val_coarse_micro_f1': val_coarse_metrics_micro[\"micro_f1\"],\n",
    "            'val_coarse_macro_f1': val_coarse_metrics_macro[\"macro_f1\"],\n",
    "            'val_coarse_accuracy': val_coarse_metrics_micro[\"accuracy\"],\n",
    "            'val_fine_micro_f1': val_fine_metrics_micro[\"micro_f1\"],\n",
    "            'val_fine_macro_f1': val_fine_metrics_macro[\"macro_f1\"],\n",
    "            'val_fine_accuracy': val_fine_metrics_micro[\"accuracy\"],\n",
    "            'val_constrained_fine_micro_f1': val_constrained_fine_metrics_micro[\"micro_f1\"],\n",
    "            'val_constrained_fine_macro_f1': val_constrained_fine_metrics_macro[\"macro_f1\"],\n",
    "            'val_constrained_fine_accuracy': val_constrained_fine_metrics_micro[\"accuracy\"],\n",
    "            'epoch': epoch + 1\n",
    "        })\n",
    "        \n",
    "        # 检查是否保存最佳模型并应用早停\n",
    "        if val_constrained_fine_metrics_micro[\"micro_f1\"] > best_val_f1:\n",
    "            best_val_f1 = val_constrained_fine_metrics_micro[\"micro_f1\"]\n",
    "            torch.save(model.state_dict(), os.path.join(checkpoint_dir, 'best_model.pt'))\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= configs.patience:\n",
    "                print('Early stopping triggered.')\n",
    "                break\n",
    "    \n",
    "    # 加载最佳模型进行测试\n",
    "    print(\"\\n===== Testing best model =====\")\n",
    "    model.load_state_dict(torch.load(os.path.join(checkpoint_dir, 'best_model.pt')))\n",
    "    model.eval()\n",
    "    \n",
    "    all_coarse_preds = []\n",
    "    all_coarse_labels = []\n",
    "    all_fine_preds = []\n",
    "    all_fine_labels = []\n",
    "    all_constrained_fine_preds = []\n",
    "    \n",
    "    # 记录测试集预测\n",
    "    test_sentence_predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask, token_type_ids, coarse_labels, fine_labels, sent_ids in test_dataloader:\n",
    "            # 前向传播\n",
    "            coarse_probs, fine_probs = model(input_ids, attention_mask, token_type_ids)\n",
    "            \n",
    "            # 应用层次约束\n",
    "            coarse_preds = (coarse_probs > configs.threshold).float()\n",
    "            fine_preds = (fine_probs > configs.threshold).float()\n",
    "            constrained_fine_preds = model.apply_hierarchical_constraint(coarse_preds, fine_preds)\n",
    "            \n",
    "            # 收集预测结果\n",
    "            all_coarse_preds.extend(coarse_preds.cpu().numpy())\n",
    "            all_coarse_labels.extend(coarse_labels.cpu().numpy())\n",
    "            all_fine_preds.extend(fine_preds.cpu().numpy())\n",
    "            all_constrained_fine_preds.extend(constrained_fine_preds.cpu().numpy())\n",
    "            all_fine_labels.extend(fine_labels.cpu().numpy())\n",
    "            \n",
    "            # 获取当前批次的原始句子\n",
    "            batch_sentences = []\n",
    "            for sid in sent_ids:\n",
    "                # 如果sent_ids是数字索引\n",
    "                if isinstance(sid, int) or (isinstance(sid, torch.Tensor) and sid.numel() == 1):\n",
    "                    idx = sid if isinstance(sid, int) else sid.item()\n",
    "                    if idx >= 0 and idx < len(test_dataset):\n",
    "                        batch_sentences.append(test_dataset.data[idx][0])\n",
    "                    else:\n",
    "                        # 如果索引无效，使用一个默认值\n",
    "                        batch_sentences.append(f\"[Unknown sentence with ID {sid}]\")\n",
    "                else:\n",
    "                    # 如果sent_ids本身就是句子标识符（字符串）\n",
    "                    batch_sentences.append(str(sid))\n",
    "            \n",
    "            # 记录当前 batch 中每个样本的预测结果和真实标签\n",
    "            coarse_preds_np = coarse_preds.cpu().numpy()\n",
    "            fine_preds_np = constrained_fine_preds.cpu().numpy()\n",
    "            for i in range(len(batch_sentences)):\n",
    "                coarse_indices = np.where(coarse_preds_np[i] == 1)[0]\n",
    "                fine_indices = np.where(fine_preds_np[i] == 1)[0]\n",
    "                predicted_coarse = [coarse_label_map[idx] for idx in coarse_indices]\n",
    "                predicted_fine = [fine_label_map[idx] for idx in fine_indices]\n",
    "\n",
    "                # 真实标签\n",
    "                true_coarse_indices = np.where(coarse_labels[i].cpu().numpy() == 1)[0]\n",
    "                true_fine_indices = np.where(fine_labels[i].cpu().numpy() == 1)[0]\n",
    "                true_coarse = [coarse_label_map[idx] for idx in true_coarse_indices]\n",
    "                true_fine = [fine_label_map[idx] for idx in true_fine_indices]\n",
    "\n",
    "                test_sentence_predictions.append({\n",
    "                    'sentence': batch_sentences[i],\n",
    "                    'predicted_coarse': predicted_coarse,\n",
    "                    'predicted_fine': predicted_fine,\n",
    "                    'true_coarse': true_coarse,\n",
    "                    'true_fine': true_fine\n",
    "                })\n",
    "                \n",
    "    # 保存测试集预测结果和真实标签到文件\n",
    "    test_pred_file = os.path.join(checkpoint_dir, \"test_predictions.json\")\n",
    "    with open(test_pred_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(test_sentence_predictions, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    # 计算测试指标\n",
    "    test_coarse_metrics_micro = calculate_metrics(all_coarse_labels, all_coarse_preds, average='micro')\n",
    "    test_coarse_metrics_macro = calculate_metrics(all_coarse_labels, all_coarse_preds, average='macro')\n",
    "    test_fine_metrics_micro = calculate_metrics(all_fine_labels, all_fine_preds, average='micro')\n",
    "    test_fine_metrics_macro = calculate_metrics(all_fine_labels, all_fine_preds, average='macro')\n",
    "    test_constrained_fine_metrics_micro = calculate_metrics(all_fine_labels, all_constrained_fine_preds, average='micro')\n",
    "    test_constrained_fine_metrics_macro = calculate_metrics(all_fine_labels, all_constrained_fine_preds, average='macro')\n",
    "    \n",
    "    # 输出测试结果\n",
    "    print(\"\\n===== Final Test Results =====\")\n",
    "    print(f\"Final micro f1: {test_constrained_fine_metrics_micro['micro_f1']:.2f}\")\n",
    "    print(f\"Final macro f1: {test_constrained_fine_metrics_macro['macro_f1']:.2f}\")\n",
    "    \n",
    "    print(\"\\nCoarse-grained micro f1: {:.2f}\".format(test_coarse_metrics_micro['micro_f1']))\n",
    "    print(\"Fine-grained micro f1: {:.2f}\".format(test_constrained_fine_metrics_micro['micro_f1']))\n",
    "    \n",
    "    print(\"\\nCoarse-grained macro f1: {:.2f}\".format(test_coarse_metrics_macro['macro_f1']))\n",
    "    print(\"Fine-grained macro f1: {:.2f}\".format(test_constrained_fine_metrics_macro['macro_f1']))\n",
    "    \n",
    "    print(\"\\nAccuracy: {:.2f}\".format(test_constrained_fine_metrics_micro['accuracy']))\n",
    "    \n",
    "    # 以表格形式输出所有指标（与给定的评估表格格式一致）\n",
    "    print(\"\\n\")\n",
    "    print(\"+\" + \"-\"*15 + \"+\" + \"-\"*15 + \"+\" + \"-\"*15 + \"+\" + \"-\"*15 + \"+\" + \"-\"*15 + \"+\" + \"-\"*15 + \"+\")\n",
    "    print(\"| {:<13} | {:<13} | {:<13} | {:<13} | {:<13} | {:<13} |\".format(\n",
    "        \"Final\", \"Final\", \"Course-\", \"Fine-grained\", \"Course-\", \"Fine-grained\"))\n",
    "    print(\"| {:<13} | {:<13} | {:<13} | {:<13} | {:<13} | {:<13} |\".format(\n",
    "        \"micro f1\", \"macro f1\", \"grained micro f1\", \"micro f1\", \"grained macro f1\", \"macro f1\"))\n",
    "    print(\"+\" + \"-\"*15 + \"+\" + \"-\"*15 + \"+\" + \"-\"*15 + \"+\" + \"-\"*15 + \"+\" + \"-\"*15 + \"+\" + \"-\"*15 + \"+\")\n",
    "    print(\"| {:<13.2f} | {:<13.2f} | {:<13.2f} | {:<13.2f} | {:<13.2f} | {:<13.2f} |\".format(\n",
    "        test_constrained_fine_metrics_micro['micro_f1'],\n",
    "        test_constrained_fine_metrics_macro['macro_f1'],\n",
    "        test_coarse_metrics_micro['micro_f1'],\n",
    "        test_constrained_fine_metrics_micro['micro_f1'],\n",
    "        test_coarse_metrics_macro['macro_f1'],\n",
    "        test_constrained_fine_metrics_macro['macro_f1']\n",
    "    ))\n",
    "    print(\"+\" + \"-\"*15 + \"+\" + \"-\"*15 + \"+\" + \"-\"*15 + \"+\" + \"-\"*15 + \"+\" + \"-\"*15 + \"+\" + \"-\"*15 + \"+\")\n",
    "    \n",
    "    # 记录最终结果到wandb\n",
    "    wandb.log({\n",
    "        'test_coarse_micro_f1': test_coarse_metrics_micro[\"micro_f1\"],\n",
    "        'test_coarse_macro_f1': test_coarse_metrics_macro[\"macro_f1\"],\n",
    "        'test_coarse_accuracy': test_coarse_metrics_micro[\"accuracy\"],\n",
    "        'test_fine_micro_f1': test_fine_metrics_micro[\"micro_f1\"],\n",
    "        'test_fine_macro_f1': test_fine_metrics_macro[\"macro_f1\"],\n",
    "        'test_fine_accuracy': test_fine_metrics_micro[\"accuracy\"],\n",
    "        'test_constrained_fine_micro_f1': test_constrained_fine_metrics_micro[\"micro_f1\"],\n",
    "        'test_constrained_fine_macro_f1': test_constrained_fine_metrics_macro[\"macro_f1\"],\n",
    "        'test_constrained_fine_accuracy': test_constrained_fine_metrics_micro[\"accuracy\"],\n",
    "        'final_micro_f1': test_constrained_fine_metrics_micro[\"micro_f1\"],\n",
    "        'final_macro_f1': test_constrained_fine_metrics_macro[\"macro_f1\"]\n",
    "    })\n",
    "    \n",
    "    # 完成wandb记录\n",
    "    wandb.finish()\n",
    "\n",
    "def predict(model, text, tokenizer, device, threshold=0.5):\n",
    "    \"\"\"\n",
    "    对单个文本进行预测\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # 编码文本\n",
    "    encoded = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=128,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    input_ids = encoded['input_ids'].to(device)\n",
    "    attention_mask = encoded['attention_mask'].to(device)\n",
    "    token_type_ids = encoded.get('token_type_ids', None)\n",
    "    \n",
    "    if token_type_ids is not None:\n",
    "        token_type_ids = token_type_ids.to(device)\n",
    "    \n",
    "    # 获取预测结果\n",
    "    with torch.no_grad():\n",
    "        coarse_probs, fine_probs = model(input_ids, attention_mask, token_type_ids)\n",
    "        \n",
    "        # 应用阈值\n",
    "        coarse_preds = (coarse_probs > threshold).float()\n",
    "        fine_preds = (fine_probs > threshold).float()\n",
    "        \n",
    "        # 应用层次约束\n",
    "        constrained_fine_preds = model.apply_hierarchical_constraint(coarse_preds, fine_preds)\n",
    "    \n",
    "    # 映射结果到标签\n",
    "    coarse_indices = torch.nonzero(coarse_preds[0]).cpu().numpy().flatten()\n",
    "    fine_indices = torch.nonzero(constrained_fine_preds[0]).cpu().numpy().flatten()\n",
    "    \n",
    "    # 将索引转换为标签名称（需要模型中有这些映射）\n",
    "    coarse_label_map = {v: k for k, v in model.coarse_labels.items()}\n",
    "    fine_label_map = {v: k for k, v in model.fine_labels.items()}\n",
    "    \n",
    "    predicted_coarse = [coarse_label_map[idx] for idx in coarse_indices]\n",
    "    predicted_fine = [fine_label_map[idx] for idx in fine_indices]\n",
    "    \n",
    "    return predicted_coarse, predicted_fine\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 在以下主函数中添加判断Jupyter环境的逻辑\n",
    "if __name__ == '__main__':\n",
    "    # 判断是否在Jupyter环境中运行\n",
    "    try:\n",
    "        # 检查是否在Jupyter中运行\n",
    "        get_ipython = globals().get('get_ipython', None)\n",
    "        if get_ipython and 'IPKernelApp' in get_ipython().config:\n",
    "            # 在Jupyter环境中运行，使用默认配置\n",
    "            print(\"Running in Jupyter environment, using default configs\")\n",
    "            configs = get_default_configs()\n",
    "        else:\n",
    "            # 在命令行环境中运行，使用argparse\n",
    "            configs = argparser()\n",
    "    except:\n",
    "        # 任何异常都使用argparse处理\n",
    "        configs = argparser()\n",
    "    \n",
    "    # 设置实验名称\n",
    "    if configs.name is None:\n",
    "        configs.exp_name = \\\n",
    "            f'{os.path.basename(configs.model_name)}' + \\\n",
    "            f'{\"_fp\" if configs.freeze_pooler else \"\"}' + \\\n",
    "            f'_b{configs.batch_size}_e{configs.epochs}' + \\\n",
    "            f'_len{configs.max_length}_lr{configs.lr}'\n",
    "    else:\n",
    "        configs.exp_name = configs.name\n",
    "    \n",
    "    # 设置设备\n",
    "    if configs.device is None:\n",
    "        configs.device = torch.device(\n",
    "            'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        )\n",
    "    \n",
    "    # 调用训练函数\n",
    "    train(configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pykt-hzw3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
