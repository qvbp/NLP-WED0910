# åœ¨æ–‡ä»¶å¼€å¤´æ·»åŠ è¿™äº›å¯¼å…¥
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import multilabel_confusion_matrix, confusion_matrix
import matplotlib
matplotlib.use('Agg')  # ä½¿ç”¨éäº¤äº’å¼åç«¯ï¼Œé€‚åˆæœåŠ¡å™¨ç¯å¢ƒ

import torch
import torch.nn as nn
import torch.nn.functional as F
import os
import random
import argparse
import numpy as np
import json
from tqdm import tqdm
from transformers import BertModel, AutoModel
from torch.optim import AdamW
from torch.utils.data import Dataset
from torch.utils.data import DataLoader
from transformers import AutoTokenizer
from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score
import time
from datetime import datetime

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


# æ·»åŠ ä¸­æ–‡å­—ä½“æ”¯æŒ
import matplotlib.font_manager as fm
from matplotlib import rcParams

# è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒçš„å‡½æ•°
def setup_chinese_font():
    """
    è®¾ç½®matplotlibæ”¯æŒä¸­æ–‡å­—ä½“
    ä¼˜å…ˆå°è¯•ç³»ç»Ÿå¸¸è§çš„ä¸­æ–‡å­—ä½“ï¼Œå¦‚æœéƒ½æ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤å­—ä½“ä½†ç¦ç”¨å­—ä½“è­¦å‘Š
    """
    # å¸¸è§çš„ä¸­æ–‡å­—ä½“åˆ—è¡¨ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰
    chinese_fonts = [
        'SimHei',           # é»‘ä½“ (Windows)
        'Microsoft YaHei',  # å¾®è½¯é›…é»‘ (Windows)
        'PingFang SC',      # è‹¹æ–¹ (macOS)
        'Hiragino Sans GB', # å†¬é’é»‘ä½“ (macOS)
        'WenQuanYi Micro Hei', # æ–‡æ³‰é©¿å¾®ç±³é»‘ (Linux)
        'Noto Sans CJK SC',    # æ€æºé»‘ä½“ (Linux)
        'DejaVu Sans'          # å¤‡ç”¨å­—ä½“
    ]
    
    # è·å–ç³»ç»Ÿå¯ç”¨å­—ä½“
    available_fonts = [f.name for f in fm.fontManager.ttflist]
    
    # å¯»æ‰¾å¯ç”¨çš„ä¸­æ–‡å­—ä½“
    selected_font = None
    for font in chinese_fonts:
        if font in available_fonts:
            selected_font = font
            print(f"   âœ… æ‰¾åˆ°ä¸­æ–‡å­—ä½“: {font}")
            break
    
    if selected_font is None:
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä¸­æ–‡å­—ä½“ï¼Œä½¿ç”¨ç³»ç»Ÿé»˜è®¤å­—ä½“ï¼Œä½†è®¾ç½®è­¦å‘Šè¿‡æ»¤
        print("   âš ï¸  æœªæ‰¾åˆ°ä¸­æ–‡å­—ä½“ï¼Œå°†ä½¿ç”¨é»˜è®¤å­—ä½“ï¼ˆå¯èƒ½æ˜¾ç¤ºä¸ºæ–¹å—ï¼‰")
        selected_font = 'DejaVu Sans'
        # ç¦ç”¨å­—ä½“ç›¸å…³è­¦å‘Š
        import warnings
        warnings.filterwarnings('ignore', category=UserWarning, module='matplotlib')
    
    # è®¾ç½®matplotlibå‚æ•°
    rcParams['font.sans-serif'] = [selected_font]
    rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜
    
    return selected_font

def plot_multilabel_confusion_matrices(y_true, y_pred, labels, title_prefix, save_dir, epoch):
    """
    ç»˜åˆ¶å¤šæ ‡ç­¾åˆ†ç±»çš„æ··æ·†çŸ©é˜µï¼ˆæ”¯æŒä¸­æ–‡å­—ä½“ï¼‰
    
    Args:
        y_true: çœŸå®æ ‡ç­¾ (n_samples, n_labels)
        y_pred: é¢„æµ‹æ ‡ç­¾ (n_samples, n_labels)  
        labels: æ ‡ç­¾åç§°åˆ—è¡¨
        title_prefix: å›¾ç‰‡æ ‡é¢˜å‰ç¼€
        save_dir: ä¿å­˜ç›®å½•
        epoch: å½“å‰epoch
    """
    # è®¾ç½®ä¸­æ–‡å­—ä½“
    font_name = setup_chinese_font()
    
    # ç¡®ä¿æ ‡ç­¾æ˜¯numpyæ•°ç»„
    y_true = np.array(y_true)
    y_pred = np.array(y_pred)
    
    # è®¡ç®—å¤šæ ‡ç­¾æ··æ·†çŸ©é˜µ
    cm_multilabel = multilabel_confusion_matrix(y_true, y_pred)
    
    # è®¡ç®—å­å›¾çš„è¡Œåˆ—æ•°
    n_labels = len(labels)
    n_cols = min(4, n_labels)  # æœ€å¤š4åˆ—
    n_rows = (n_labels + n_cols - 1) // n_cols  # å‘ä¸Šå–æ•´
    
    # åˆ›å»ºå­å›¾ - è°ƒæ•´å›¾åƒå¤§å°ä»¥é€‚åº”ä¸­æ–‡æ ‡ç­¾
    fig, axes = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 4*n_rows))
    fig.suptitle(f'{title_prefix} - Epoch {epoch}', fontsize=16, fontweight='bold')
    
    # å¦‚æœåªæœ‰ä¸€è¡Œæˆ–ä¸€åˆ—ï¼Œç¡®ä¿axesæ˜¯2Dæ•°ç»„
    if n_rows == 1:
        axes = axes.reshape(1, -1)
    elif n_cols == 1:
        axes = axes.reshape(-1, 1)
    
    for i, label in enumerate(labels):
        row = i // n_cols
        col = i % n_cols
        ax = axes[row, col]
        
        # è·å–å½“å‰æ ‡ç­¾çš„æ··æ·†çŸ©é˜µ
        cm = cm_multilabel[i]
        
        # ç»˜åˆ¶çƒ­åŠ›å›¾
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,
                   xticklabels=['è´Ÿä¾‹', 'æ­£ä¾‹'],  # ä½¿ç”¨ä¸­æ–‡æ ‡ç­¾
                   yticklabels=['è´Ÿä¾‹', 'æ­£ä¾‹'],
                   annot_kws={'size': 12})  # è®¾ç½®æ³¨é‡Šå­—ä½“å¤§å°
        
        # è®¾ç½®æ ‡é¢˜ - ä½¿ç”¨ä¸­æ–‡å­—ä½“
        ax.set_title(f'{label}', fontsize=12, fontweight='bold', pad=20)
        ax.set_xlabel('é¢„æµ‹æ ‡ç­¾', fontsize=10)
        ax.set_ylabel('çœŸå®æ ‡ç­¾', fontsize=10)
        
        # è®¡ç®—å¹¶æ˜¾ç¤ºä¸€äº›æŒ‡æ ‡
        tn, fp, fn, tp = cm.ravel()
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
        
        # åœ¨æ ‡é¢˜ä¸‹æ–¹æ˜¾ç¤ºF1åˆ†æ•°
        ax.text(0.5, -0.15, f'F1: {f1:.3f}', transform=ax.transAxes, 
               ha='center', va='top', fontsize=9, fontweight='bold')
    
    # éšè—å¤šä½™çš„å­å›¾
    for i in range(n_labels, n_rows * n_cols):
        row = i // n_cols
        col = i % n_cols
        axes[row, col].set_visible(False)
    
    # è°ƒæ•´å¸ƒå±€ï¼Œç»™ä¸­æ–‡æ ‡ç­¾ç•™å‡ºæ›´å¤šç©ºé—´
    plt.tight_layout()
    plt.subplots_adjust(hspace=0.4, wspace=0.3, top=0.93)
    
    # ä¿å­˜å›¾ç‰‡
    save_path = os.path.join(save_dir, f'{title_prefix.lower().replace(" ", "_")}_epoch_{epoch}.png')
    plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()
    
    print(f"   ğŸ“Š æ··æ·†çŸ©é˜µå·²ä¿å­˜: {save_path}")
    
    return save_path

def plot_overall_performance_heatmap(metrics_history, save_dir):
    """
    ç»˜åˆ¶è®­ç»ƒè¿‡ç¨‹ä¸­å„ä¸ªæŒ‡æ ‡çš„çƒ­åŠ›å›¾ï¼ˆæ”¯æŒä¸­æ–‡å­—ä½“ï¼‰
    
    Args:
        metrics_history: è®­ç»ƒå†å²ä¸­çš„éªŒè¯æŒ‡æ ‡
        save_dir: ä¿å­˜ç›®å½•
    """
    # è®¾ç½®ä¸­æ–‡å­—ä½“
    font_name = setup_chinese_font()
    
    # æå–æŒ‡æ ‡æ•°æ®
    epochs = [m['epoch'] for m in metrics_history]
    
    # ä½¿ç”¨ä¸­æ–‡æŒ‡æ ‡åç§°
    metrics_data = {
        'æœ€ç»ˆå¾®å¹³å‡F1': [m['hierarchical_final_micro_f1'] for m in metrics_history],
        'æœ€ç»ˆå®å¹³å‡F1': [m['hierarchical_final_macro_f1'] for m in metrics_history],
        'ç²—ç²’åº¦å¾®å¹³å‡F1': [m['hierarchical_coarse_micro_f1'] for m in metrics_history],
        'ç»†ç²’åº¦å¾®å¹³å‡F1': [m['hierarchical_fine_micro_f1'] for m in metrics_history],
        'ç²—ç²’åº¦å®å¹³å‡F1': [m['hierarchical_coarse_macro_f1'] for m in metrics_history],
        'ç»†ç²’åº¦å®å¹³å‡F1': [m['hierarchical_fine_macro_f1'] for m in metrics_history]
    }
    
    # åˆ›å»ºDataFrame
    import pandas as pd
    df = pd.DataFrame(metrics_data, index=epochs)
    
    # ç»˜åˆ¶çƒ­åŠ›å›¾
    plt.figure(figsize=(12, 8))
    
    # åˆ›å»ºçƒ­åŠ›å›¾
    ax = sns.heatmap(df.T, annot=True, fmt='.2f', cmap='RdYlBu_r', 
                     cbar_kws={'label': 'F1åˆ†æ•° (%)'})
    
    # è®¾ç½®æ ‡é¢˜å’Œæ ‡ç­¾
    plt.title('å„è½®æ¬¡éªŒè¯æŒ‡æ ‡è¡¨ç°', fontsize=16, fontweight='bold', pad=20)
    plt.xlabel('è®­ç»ƒè½®æ¬¡', fontsize=12)
    plt.ylabel('è¯„ä¼°æŒ‡æ ‡', fontsize=12)
    
    # æ—‹è½¬yè½´æ ‡ç­¾ä»¥æ›´å¥½æ˜¾ç¤ºä¸­æ–‡
    plt.yticks(rotation=0)
    
    # è°ƒæ•´å¸ƒå±€
    plt.tight_layout()
    
    # ä¿å­˜å›¾ç‰‡
    save_path = os.path.join(save_dir, 'validation_metrics_heatmap.png')
    plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()
    
    print(f"   ğŸ“ˆ æŒ‡æ ‡çƒ­åŠ›å›¾å·²ä¿å­˜: {save_path}")
    
    return save_path

# å¦‚æœä½ æƒ³è¦æ‰‹åŠ¨å®‰è£…ä¸­æ–‡å­—ä½“ï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‡½æ•°
def install_chinese_font_if_needed():
    """
    æ£€æŸ¥å¹¶å°è¯•å®‰è£…ä¸­æ–‡å­—ä½“ï¼ˆé€‚ç”¨äºLinuxç¯å¢ƒï¼‰
    """
    import subprocess
    import sys
    
    try:
        # æ£€æŸ¥æ˜¯å¦ä¸ºLinuxç³»ç»Ÿ
        if sys.platform.startswith('linux'):
            # å°è¯•å®‰è£…ä¸­æ–‡å­—ä½“åŒ…
            print("ğŸ”§ å°è¯•å®‰è£…ä¸­æ–‡å­—ä½“åŒ…...")
            subprocess.run(['sudo', 'apt-get', 'update'], capture_output=True)
            subprocess.run(['sudo', 'apt-get', 'install', '-y', 'fonts-wqy-microhei', 'fonts-wqy-zenhei'], 
                          capture_output=True)
            
            # æ¸…é™¤matplotlibå­—ä½“ç¼“å­˜
            import matplotlib.font_manager
            matplotlib.font_manager._rebuild()
            print("âœ… ä¸­æ–‡å­—ä½“å®‰è£…å®Œæˆï¼Œè¯·é‡å¯ç¨‹åº")
            
    except Exception as e:
        print(f"âš ï¸  è‡ªåŠ¨å®‰è£…å­—ä½“å¤±è´¥: {e}")
        print("è¯·æ‰‹åŠ¨å®‰è£…ä¸­æ–‡å­—ä½“åŒ…ï¼Œä¾‹å¦‚ï¼š")
        print("Ubuntu/Debian: sudo apt-get install fonts-wqy-microhei")
        print("CentOS/RHEL: sudo yum install wqy-microhei-fonts")

class ErrorDetectionDataset(Dataset):
    def __init__(
            self,
            data_path,
            coarse_labels={
                "å­—ç¬¦çº§é”™è¯¯": 0,
                "æˆåˆ†æ®‹ç¼ºå‹é”™è¯¯": 1, 
                "æˆåˆ†èµ˜ä½™å‹é”™è¯¯": 2,
                "æˆåˆ†æ­é…ä¸å½“å‹é”™è¯¯": 3
            },
            fine_labels={
                "ç¼ºå­—æ¼å­—": 0,
                "é”™åˆ«å­—é”™è¯¯": 1,
                "ç¼ºå°‘æ ‡ç‚¹": 2,
                "é”™ç”¨æ ‡ç‚¹": 3,
                "ä¸»è¯­ä¸æ˜": 4,
                "è°“è¯­æ®‹ç¼º": 5,
                "å®¾è¯­æ®‹ç¼º": 6,
                "å…¶ä»–æˆåˆ†æ®‹ç¼º": 7,
                "ä¸»è¯­å¤šä½™": 8,
                "è™šè¯å¤šä½™": 9,
                "å…¶ä»–æˆåˆ†å¤šä½™": 10,
                "è¯­åºä¸å½“": 11,
                "åŠ¨å®¾æ­é…ä¸å½“": 12,
                "å…¶ä»–æ­é…ä¸å½“": 13
            }
    ):
        self.data_path = data_path
        self.coarse_labels = coarse_labels
        self.fine_labels = fine_labels
        self._get_data()
    
    def _get_data(self):
        with open(self.data_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        self.data = []
        for item in data:
            sent = item['sent']
            coarse_types = item['CourseGrainedErrorType']
            fine_types = item['FineGrainedErrorType']
            
            # æ„å»ºç²—ç²’åº¦æ ‡ç­¾ï¼ˆå¤šæ ‡ç­¾ï¼‰
            coarse_label = [0] * len(self.coarse_labels)
            for c_type in coarse_types:
                if c_type in self.coarse_labels:
                    coarse_label[self.coarse_labels[c_type]] = 1
            
            # æ„å»ºç»†ç²’åº¦æ ‡ç­¾ï¼ˆå¤šæ ‡ç­¾ï¼‰
            fine_label = [0] * len(self.fine_labels)
            for f_type in fine_types:
                if f_type in self.fine_labels:
                    fine_label[self.fine_labels[f_type]] = 1
            
            self.data.append((sent, coarse_label, fine_label, item.get('sent_id', -1)))
    
    def __len__(self):
        return len(self.data)
    
    def get_coarse_labels(self):
        return self.coarse_labels
    
    def get_fine_labels(self):
        return self.fine_labels
    
    def __getitem__(self, idx):
        return self.data[idx]


class ErrorDetectionDataLoader:
    def __init__(
        self,
        dataset,
        batch_size=16,
        max_length=128,
        shuffle=True,
        drop_last=True,
        device=None,
        tokenizer_name='/mnt/cfs/huangzhiwei/NLP-WED0910/projects/models/bge-large-zh-v1.5'
    ):
        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
        self.dataset = dataset
        self.batch_size = batch_size
        self.max_length = max_length
        self.shuffle = shuffle
        self.drop_last = drop_last
        
        if device is None:
            self.device = torch.device(
                'cuda' if torch.cuda.is_available() else 'cpu'
            )
        else:
            self.device = device
        
        self.loader = DataLoader(
            dataset=self.dataset,
            batch_size=self.batch_size,
            collate_fn=self.collate_fn,
            shuffle=self.shuffle,
            drop_last=self.drop_last
        )
    
    def collate_fn(self, data):
        sents = [item[0] for item in data]
        coarse_labels = [item[1] for item in data]
        fine_labels = [item[2] for item in data]
        sent_ids = [item[3] for item in data]
        
        # ç¼–ç æ–‡æœ¬
        encoded = self.tokenizer.batch_encode_plus(
            batch_text_or_text_pairs=sents,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt',
            return_length=True
        )
        
        input_ids = encoded['input_ids'].to(self.device)
        attention_mask = encoded['attention_mask'].to(self.device)
        
        # å¤„ç†æ ‡ç­¾
        if coarse_labels[0] == -1:
            coarse_labels = None
            fine_labels = None
        else:
            coarse_labels = torch.tensor(coarse_labels, dtype=torch.float).to(self.device)
            fine_labels = torch.tensor(fine_labels, dtype=torch.float).to(self.device)
        
        return input_ids, attention_mask, coarse_labels, fine_labels, sent_ids
    
    def __iter__(self):
        for data in self.loader:
            yield data
    
    def __len__(self):
        return len(self.loader)

class MoELayer(nn.Module):
    """Mixture of Experts Layer"""
    def __init__(self, input_dim, expert_dim, num_experts=8, top_k=2, dropout=0.1, load_balance_weight=0.01):
        super().__init__()
        self.num_experts = num_experts
        self.top_k = top_k
        self.input_dim = input_dim
        self.expert_dim = expert_dim
        
        # Gateç½‘ç»œï¼Œç”¨äºé€‰æ‹©ä¸“å®¶
        self.gate = nn.Linear(input_dim, num_experts)

        # ä¸“å®¶ç½‘ç»œï¼šæ¯ä¸€ä¸ªä¸“å®¶éƒ½æ˜¯ä¸¤å±‚çš„MLP
        self.experts = nn.ModuleList([
            nn.Sequential(
                nn.Linear(input_dim, expert_dim),
                nn.ReLU(),
                nn.Dropout(dropout),
                nn.Linear(expert_dim, input_dim),
                nn.Dropout(dropout)
            ) for _ in range(num_experts)
        ])

        # è´Ÿè½½å‡è¡¡æŸå¤±çš„æƒé‡
        self.load_balance_weight = load_balance_weight
    
    def forward(self, x):
        batch_size, seq_len = x.size(0), x.size(1) if len(x.shape) > 2 else 1

        # å¦‚æœè¾“å…¥æ˜¯3D (batch, seq, hidden)ï¼Œæˆ‘ä»¬åªç”¨[CLS]ä½ç½®
        if len(x.shape) == 3:
            x = x[:, 0, :]  # å–[CLS]ä½ç½®

        # Gateç½‘ç»œè¾“å‡º: é€‰æ‹©ä¸“å®¶çš„æ¦‚ç‡
        gate_logits = self.gate(x) # (batch_size, num_experts)
        gate_probs = F.softmax(gate_logits, dim=-1)

        # é€‰æ‹©å‰top_kä¸ªä¸“å®¶
        top_k_probs, top_k_indices = torch.topk(gate_probs, self.top_k, dim=-1)
        top_k_probs = top_k_probs / top_k_probs.sum(dim=-1, keepdim=True)  # å½’ä¸€åŒ–

        # è®¡ç®—ä¸“å®¶è¾“å‡º
        expert_outputs = []
        for i in range(self.num_experts):
            expert_output = self.experts[i](x) # (batch_size, input_dim)
            expert_outputs.append(expert_output)

        expert_outputs = torch.stack(expert_outputs, dim=1)  # (batch_size, num_experts, input_dim)

        # ç»„åˆtop-kä¸“å®¶çš„è¾“å‡º
        final_output = torch.zeros_like(x) # (batch_size, input_dim)
        for i in range(self.top_k):
            expert_idx = top_k_indices[:, i]  # ä¿®å¤è¯­æ³•é”™è¯¯
            expert_weight = top_k_probs[:, i:i+1]  # (batch_size, 1)

            # é€‰æ‹©å¯¹åº”ä¸“å®¶çš„è¾“å‡º
            selected_output = expert_outputs[torch.arange(batch_size), expert_idx]  # (batch_size, input_dim)
            final_output += expert_weight * selected_output

        # è®¡ç®—è´Ÿè½½å‡è¡¡æŸå¤±
        load_balance_loss = self.compute_load_balance_loss(gate_probs)
        
        return final_output, load_balance_loss

    
    def compute_load_balance_loss(self, gate_probs):
        """è®¡ç®—è´Ÿè½½å‡è¡¡æŸå¤±ï¼Œé¼“åŠ±ä¸“å®¶ä½¿ç”¨çš„å‡è¡¡æ€§"""
        # è®¡ç®—æ¯ä¸ªä¸“å®¶è¢«é€‰æ‹©çš„å¹³å‡æ¦‚ç‡
        expert_usage = gate_probs.mean(dim=0)  # (num_experts,)
        
        # ç†æƒ³æƒ…å†µä¸‹æ¯ä¸ªä¸“å®¶è¢«ä½¿ç”¨çš„æ¦‚ç‡åº”è¯¥æ˜¯ 1/num_experts
        ideal_usage = 1.0 / self.num_experts
        
        # è®¡ç®—è´Ÿè½½å‡è¡¡æŸå¤±ï¼ˆä½¿ç”¨L2è·ç¦»ï¼‰
        load_balance_loss = torch.sum((expert_usage - ideal_usage) ** 2)
        
        return self.load_balance_weight * load_balance_loss


class HierarchicalErrorClassifier(nn.Module):
    def __init__(
        self, 
        pretrained_model_name, 
        num_coarse_labels=4, 
        num_fine_labels=14, 
        dropout=0.2,
        num_experts=8,
        expert_dim=512,
        top_k=2,
        use_separate_moe=True,  # æ˜¯å¦ä¸ºç²—ç²’åº¦å’Œç»†ç²’åº¦ä½¿ç”¨ä¸åŒçš„MoE
        load_balance_weight=0.01
    ):
        super().__init__()
        
        self.bert = AutoModel.from_pretrained(pretrained_model_name)
        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()
        self.use_separate_moe = use_separate_moe
        
        hidden_size = self.bert.config.hidden_size

        
        if use_separate_moe:
            # ä¸ºç²—ç²’åº¦å’Œç»†ç²’åº¦åˆ†ç±»åˆ†åˆ«ä½¿ç”¨ä¸åŒçš„MoE
            self.coarse_moe = MoELayer(
                input_dim=hidden_size,
                expert_dim=expert_dim,
                num_experts=num_experts,
                top_k=top_k,
                dropout=dropout,
                load_balance_weight=load_balance_weight
            )
            self.fine_moe = MoELayer(
                input_dim=hidden_size,
                expert_dim=expert_dim,
                num_experts=num_experts,
                top_k=top_k,
                dropout=dropout,
                load_balance_weight=load_balance_weight
            )
        else:
            # å…±äº«ä¸€ä¸ªMoEå±‚
            self.shared_moe = MoELayer(
                input_dim=hidden_size,
                expert_dim=expert_dim,
                num_experts=num_experts,
                top_k=top_k,
                dropout=dropout,
                load_balance_weight=load_balance_weight
            )
        
        # ç²—ç²’åº¦åˆ†ç±»å™¨
        self.coarse_classifier = nn.Linear(self.bert.config.hidden_size, num_coarse_labels)
        
        # ç»†ç²’åº¦åˆ†ç±»å™¨
        self.fine_classifier = nn.Linear(self.bert.config.hidden_size, num_fine_labels)
        
        # å®šä¹‰ç²—ç²’åº¦ç±»åˆ«å’Œå¯¹åº”çš„ç»†ç²’åº¦ç´¢å¼•çš„æ˜ å°„
        self.coarse_to_fine_indices = {
            0: [0, 1, 2, 3],    # å­—ç¬¦çº§é”™è¯¯
            1: [4, 5, 6, 7],    # æˆåˆ†æ®‹ç¼ºå‹é”™è¯¯
            2: [8, 9, 10],      # æˆåˆ†å†—ä½™å‹é”™è¯¯
            3: [11, 12, 13]     # æˆåˆ†æ­é…ä¸å½“å‹é”™è¯¯
        }
    
    def forward(self, input_ids, attention_mask, token_type_ids=None):
        # BERTç¼–ç 
        if token_type_ids is not None:
            outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)
        else:
            outputs = self.bert(input_ids, attention_mask=attention_mask)
        
        pooled_output = outputs.last_hidden_state[:,0,:]  # å–[CLS]çš„è¾“å‡º
        pooled_output = self.dropout(pooled_output)

        total_load_balance_loss = 0
        
        if self.use_separate_moe:
            # ä½¿ç”¨ä¸åŒçš„MoEå¤„ç†ç²—ç²’åº¦å’Œç»†ç²’åº¦åˆ†ç±»
            coarse_features, coarse_lb_loss = self.coarse_moe(pooled_output)
            fine_features, fine_lb_loss = self.fine_moe(pooled_output)
            total_load_balance_loss = coarse_lb_loss + fine_lb_loss
        else:
            # ä½¿ç”¨å…±äº«MoE
            shared_features, shared_lb_loss = self.shared_moe(pooled_output)
            coarse_features = fine_features = shared_features
            total_load_balance_loss = shared_lb_loss
        
        # ç²—ç²’åº¦åˆ†ç±»
        coarse_logits = self.coarse_classifier(pooled_output)
        coarse_probs = torch.sigmoid(coarse_logits)
        
        # ç»†ç²’åº¦åˆ†ç±»
        fine_logits = self.fine_classifier(pooled_output)
        fine_probs = torch.sigmoid(fine_logits)
        
        return coarse_probs, fine_probs, total_load_balance_loss
    
    def apply_hierarchical_constraint(self, coarse_preds, fine_preds):
        """
        åº”ç”¨å±‚æ¬¡çº¦æŸï¼šå¦‚æœç²—ç²’åº¦ç±»åˆ«é¢„æµ‹ä¸ºè´Ÿï¼Œåˆ™è¯¥ç²—ç²’åº¦ä¸‹çš„æ‰€æœ‰ç»†ç²’åº¦ç±»åˆ«å‡è®¾ä¸ºè´Ÿ
        """
        constrained_fine_preds = fine_preds.clone()
        
        # éå†æ¯ä¸ªæ ·æœ¬
        for i in range(coarse_preds.size(0)):
            # å¯¹æ¯ä¸ªç²—ç²’åº¦ç±»åˆ«
            for coarse_idx, fine_indices in self.coarse_to_fine_indices.items():
                # å¦‚æœç²—ç²’åº¦ä¸ºè´Ÿï¼Œåˆ™å¯¹åº”çš„ç»†ç²’åº¦å…¨éƒ¨è®¾ä¸ºè´Ÿ
                if coarse_preds[i, coarse_idx] == 0:
                    constrained_fine_preds[i, fine_indices] = 0
        
        return constrained_fine_preds


def calculate_metrics(labels, predictions, average='micro'):
    """è®¡ç®—å„ç§è¯„ä¼°æŒ‡æ ‡"""
    labels = np.array(labels)
    predictions = np.array(predictions)
    
    micro_f1 = f1_score(labels, predictions, average='micro')
    macro_f1 = f1_score(labels, predictions, average='macro')
    sample_acc = accuracy_score(labels, predictions)
    
    return {
        'micro_f1': micro_f1 * 100,
        'macro_f1': macro_f1 * 100,
        'accuracy': sample_acc * 100
    }


# ä¿®æ”¹train_modelå‡½æ•°ä¸­çš„éªŒè¯éƒ¨åˆ†
def train_model_with_confusion_matrix(config):
    """ä¸»è®­ç»ƒå‡½æ•° - å¸¦æ··æ·†çŸ©é˜µåŠŸèƒ½"""
    print("ğŸš€ å¼€å§‹è®­ç»ƒå±‚æ¬¡åŒ–é”™è¯¯åˆ†ç±»æ¨¡å‹")
    print(f"ğŸ“± è®¾å¤‡: {config['device']}")
    print(f"ğŸ·ï¸  ç²—ç²’åº¦ç±»åˆ«æ•°: {config['num_coarse_labels']}")
    print(f"ğŸ·ï¸  ç»†ç²’åº¦ç±»åˆ«æ•°: {config['num_fine_labels']}")
    print(f"ğŸ“ æœ€å¤§åºåˆ—é•¿åº¦: {config['max_length']}")
    print(f"ğŸ”„ æ‰¹æ¬¡å¤§å°: {config['batch_size']}")
    print(f"ğŸ“š è®­ç»ƒè½®æ•°: {config['epochs']}")
    print("="*80)

    # åœ¨è®­ç»ƒå¼€å§‹å‰è®¾ç½®ä¸­æ–‡å­—ä½“
    print("ğŸ¨ è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ...")
    setup_chinese_font()
    
    # è®¾ç½®éšæœºç§å­
    random.seed(config['seed'])
    np.random.seed(config['seed'])
    torch.manual_seed(config['seed'])
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    
    # åˆ›å»ºç»“æœä¿å­˜ç›®å½•
    os.makedirs(config['results_dir'], exist_ok=True)
    os.makedirs(config['checkpoint_dir'], exist_ok=True)
    
    # åˆ›å»ºæ··æ·†çŸ©é˜µä¿å­˜ç›®å½•
    confusion_matrix_dir = os.path.join(config['results_dir'], 'confusion_matrices')
    os.makedirs(confusion_matrix_dir, exist_ok=True)
    
    # åŠ è½½æ•°æ®é›†
    print("ğŸ“‚ åŠ è½½æ•°æ®é›†...")
    train_dataset = ErrorDetectionDataset(config['data_path'])
    val_dataset = ErrorDetectionDataset(config['val_data_path'])
    print(f"ğŸ“Š è®­ç»ƒé›†å¤§å°: {len(train_dataset)}")
    print(f"ğŸ“Š éªŒè¯é›†å¤§å°: {len(val_dataset)}")
    
    # è·å–æ ‡ç­¾åç§°
    coarse_label_names = list(train_dataset.get_coarse_labels().keys())
    fine_label_names = list(train_dataset.get_fine_labels().keys())
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_dataloader = ErrorDetectionDataLoader(
        dataset=train_dataset,
        batch_size=config['batch_size'],
        max_length=config['max_length'],
        shuffle=True,
        drop_last=True,
        device=config['device'],
        tokenizer_name=config['model_name']
    )

    val_dataloader = ErrorDetectionDataLoader(
        dataset=val_dataset,
        batch_size=config['batch_size'],
        max_length=config['max_length'],
        shuffle=False,
        drop_last=False,
        device=config['device'],
        tokenizer_name=config['model_name']
    )
    
    # åˆ›å»ºæ¨¡å‹
    print("ğŸ—ï¸  æ„å»ºæ¨¡å‹...")
    model = HierarchicalErrorClassifier(
        pretrained_model_name=config['model_name'],
        num_coarse_labels=config['num_coarse_labels'],
        num_fine_labels=config['num_fine_labels'],
        dropout=config['dropout'],
        num_experts=config['num_experts'],
        expert_dim=config['expert_dim'],
        top_k=config['top_k'],
        use_separate_moe=config['use_separate_moe'],
        load_balance_weight=config['load_balance_weight']
    ).to(config['device'])
    
    print(f"ğŸ”§ æ¨¡å‹å‚æ•°:")
    print(f"   - Dropout: {config['dropout']}")
    print(f"   - ä¸“å®¶æ•°é‡: {config['num_experts']}")
    print(f"   - Top-K: {config['top_k']}")
    print(f"   - ä¸“å®¶ç»´åº¦: {config['expert_dim']}")
    print(f"   - ç‹¬ç«‹MoE: {config['use_separate_moe']}")
    print(f"   - è´Ÿè½½å‡è¡¡æƒé‡: {config['load_balance_weight']}")
    
    # å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.BCELoss()
    optimizer = AdamW(
        filter(lambda p: p.requires_grad, model.parameters()),
        lr=config['lr'],
        weight_decay=config.get('weight_decay', 0.01)
    )
    
    # åˆå§‹åŒ–æœ€ä½³éªŒè¯æ€§èƒ½å’Œ6ä¸ªæŒ‡æ ‡
    best_metrics = {
        'hierarchical_final_micro_f1': 0,
        'hierarchical_final_macro_f1': 0,
        'hierarchical_coarse_micro_f1': 0,
        'hierarchical_fine_micro_f1': 0,
        'hierarchical_coarse_macro_f1': 0,
        'hierarchical_fine_macro_f1': 0,
        'epoch': 0
    }
    
    best_val_f1 = 0
    patience_counter = 0
    
    # è®°å½•æ‰€æœ‰epochçš„ç»“æœ
    serializable_config = config.copy()
    serializable_config['device'] = str(config['device'])
    
    training_history = {
        'train_loss': [],
        'val_metrics': [],
        'best_metrics': best_metrics,
        'config': serializable_config
    }
    
    print("\nğŸ¯ å¼€å§‹è®­ç»ƒ...")
    start_time = time.time()
    
    # è®­ç»ƒå¾ªç¯
    for epoch in range(config['epochs']):
        epoch_start_time = time.time()
        
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0.0
        train_steps = 0
        
        train_progress = tqdm(train_dataloader, desc=f"Epoch {epoch+1}/{config['epochs']} [Train]")
        for input_ids, attention_mask, coarse_labels, fine_labels, sent_ids in train_progress:
            optimizer.zero_grad()
            
            # å‰å‘ä¼ æ’­
            coarse_probs, fine_probs, load_balance_loss = model(input_ids, attention_mask)
            
            # è®¡ç®—åˆ†ç±»æŸå¤±
            coarse_loss = criterion(coarse_probs, coarse_labels)
            fine_loss = criterion(fine_probs, fine_labels)
            classification_loss = coarse_loss + fine_loss
            
            # æ€»æŸå¤± = åˆ†ç±»æŸå¤± + è´Ÿè½½å‡è¡¡æŸå¤±
            total_loss = classification_loss + load_balance_loss
            
            # åå‘ä¼ æ’­
            total_loss.backward()
            optimizer.step()
            
            train_loss += classification_loss.item()
            train_steps += 1
            
            # æ›´æ–°è¿›åº¦æ¡
            train_progress.set_postfix({
                'Loss': f'{classification_loss.item():.4f}',
                'LB_Loss': f'{load_balance_loss.item():.4f}'
            })
        
        avg_train_loss = train_loss / train_steps
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        all_coarse_preds = []
        all_coarse_labels = []
        all_fine_labels = []
        all_constrained_fine_preds = []
        
        val_progress = tqdm(val_dataloader, desc=f"Epoch {epoch+1}/{config['epochs']} [Val]")
        with torch.no_grad():
            for input_ids, attention_mask, coarse_labels, fine_labels, sent_ids in val_progress:
                # å‰å‘ä¼ æ’­
                coarse_probs, fine_probs, load_balance_loss = model(input_ids, attention_mask)
                
                # åº”ç”¨å±‚æ¬¡çº¦æŸ
                coarse_preds = (coarse_probs > config['threshold']).float()
                fine_preds = (fine_probs > config['threshold']).float()
                constrained_fine_preds = model.apply_hierarchical_constraint(coarse_preds, fine_preds)
                
                # æ”¶é›†é¢„æµ‹ç»“æœ
                all_coarse_preds.extend(coarse_preds.cpu().numpy())
                all_coarse_labels.extend(coarse_labels.cpu().numpy())
                all_constrained_fine_preds.extend(constrained_fine_preds.cpu().numpy())
                all_fine_labels.extend(fine_labels.cpu().numpy())
        
        # è®¡ç®—éªŒè¯æŒ‡æ ‡
        val_coarse_metrics_micro = calculate_metrics(all_coarse_labels, all_coarse_preds, average='micro')
        val_coarse_metrics_macro = calculate_metrics(all_coarse_labels, all_coarse_preds, average='macro')
        val_constrained_fine_metrics_micro = calculate_metrics(all_fine_labels, all_constrained_fine_preds, average='micro')
        val_constrained_fine_metrics_macro = calculate_metrics(all_fine_labels, all_constrained_fine_preds, average='macro')
        
        # è®¡ç®—6ä¸ªæŒ‡æ ‡
        hierarchical_final_micro_f1 = (val_constrained_fine_metrics_micro['micro_f1'] + val_coarse_metrics_micro['micro_f1']) / 2
        hierarchical_final_macro_f1 = (val_constrained_fine_metrics_macro['macro_f1'] + val_coarse_metrics_macro['macro_f1']) / 2
        hierarchical_coarse_micro_f1 = val_coarse_metrics_micro['micro_f1']
        hierarchical_fine_micro_f1 = val_constrained_fine_metrics_micro['micro_f1']
        hierarchical_coarse_macro_f1 = val_coarse_metrics_macro['macro_f1']
        hierarchical_fine_macro_f1 = val_constrained_fine_metrics_macro['macro_f1']
        
        current_metrics = {
            'hierarchical_final_micro_f1': hierarchical_final_micro_f1,
            'hierarchical_final_macro_f1': hierarchical_final_macro_f1,
            'hierarchical_coarse_micro_f1': hierarchical_coarse_micro_f1,
            'hierarchical_fine_micro_f1': hierarchical_fine_micro_f1,
            'hierarchical_coarse_macro_f1': hierarchical_coarse_macro_f1,
            'hierarchical_fine_macro_f1': hierarchical_fine_macro_f1,
            'epoch': epoch + 1
        }
        
        # è®°å½•è®­ç»ƒå†å²
        training_history['train_loss'].append(avg_train_loss)
        training_history['val_metrics'].append(current_metrics.copy())
        
        epoch_time = time.time() - epoch_start_time
        
        # æ‰“å°å½“å‰epochç»“æœ
        print(f"\nğŸ“Š Epoch {epoch+1}/{config['epochs']} ç»“æœ (è€—æ—¶: {epoch_time:.1f}s):")
        print(f"   ğŸ”¥ è®­ç»ƒæŸå¤±: {avg_train_loss:.4f}")
        print(f"   ğŸ“ˆ Final Micro F1: {hierarchical_final_micro_f1:.2f}%")
        print(f"   ğŸ“ˆ Final Macro F1: {hierarchical_final_macro_f1:.2f}%")
        print(f"   ğŸ“Š Coarse Micro F1: {hierarchical_coarse_micro_f1:.2f}%")
        print(f"   ğŸ“Š Fine Micro F1: {hierarchical_fine_micro_f1:.2f}%")
        print(f"   ğŸ“Š Coarse Macro F1: {hierarchical_coarse_macro_f1:.2f}%")
        print(f"   ğŸ“Š Fine Macro F1: {hierarchical_fine_macro_f1:.2f}%")
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºæœ€ä½³æ€§èƒ½ï¼ˆä½¿ç”¨Final micro f1ä½œä¸ºä¸»è¦æŒ‡æ ‡ï¼‰
        if hierarchical_final_micro_f1 > best_val_f1:
            best_val_f1 = hierarchical_final_micro_f1
            best_metrics = current_metrics.copy()
            patience_counter = 0
            
            print(f"   ğŸ‰ æ–°çš„æœ€ä½³æ¨¡å‹!")
            
            # ç»˜åˆ¶æ··æ·†çŸ©é˜µï¼ˆä»…åœ¨æœ€ä½³epochæ—¶ç»˜åˆ¶ï¼‰
            print("   ğŸ¨ ç»˜åˆ¶æ··æ·†çŸ©é˜µ...")
            
            # ç»˜åˆ¶ç²—ç²’åº¦æ··æ·†çŸ©é˜µ
            plot_multilabel_confusion_matrices(
                all_coarse_labels, all_coarse_preds, 
                coarse_label_names, 
                "Coarse-grained Confusion Matrices", 
                confusion_matrix_dir, 
                epoch + 1
            )
            
            # ç»˜åˆ¶ç»†ç²’åº¦æ··æ·†çŸ©é˜µ
            plot_multilabel_confusion_matrices(
                all_fine_labels, all_constrained_fine_preds, 
                fine_label_names, 
                "Fine-grained Confusion Matrices", 
                confusion_matrix_dir, 
                epoch + 1
            )
            
        else:
            patience_counter += 1
            print(f"   â³ è€å¿ƒç­‰å¾…: {patience_counter}/{config['patience']}")
        
        # æ›´æ–°è®­ç»ƒå†å²ä¸­çš„æœ€ä½³æŒ‡æ ‡
        training_history['best_metrics'] = best_metrics.copy()
        
        # æ—©åœæ£€æŸ¥
        if patience_counter >= config['patience']:
            print(f"\nğŸ›‘ æ—©åœè§¦å‘! å·²è¿ç»­ {config['patience']} ä¸ªepochæ²¡æœ‰æ”¹å–„")
            break
        
        print("-" * 80)
    
    total_time = time.time() - start_time
    
    # ç»˜åˆ¶è®­ç»ƒè¿‡ç¨‹ä¸­çš„æŒ‡æ ‡çƒ­åŠ›å›¾
    if len(training_history['val_metrics']) > 1:
        print("ğŸ¨ ç»˜åˆ¶è®­ç»ƒæŒ‡æ ‡çƒ­åŠ›å›¾...")
        plot_overall_performance_heatmap(training_history['val_metrics'], confusion_matrix_dir)
    
    # ä¿å­˜å®Œæ•´çš„è®­ç»ƒå†å²å’Œæœ€ç»ˆç»“æœ
    training_history['total_time_hours'] = total_time / 3600
    training_history['final_epoch'] = epoch + 1
    training_history['early_stopped'] = patience_counter >= config['patience']
    
    history_save_path = os.path.join(config['results_dir'], 'training_history.json')
    with open(history_save_path, 'w', encoding='utf-8') as f:
        json.dump(training_history, f, ensure_ascii=False, indent=2)
    
    serializable_config_final = config.copy()
    serializable_config_final['device'] = str(config['device'])
    
    final_result = {
        'best_6_metrics': best_metrics,
        'config': serializable_config_final,
        'training_summary': {
            'total_epochs': epoch + 1,
            'total_time_hours': total_time / 3600,
            'early_stopped': patience_counter >= config['patience'],
            'best_epoch': best_metrics['epoch'],
            'training_date': datetime.now().isoformat()
        }
    }
    
    result_save_path = os.path.join(config['results_dir'], 'final_result.json')
    with open(result_save_path, 'w', encoding='utf-8') as f:
        json.dump(final_result, f, ensure_ascii=False, indent=2)
    
    # æ‰“å°æœ€ç»ˆç»“æœ
    print(f"\n{'='*80}")
    print("ğŸ‰ è®­ç»ƒå®Œæˆ!")
    print(f"â±ï¸  æ€»è€—æ—¶: {total_time/3600:.2f} å°æ—¶")
    print(f"ğŸ”„ è®­ç»ƒè½®æ•°: {epoch + 1}/{config['epochs']}")
    print(f"ğŸ¯ æœ€ä½³epoch: {best_metrics['epoch']}")
    if patience_counter >= config['patience']:
        print("ğŸ›‘ æ—©åœè§¦å‘")
    
    print(f"\nğŸ† æœ€ä½³6ä¸ªæŒ‡æ ‡:")
    print(f"   1. Final Micro F1: {best_metrics['hierarchical_final_micro_f1']:.2f}%")
    print(f"   2. Final Macro F1: {best_metrics['hierarchical_final_macro_f1']:.2f}%")
    print(f"   3. Coarse Micro F1: {best_metrics['hierarchical_coarse_micro_f1']:.2f}%")
    print(f"   4. Fine Micro F1: {best_metrics['hierarchical_fine_micro_f1']:.2f}%")
    print(f"   5. Coarse Macro F1: {best_metrics['hierarchical_coarse_macro_f1']:.2f}%")
    print(f"   6. Fine Macro F1: {best_metrics['hierarchical_fine_macro_f1']:.2f}%")
    
    print(f"\nğŸ“ ç»“æœå·²ä¿å­˜:")
    print(f"   - è®­ç»ƒå†å²: {history_save_path}")
    print(f"   - æœ€ç»ˆç»“æœ: {result_save_path}")
    print(f"   - æ··æ·†çŸ©é˜µ: {confusion_matrix_dir}")
    
    return best_metrics, training_history

# ä½¿ç”¨æ–°çš„è®­ç»ƒå‡½æ•°æ›¿æ¢åŸæ¥çš„mainå‡½æ•°è°ƒç”¨
def main_with_confusion_matrix():
    """ä¸»å‡½æ•° - å¸¦æ··æ·†çŸ©é˜µåŠŸèƒ½"""
    parser = argparse.ArgumentParser(description='å±‚æ¬¡åŒ–é”™è¯¯åˆ†ç±»æ¨¡å‹è®­ç»ƒ')
    
    # æ•°æ®è·¯å¾„å‚æ•°
    parser.add_argument('--data_path', type=str, 
                       default='/mnt/cfs/huangzhiwei/NLP-WED0910/datas/train.json',
                       help='è®­ç»ƒæ•°æ®è·¯å¾„')
    parser.add_argument('--val_data_path', type=str,
                       default='/mnt/cfs/huangzhiwei/NLP-WED0910/datas/val.json', 
                       help='éªŒè¯æ•°æ®è·¯å¾„')
    parser.add_argument('--model_name', type=str,
                       default='/mnt/cfs/huangzhiwei/NLP-WED0910/projects/models/bge-large-zh-v1.5',
                       help='é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„')
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument('--num_coarse_labels', type=int, default=4, help='ç²—ç²’åº¦æ ‡ç­¾æ•°')
    parser.add_argument('--num_fine_labels', type=int, default=14, help='ç»†ç²’åº¦æ ‡ç­¾æ•°')
    parser.add_argument('--max_length', type=int, default=128, help='æœ€å¤§åºåˆ—é•¿åº¦')
    parser.add_argument('--dropout', type=float, default=0.3135999246870766, help='Dropoutç‡')
    
    # MoEå‚æ•°
    parser.add_argument('--num_experts', type=int, default=15, help='ä¸“å®¶æ•°é‡')
    parser.add_argument('--expert_dim', type=int, default=512, help='ä¸“å®¶ç»´åº¦')
    parser.add_argument('--top_k', type=int, default=1, help='Top-Kä¸“å®¶')
    parser.add_argument('--use_separate_moe', type=bool, default=True, help='æ˜¯å¦ä½¿ç”¨ç‹¬ç«‹MoE')
    parser.add_argument('--load_balance_weight', type=float, default=0.013540158381723238, help='è´Ÿè½½å‡è¡¡æƒé‡')
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument('--batch_size', type=int, default=32, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--lr', type=float, default=1.86884655838032e-05, help='å­¦ä¹ ç‡')
    parser.add_argument('--weight_decay', type=float, default=0.01, help='æƒé‡è¡°å‡')
    parser.add_argument('--epochs', type=int, default=40, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--patience', type=int, default=5, help='æ—©åœè€å¿ƒå€¼')
    parser.add_argument('--threshold', type=float, default=0.31676353792873924, help='äºŒåˆ†ç±»é˜ˆå€¼')
    parser.add_argument('--seed', type=int, default=3407, help='éšæœºç§å­')
    
    # ä¿å­˜è·¯å¾„
    parser.add_argument('--results_dir', type=str, default='training_results', help='ç»“æœä¿å­˜ç›®å½•')
    parser.add_argument('--checkpoint_dir', type=str, default='checkpoints', help='æ¨¡å‹ä¿å­˜ç›®å½•')
    
    args = parser.parse_args()
    
    # è½¬æ¢ä¸ºé…ç½®å­—å…¸
    config = vars(args)
    config['device'] = device
    
    # çº¦æŸæ£€æŸ¥
    if config['top_k'] > config['num_experts']:
        config['top_k'] = config['num_experts']
        print(f"âš ï¸  Top-K ({args.top_k}) å¤§äºä¸“å®¶æ•°é‡ ({config['num_experts']})ï¼Œå·²è°ƒæ•´ä¸º {config['top_k']}")
    
    # æ£€æŸ¥æ•°æ®æ–‡ä»¶
    print("ğŸ” æ£€æŸ¥æ•°æ®æ–‡ä»¶...")
    for path_key in ['data_path', 'val_data_path']:
        if not os.path.exists(config[path_key]):
            print(f"âŒ {config[path_key]} ä¸å­˜åœ¨")
            return
        else:
            print(f"âœ… {config[path_key]} å­˜åœ¨")
    
    # æ£€æŸ¥æ¨¡å‹è·¯å¾„
    if not (os.path.exists(config['model_name']) or config['model_name'].startswith('bert-')):
        print(f"âŒ æ¨¡å‹è·¯å¾„ {config['model_name']} ä¸å­˜åœ¨")
        return
    else:
        print(f"âœ… æ¨¡å‹è·¯å¾„å­˜åœ¨")
    
    print(f"\nğŸ›ï¸  é…ç½®å‚æ•°:")
    for key, value in config.items():
        if key not in ['device']:
            print(f"   {key}: {value}")
    
    # å¼€å§‹è®­ç»ƒ
    best_metrics, training_history = train_model_with_confusion_matrix(config)
    
    print(f"\nâœ¨ è®­ç»ƒå®Œæˆ! æœ€ä½³ Final Micro F1: {best_metrics['hierarchical_final_micro_f1']:.2f}%")

# å¦‚æœè¦ä½¿ç”¨å¸¦æ··æ·†çŸ©é˜µçš„ç‰ˆæœ¬ï¼Œè¿è¡Œï¼š
if __name__ == '__main__':
    main_with_confusion_matrix()